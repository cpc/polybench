; ModuleID = './GN/DKMIEIPKDGJKEKAOHMBJLDNFAPOMGDMEACPLP/reduce_kernel/32-8-1-goffs0-smallgrid/parallel.bc'
source_filename = "parallel_bc"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: alwaysinline nofree norecurse nounwind
define void @_pocl_kernel_reduce_kernel(float* nocapture readonly %0, float* nocapture %1, i32 %2, i32 %3, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %4, i64 %5, i64 %6, i64 %7) local_unnamed_addr #0 !kernel_arg_addr_space !5 !kernel_arg_access_qual !6 !kernel_arg_type !7 !kernel_arg_base_type !8 !kernel_arg_type_qual !9 !kernel_arg_name !10 !pocl_generated !11 {
pregion_for_entry.pregion_for_init.i:
  %mul.i.i = shl i64 %5, 5
  %mul3.i.i = shl i64 %6, 3
  %conv2.i = trunc i64 %mul3.i.i to i32
  %mul.i = mul nsw i32 %conv2.i, %2
  %8 = trunc i64 %mul.i.i to i32
  %9 = shl i64 %5, 37
  %10 = ashr exact i64 %9, 32
  %11 = getelementptr inbounds float, float* %0, i64 %10
  %12 = bitcast float* %11 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %12, align 4, !tbaa !12, !llvm.access.group !16
  %13 = add nsw i32 %mul.i, %8
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds float, float* %1, i64 %14
  %16 = bitcast float* %15 to <8 x float>*
  %wide.load2 = load <8 x float>, <8 x float>* %16, align 4, !tbaa !12, !llvm.access.group !16
  %17 = fsub <8 x float> %wide.load2, %wide.load
  %18 = bitcast float* %15 to <8 x float>*
  store <8 x float> %17, <8 x float>* %18, align 4, !tbaa !12, !llvm.access.group !16
  %19 = or i64 %mul.i.i, 8
  %20 = trunc i64 %19 to i32
  %21 = shl i64 %19, 32
  %22 = ashr exact i64 %21, 32
  %23 = getelementptr inbounds float, float* %0, i64 %22
  %24 = bitcast float* %23 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %24, align 4, !tbaa !12, !llvm.access.group !16
  %25 = add nsw i32 %mul.i, %20
  %26 = sext i32 %25 to i64
  %27 = getelementptr inbounds float, float* %1, i64 %26
  %28 = bitcast float* %27 to <8 x float>*
  %wide.load2.1 = load <8 x float>, <8 x float>* %28, align 4, !tbaa !12, !llvm.access.group !16
  %29 = fsub <8 x float> %wide.load2.1, %wide.load.1
  %30 = bitcast float* %27 to <8 x float>*
  store <8 x float> %29, <8 x float>* %30, align 4, !tbaa !12, !llvm.access.group !16
  %31 = or i64 %mul.i.i, 16
  %32 = trunc i64 %31 to i32
  %33 = shl i64 %31, 32
  %34 = ashr exact i64 %33, 32
  %35 = getelementptr inbounds float, float* %0, i64 %34
  %36 = bitcast float* %35 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %36, align 4, !tbaa !12, !llvm.access.group !16
  %37 = add nsw i32 %mul.i, %32
  %38 = sext i32 %37 to i64
  %39 = getelementptr inbounds float, float* %1, i64 %38
  %40 = bitcast float* %39 to <8 x float>*
  %wide.load2.2 = load <8 x float>, <8 x float>* %40, align 4, !tbaa !12, !llvm.access.group !16
  %41 = fsub <8 x float> %wide.load2.2, %wide.load.2
  %42 = bitcast float* %39 to <8 x float>*
  store <8 x float> %41, <8 x float>* %42, align 4, !tbaa !12, !llvm.access.group !16
  %43 = or i64 %mul.i.i, 24
  %44 = trunc i64 %43 to i32
  %45 = shl i64 %43, 32
  %46 = ashr exact i64 %45, 32
  %47 = getelementptr inbounds float, float* %0, i64 %46
  %48 = bitcast float* %47 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %48, align 4, !tbaa !12, !llvm.access.group !16
  %49 = add nsw i32 %mul.i, %44
  %50 = sext i32 %49 to i64
  %51 = getelementptr inbounds float, float* %1, i64 %50
  %52 = bitcast float* %51 to <8 x float>*
  %wide.load2.3 = load <8 x float>, <8 x float>* %52, align 4, !tbaa !12, !llvm.access.group !16
  %53 = fsub <8 x float> %wide.load2.3, %wide.load.3
  %54 = bitcast float* %51 to <8 x float>*
  store <8 x float> %53, <8 x float>* %54, align 4, !tbaa !12, !llvm.access.group !16
  %55 = trunc i64 %mul3.i.i to i32
  %conv2.i.1 = or i32 %55, 1
  %mul.i.1 = mul nsw i32 %conv2.i.1, %2
  %56 = trunc i64 %mul.i.i to i32
  %57 = shl i64 %5, 37
  %58 = ashr exact i64 %57, 32
  %59 = getelementptr inbounds float, float* %0, i64 %58
  %60 = bitcast float* %59 to <8 x float>*
  %wide.load11 = load <8 x float>, <8 x float>* %60, align 4, !tbaa !12, !llvm.access.group !16
  %61 = add nsw i32 %mul.i.1, %56
  %62 = sext i32 %61 to i64
  %63 = getelementptr inbounds float, float* %1, i64 %62
  %64 = bitcast float* %63 to <8 x float>*
  %wide.load12 = load <8 x float>, <8 x float>* %64, align 4, !tbaa !12, !llvm.access.group !16
  %65 = fsub <8 x float> %wide.load12, %wide.load11
  %66 = bitcast float* %63 to <8 x float>*
  store <8 x float> %65, <8 x float>* %66, align 4, !tbaa !12, !llvm.access.group !16
  %67 = or i64 %mul.i.i, 8
  %68 = trunc i64 %67 to i32
  %69 = shl i64 %67, 32
  %70 = ashr exact i64 %69, 32
  %71 = getelementptr inbounds float, float* %0, i64 %70
  %72 = bitcast float* %71 to <8 x float>*
  %wide.load11.1 = load <8 x float>, <8 x float>* %72, align 4, !tbaa !12, !llvm.access.group !16
  %73 = add nsw i32 %mul.i.1, %68
  %74 = sext i32 %73 to i64
  %75 = getelementptr inbounds float, float* %1, i64 %74
  %76 = bitcast float* %75 to <8 x float>*
  %wide.load12.1 = load <8 x float>, <8 x float>* %76, align 4, !tbaa !12, !llvm.access.group !16
  %77 = fsub <8 x float> %wide.load12.1, %wide.load11.1
  %78 = bitcast float* %75 to <8 x float>*
  store <8 x float> %77, <8 x float>* %78, align 4, !tbaa !12, !llvm.access.group !16
  %79 = or i64 %mul.i.i, 16
  %80 = trunc i64 %79 to i32
  %81 = shl i64 %79, 32
  %82 = ashr exact i64 %81, 32
  %83 = getelementptr inbounds float, float* %0, i64 %82
  %84 = bitcast float* %83 to <8 x float>*
  %wide.load11.2 = load <8 x float>, <8 x float>* %84, align 4, !tbaa !12, !llvm.access.group !16
  %85 = add nsw i32 %mul.i.1, %80
  %86 = sext i32 %85 to i64
  %87 = getelementptr inbounds float, float* %1, i64 %86
  %88 = bitcast float* %87 to <8 x float>*
  %wide.load12.2 = load <8 x float>, <8 x float>* %88, align 4, !tbaa !12, !llvm.access.group !16
  %89 = fsub <8 x float> %wide.load12.2, %wide.load11.2
  %90 = bitcast float* %87 to <8 x float>*
  store <8 x float> %89, <8 x float>* %90, align 4, !tbaa !12, !llvm.access.group !16
  %91 = or i64 %mul.i.i, 24
  %92 = trunc i64 %91 to i32
  %93 = shl i64 %91, 32
  %94 = ashr exact i64 %93, 32
  %95 = getelementptr inbounds float, float* %0, i64 %94
  %96 = bitcast float* %95 to <8 x float>*
  %wide.load11.3 = load <8 x float>, <8 x float>* %96, align 4, !tbaa !12, !llvm.access.group !16
  %97 = add nsw i32 %mul.i.1, %92
  %98 = sext i32 %97 to i64
  %99 = getelementptr inbounds float, float* %1, i64 %98
  %100 = bitcast float* %99 to <8 x float>*
  %wide.load12.3 = load <8 x float>, <8 x float>* %100, align 4, !tbaa !12, !llvm.access.group !16
  %101 = fsub <8 x float> %wide.load12.3, %wide.load11.3
  %102 = bitcast float* %99 to <8 x float>*
  store <8 x float> %101, <8 x float>* %102, align 4, !tbaa !12, !llvm.access.group !16
  %103 = trunc i64 %mul3.i.i to i32
  %conv2.i.2 = or i32 %103, 2
  %mul.i.2 = mul nsw i32 %conv2.i.2, %2
  %104 = trunc i64 %mul.i.i to i32
  %105 = shl i64 %5, 37
  %106 = ashr exact i64 %105, 32
  %107 = getelementptr inbounds float, float* %0, i64 %106
  %108 = bitcast float* %107 to <8 x float>*
  %wide.load21 = load <8 x float>, <8 x float>* %108, align 4, !tbaa !12, !llvm.access.group !16
  %109 = add nsw i32 %mul.i.2, %104
  %110 = sext i32 %109 to i64
  %111 = getelementptr inbounds float, float* %1, i64 %110
  %112 = bitcast float* %111 to <8 x float>*
  %wide.load22 = load <8 x float>, <8 x float>* %112, align 4, !tbaa !12, !llvm.access.group !16
  %113 = fsub <8 x float> %wide.load22, %wide.load21
  %114 = bitcast float* %111 to <8 x float>*
  store <8 x float> %113, <8 x float>* %114, align 4, !tbaa !12, !llvm.access.group !16
  %115 = or i64 %mul.i.i, 8
  %116 = trunc i64 %115 to i32
  %117 = shl i64 %115, 32
  %118 = ashr exact i64 %117, 32
  %119 = getelementptr inbounds float, float* %0, i64 %118
  %120 = bitcast float* %119 to <8 x float>*
  %wide.load21.1 = load <8 x float>, <8 x float>* %120, align 4, !tbaa !12, !llvm.access.group !16
  %121 = add nsw i32 %mul.i.2, %116
  %122 = sext i32 %121 to i64
  %123 = getelementptr inbounds float, float* %1, i64 %122
  %124 = bitcast float* %123 to <8 x float>*
  %wide.load22.1 = load <8 x float>, <8 x float>* %124, align 4, !tbaa !12, !llvm.access.group !16
  %125 = fsub <8 x float> %wide.load22.1, %wide.load21.1
  %126 = bitcast float* %123 to <8 x float>*
  store <8 x float> %125, <8 x float>* %126, align 4, !tbaa !12, !llvm.access.group !16
  %127 = or i64 %mul.i.i, 16
  %128 = trunc i64 %127 to i32
  %129 = shl i64 %127, 32
  %130 = ashr exact i64 %129, 32
  %131 = getelementptr inbounds float, float* %0, i64 %130
  %132 = bitcast float* %131 to <8 x float>*
  %wide.load21.2 = load <8 x float>, <8 x float>* %132, align 4, !tbaa !12, !llvm.access.group !16
  %133 = add nsw i32 %mul.i.2, %128
  %134 = sext i32 %133 to i64
  %135 = getelementptr inbounds float, float* %1, i64 %134
  %136 = bitcast float* %135 to <8 x float>*
  %wide.load22.2 = load <8 x float>, <8 x float>* %136, align 4, !tbaa !12, !llvm.access.group !16
  %137 = fsub <8 x float> %wide.load22.2, %wide.load21.2
  %138 = bitcast float* %135 to <8 x float>*
  store <8 x float> %137, <8 x float>* %138, align 4, !tbaa !12, !llvm.access.group !16
  %139 = or i64 %mul.i.i, 24
  %140 = trunc i64 %139 to i32
  %141 = shl i64 %139, 32
  %142 = ashr exact i64 %141, 32
  %143 = getelementptr inbounds float, float* %0, i64 %142
  %144 = bitcast float* %143 to <8 x float>*
  %wide.load21.3 = load <8 x float>, <8 x float>* %144, align 4, !tbaa !12, !llvm.access.group !16
  %145 = add nsw i32 %mul.i.2, %140
  %146 = sext i32 %145 to i64
  %147 = getelementptr inbounds float, float* %1, i64 %146
  %148 = bitcast float* %147 to <8 x float>*
  %wide.load22.3 = load <8 x float>, <8 x float>* %148, align 4, !tbaa !12, !llvm.access.group !16
  %149 = fsub <8 x float> %wide.load22.3, %wide.load21.3
  %150 = bitcast float* %147 to <8 x float>*
  store <8 x float> %149, <8 x float>* %150, align 4, !tbaa !12, !llvm.access.group !16
  %151 = trunc i64 %mul3.i.i to i32
  %conv2.i.3 = or i32 %151, 3
  %mul.i.3 = mul nsw i32 %conv2.i.3, %2
  %152 = trunc i64 %mul.i.i to i32
  %153 = shl i64 %5, 37
  %154 = ashr exact i64 %153, 32
  %155 = getelementptr inbounds float, float* %0, i64 %154
  %156 = bitcast float* %155 to <8 x float>*
  %wide.load31 = load <8 x float>, <8 x float>* %156, align 4, !tbaa !12, !llvm.access.group !16
  %157 = add nsw i32 %mul.i.3, %152
  %158 = sext i32 %157 to i64
  %159 = getelementptr inbounds float, float* %1, i64 %158
  %160 = bitcast float* %159 to <8 x float>*
  %wide.load32 = load <8 x float>, <8 x float>* %160, align 4, !tbaa !12, !llvm.access.group !16
  %161 = fsub <8 x float> %wide.load32, %wide.load31
  %162 = bitcast float* %159 to <8 x float>*
  store <8 x float> %161, <8 x float>* %162, align 4, !tbaa !12, !llvm.access.group !16
  %163 = or i64 %mul.i.i, 8
  %164 = trunc i64 %163 to i32
  %165 = shl i64 %163, 32
  %166 = ashr exact i64 %165, 32
  %167 = getelementptr inbounds float, float* %0, i64 %166
  %168 = bitcast float* %167 to <8 x float>*
  %wide.load31.1 = load <8 x float>, <8 x float>* %168, align 4, !tbaa !12, !llvm.access.group !16
  %169 = add nsw i32 %mul.i.3, %164
  %170 = sext i32 %169 to i64
  %171 = getelementptr inbounds float, float* %1, i64 %170
  %172 = bitcast float* %171 to <8 x float>*
  %wide.load32.1 = load <8 x float>, <8 x float>* %172, align 4, !tbaa !12, !llvm.access.group !16
  %173 = fsub <8 x float> %wide.load32.1, %wide.load31.1
  %174 = bitcast float* %171 to <8 x float>*
  store <8 x float> %173, <8 x float>* %174, align 4, !tbaa !12, !llvm.access.group !16
  %175 = or i64 %mul.i.i, 16
  %176 = trunc i64 %175 to i32
  %177 = shl i64 %175, 32
  %178 = ashr exact i64 %177, 32
  %179 = getelementptr inbounds float, float* %0, i64 %178
  %180 = bitcast float* %179 to <8 x float>*
  %wide.load31.2 = load <8 x float>, <8 x float>* %180, align 4, !tbaa !12, !llvm.access.group !16
  %181 = add nsw i32 %mul.i.3, %176
  %182 = sext i32 %181 to i64
  %183 = getelementptr inbounds float, float* %1, i64 %182
  %184 = bitcast float* %183 to <8 x float>*
  %wide.load32.2 = load <8 x float>, <8 x float>* %184, align 4, !tbaa !12, !llvm.access.group !16
  %185 = fsub <8 x float> %wide.load32.2, %wide.load31.2
  %186 = bitcast float* %183 to <8 x float>*
  store <8 x float> %185, <8 x float>* %186, align 4, !tbaa !12, !llvm.access.group !16
  %187 = or i64 %mul.i.i, 24
  %188 = trunc i64 %187 to i32
  %189 = shl i64 %187, 32
  %190 = ashr exact i64 %189, 32
  %191 = getelementptr inbounds float, float* %0, i64 %190
  %192 = bitcast float* %191 to <8 x float>*
  %wide.load31.3 = load <8 x float>, <8 x float>* %192, align 4, !tbaa !12, !llvm.access.group !16
  %193 = add nsw i32 %mul.i.3, %188
  %194 = sext i32 %193 to i64
  %195 = getelementptr inbounds float, float* %1, i64 %194
  %196 = bitcast float* %195 to <8 x float>*
  %wide.load32.3 = load <8 x float>, <8 x float>* %196, align 4, !tbaa !12, !llvm.access.group !16
  %197 = fsub <8 x float> %wide.load32.3, %wide.load31.3
  %198 = bitcast float* %195 to <8 x float>*
  store <8 x float> %197, <8 x float>* %198, align 4, !tbaa !12, !llvm.access.group !16
  %199 = trunc i64 %mul3.i.i to i32
  %conv2.i.4 = or i32 %199, 4
  %mul.i.4 = mul nsw i32 %conv2.i.4, %2
  %200 = trunc i64 %mul.i.i to i32
  %201 = shl i64 %5, 37
  %202 = ashr exact i64 %201, 32
  %203 = getelementptr inbounds float, float* %0, i64 %202
  %204 = bitcast float* %203 to <8 x float>*
  %wide.load41 = load <8 x float>, <8 x float>* %204, align 4, !tbaa !12, !llvm.access.group !16
  %205 = add nsw i32 %mul.i.4, %200
  %206 = sext i32 %205 to i64
  %207 = getelementptr inbounds float, float* %1, i64 %206
  %208 = bitcast float* %207 to <8 x float>*
  %wide.load42 = load <8 x float>, <8 x float>* %208, align 4, !tbaa !12, !llvm.access.group !16
  %209 = fsub <8 x float> %wide.load42, %wide.load41
  %210 = bitcast float* %207 to <8 x float>*
  store <8 x float> %209, <8 x float>* %210, align 4, !tbaa !12, !llvm.access.group !16
  %211 = or i64 %mul.i.i, 8
  %212 = trunc i64 %211 to i32
  %213 = shl i64 %211, 32
  %214 = ashr exact i64 %213, 32
  %215 = getelementptr inbounds float, float* %0, i64 %214
  %216 = bitcast float* %215 to <8 x float>*
  %wide.load41.1 = load <8 x float>, <8 x float>* %216, align 4, !tbaa !12, !llvm.access.group !16
  %217 = add nsw i32 %mul.i.4, %212
  %218 = sext i32 %217 to i64
  %219 = getelementptr inbounds float, float* %1, i64 %218
  %220 = bitcast float* %219 to <8 x float>*
  %wide.load42.1 = load <8 x float>, <8 x float>* %220, align 4, !tbaa !12, !llvm.access.group !16
  %221 = fsub <8 x float> %wide.load42.1, %wide.load41.1
  %222 = bitcast float* %219 to <8 x float>*
  store <8 x float> %221, <8 x float>* %222, align 4, !tbaa !12, !llvm.access.group !16
  %223 = or i64 %mul.i.i, 16
  %224 = trunc i64 %223 to i32
  %225 = shl i64 %223, 32
  %226 = ashr exact i64 %225, 32
  %227 = getelementptr inbounds float, float* %0, i64 %226
  %228 = bitcast float* %227 to <8 x float>*
  %wide.load41.2 = load <8 x float>, <8 x float>* %228, align 4, !tbaa !12, !llvm.access.group !16
  %229 = add nsw i32 %mul.i.4, %224
  %230 = sext i32 %229 to i64
  %231 = getelementptr inbounds float, float* %1, i64 %230
  %232 = bitcast float* %231 to <8 x float>*
  %wide.load42.2 = load <8 x float>, <8 x float>* %232, align 4, !tbaa !12, !llvm.access.group !16
  %233 = fsub <8 x float> %wide.load42.2, %wide.load41.2
  %234 = bitcast float* %231 to <8 x float>*
  store <8 x float> %233, <8 x float>* %234, align 4, !tbaa !12, !llvm.access.group !16
  %235 = or i64 %mul.i.i, 24
  %236 = trunc i64 %235 to i32
  %237 = shl i64 %235, 32
  %238 = ashr exact i64 %237, 32
  %239 = getelementptr inbounds float, float* %0, i64 %238
  %240 = bitcast float* %239 to <8 x float>*
  %wide.load41.3 = load <8 x float>, <8 x float>* %240, align 4, !tbaa !12, !llvm.access.group !16
  %241 = add nsw i32 %mul.i.4, %236
  %242 = sext i32 %241 to i64
  %243 = getelementptr inbounds float, float* %1, i64 %242
  %244 = bitcast float* %243 to <8 x float>*
  %wide.load42.3 = load <8 x float>, <8 x float>* %244, align 4, !tbaa !12, !llvm.access.group !16
  %245 = fsub <8 x float> %wide.load42.3, %wide.load41.3
  %246 = bitcast float* %243 to <8 x float>*
  store <8 x float> %245, <8 x float>* %246, align 4, !tbaa !12, !llvm.access.group !16
  %247 = trunc i64 %mul3.i.i to i32
  %conv2.i.5 = or i32 %247, 5
  %mul.i.5 = mul nsw i32 %conv2.i.5, %2
  %248 = trunc i64 %mul.i.i to i32
  %249 = shl i64 %5, 37
  %250 = ashr exact i64 %249, 32
  %251 = getelementptr inbounds float, float* %0, i64 %250
  %252 = bitcast float* %251 to <8 x float>*
  %wide.load51 = load <8 x float>, <8 x float>* %252, align 4, !tbaa !12, !llvm.access.group !16
  %253 = add nsw i32 %mul.i.5, %248
  %254 = sext i32 %253 to i64
  %255 = getelementptr inbounds float, float* %1, i64 %254
  %256 = bitcast float* %255 to <8 x float>*
  %wide.load52 = load <8 x float>, <8 x float>* %256, align 4, !tbaa !12, !llvm.access.group !16
  %257 = fsub <8 x float> %wide.load52, %wide.load51
  %258 = bitcast float* %255 to <8 x float>*
  store <8 x float> %257, <8 x float>* %258, align 4, !tbaa !12, !llvm.access.group !16
  %259 = or i64 %mul.i.i, 8
  %260 = trunc i64 %259 to i32
  %261 = shl i64 %259, 32
  %262 = ashr exact i64 %261, 32
  %263 = getelementptr inbounds float, float* %0, i64 %262
  %264 = bitcast float* %263 to <8 x float>*
  %wide.load51.1 = load <8 x float>, <8 x float>* %264, align 4, !tbaa !12, !llvm.access.group !16
  %265 = add nsw i32 %mul.i.5, %260
  %266 = sext i32 %265 to i64
  %267 = getelementptr inbounds float, float* %1, i64 %266
  %268 = bitcast float* %267 to <8 x float>*
  %wide.load52.1 = load <8 x float>, <8 x float>* %268, align 4, !tbaa !12, !llvm.access.group !16
  %269 = fsub <8 x float> %wide.load52.1, %wide.load51.1
  %270 = bitcast float* %267 to <8 x float>*
  store <8 x float> %269, <8 x float>* %270, align 4, !tbaa !12, !llvm.access.group !16
  %271 = or i64 %mul.i.i, 16
  %272 = trunc i64 %271 to i32
  %273 = shl i64 %271, 32
  %274 = ashr exact i64 %273, 32
  %275 = getelementptr inbounds float, float* %0, i64 %274
  %276 = bitcast float* %275 to <8 x float>*
  %wide.load51.2 = load <8 x float>, <8 x float>* %276, align 4, !tbaa !12, !llvm.access.group !16
  %277 = add nsw i32 %mul.i.5, %272
  %278 = sext i32 %277 to i64
  %279 = getelementptr inbounds float, float* %1, i64 %278
  %280 = bitcast float* %279 to <8 x float>*
  %wide.load52.2 = load <8 x float>, <8 x float>* %280, align 4, !tbaa !12, !llvm.access.group !16
  %281 = fsub <8 x float> %wide.load52.2, %wide.load51.2
  %282 = bitcast float* %279 to <8 x float>*
  store <8 x float> %281, <8 x float>* %282, align 4, !tbaa !12, !llvm.access.group !16
  %283 = or i64 %mul.i.i, 24
  %284 = trunc i64 %283 to i32
  %285 = shl i64 %283, 32
  %286 = ashr exact i64 %285, 32
  %287 = getelementptr inbounds float, float* %0, i64 %286
  %288 = bitcast float* %287 to <8 x float>*
  %wide.load51.3 = load <8 x float>, <8 x float>* %288, align 4, !tbaa !12, !llvm.access.group !16
  %289 = add nsw i32 %mul.i.5, %284
  %290 = sext i32 %289 to i64
  %291 = getelementptr inbounds float, float* %1, i64 %290
  %292 = bitcast float* %291 to <8 x float>*
  %wide.load52.3 = load <8 x float>, <8 x float>* %292, align 4, !tbaa !12, !llvm.access.group !16
  %293 = fsub <8 x float> %wide.load52.3, %wide.load51.3
  %294 = bitcast float* %291 to <8 x float>*
  store <8 x float> %293, <8 x float>* %294, align 4, !tbaa !12, !llvm.access.group !16
  %295 = trunc i64 %mul3.i.i to i32
  %conv2.i.6 = or i32 %295, 6
  %mul.i.6 = mul nsw i32 %conv2.i.6, %2
  %296 = trunc i64 %mul.i.i to i32
  %297 = shl i64 %5, 37
  %298 = ashr exact i64 %297, 32
  %299 = getelementptr inbounds float, float* %0, i64 %298
  %300 = bitcast float* %299 to <8 x float>*
  %wide.load61 = load <8 x float>, <8 x float>* %300, align 4, !tbaa !12, !llvm.access.group !16
  %301 = add nsw i32 %mul.i.6, %296
  %302 = sext i32 %301 to i64
  %303 = getelementptr inbounds float, float* %1, i64 %302
  %304 = bitcast float* %303 to <8 x float>*
  %wide.load62 = load <8 x float>, <8 x float>* %304, align 4, !tbaa !12, !llvm.access.group !16
  %305 = fsub <8 x float> %wide.load62, %wide.load61
  %306 = bitcast float* %303 to <8 x float>*
  store <8 x float> %305, <8 x float>* %306, align 4, !tbaa !12, !llvm.access.group !16
  %307 = or i64 %mul.i.i, 8
  %308 = trunc i64 %307 to i32
  %309 = shl i64 %307, 32
  %310 = ashr exact i64 %309, 32
  %311 = getelementptr inbounds float, float* %0, i64 %310
  %312 = bitcast float* %311 to <8 x float>*
  %wide.load61.1 = load <8 x float>, <8 x float>* %312, align 4, !tbaa !12, !llvm.access.group !16
  %313 = add nsw i32 %mul.i.6, %308
  %314 = sext i32 %313 to i64
  %315 = getelementptr inbounds float, float* %1, i64 %314
  %316 = bitcast float* %315 to <8 x float>*
  %wide.load62.1 = load <8 x float>, <8 x float>* %316, align 4, !tbaa !12, !llvm.access.group !16
  %317 = fsub <8 x float> %wide.load62.1, %wide.load61.1
  %318 = bitcast float* %315 to <8 x float>*
  store <8 x float> %317, <8 x float>* %318, align 4, !tbaa !12, !llvm.access.group !16
  %319 = or i64 %mul.i.i, 16
  %320 = trunc i64 %319 to i32
  %321 = shl i64 %319, 32
  %322 = ashr exact i64 %321, 32
  %323 = getelementptr inbounds float, float* %0, i64 %322
  %324 = bitcast float* %323 to <8 x float>*
  %wide.load61.2 = load <8 x float>, <8 x float>* %324, align 4, !tbaa !12, !llvm.access.group !16
  %325 = add nsw i32 %mul.i.6, %320
  %326 = sext i32 %325 to i64
  %327 = getelementptr inbounds float, float* %1, i64 %326
  %328 = bitcast float* %327 to <8 x float>*
  %wide.load62.2 = load <8 x float>, <8 x float>* %328, align 4, !tbaa !12, !llvm.access.group !16
  %329 = fsub <8 x float> %wide.load62.2, %wide.load61.2
  %330 = bitcast float* %327 to <8 x float>*
  store <8 x float> %329, <8 x float>* %330, align 4, !tbaa !12, !llvm.access.group !16
  %331 = or i64 %mul.i.i, 24
  %332 = trunc i64 %331 to i32
  %333 = shl i64 %331, 32
  %334 = ashr exact i64 %333, 32
  %335 = getelementptr inbounds float, float* %0, i64 %334
  %336 = bitcast float* %335 to <8 x float>*
  %wide.load61.3 = load <8 x float>, <8 x float>* %336, align 4, !tbaa !12, !llvm.access.group !16
  %337 = add nsw i32 %mul.i.6, %332
  %338 = sext i32 %337 to i64
  %339 = getelementptr inbounds float, float* %1, i64 %338
  %340 = bitcast float* %339 to <8 x float>*
  %wide.load62.3 = load <8 x float>, <8 x float>* %340, align 4, !tbaa !12, !llvm.access.group !16
  %341 = fsub <8 x float> %wide.load62.3, %wide.load61.3
  %342 = bitcast float* %339 to <8 x float>*
  store <8 x float> %341, <8 x float>* %342, align 4, !tbaa !12, !llvm.access.group !16
  %343 = trunc i64 %mul3.i.i to i32
  %conv2.i.7 = or i32 %343, 7
  %mul.i.7 = mul nsw i32 %conv2.i.7, %2
  %344 = trunc i64 %mul.i.i to i32
  %345 = shl i64 %5, 37
  %346 = ashr exact i64 %345, 32
  %347 = getelementptr inbounds float, float* %0, i64 %346
  %348 = bitcast float* %347 to <8 x float>*
  %wide.load71 = load <8 x float>, <8 x float>* %348, align 4, !tbaa !12, !llvm.access.group !16
  %349 = add nsw i32 %mul.i.7, %344
  %350 = sext i32 %349 to i64
  %351 = getelementptr inbounds float, float* %1, i64 %350
  %352 = bitcast float* %351 to <8 x float>*
  %wide.load72 = load <8 x float>, <8 x float>* %352, align 4, !tbaa !12, !llvm.access.group !16
  %353 = fsub <8 x float> %wide.load72, %wide.load71
  %354 = bitcast float* %351 to <8 x float>*
  store <8 x float> %353, <8 x float>* %354, align 4, !tbaa !12, !llvm.access.group !16
  %355 = or i64 %mul.i.i, 8
  %356 = trunc i64 %355 to i32
  %357 = shl i64 %355, 32
  %358 = ashr exact i64 %357, 32
  %359 = getelementptr inbounds float, float* %0, i64 %358
  %360 = bitcast float* %359 to <8 x float>*
  %wide.load71.1 = load <8 x float>, <8 x float>* %360, align 4, !tbaa !12, !llvm.access.group !16
  %361 = add nsw i32 %mul.i.7, %356
  %362 = sext i32 %361 to i64
  %363 = getelementptr inbounds float, float* %1, i64 %362
  %364 = bitcast float* %363 to <8 x float>*
  %wide.load72.1 = load <8 x float>, <8 x float>* %364, align 4, !tbaa !12, !llvm.access.group !16
  %365 = fsub <8 x float> %wide.load72.1, %wide.load71.1
  %366 = bitcast float* %363 to <8 x float>*
  store <8 x float> %365, <8 x float>* %366, align 4, !tbaa !12, !llvm.access.group !16
  %367 = or i64 %mul.i.i, 16
  %368 = trunc i64 %367 to i32
  %369 = shl i64 %367, 32
  %370 = ashr exact i64 %369, 32
  %371 = getelementptr inbounds float, float* %0, i64 %370
  %372 = bitcast float* %371 to <8 x float>*
  %wide.load71.2 = load <8 x float>, <8 x float>* %372, align 4, !tbaa !12, !llvm.access.group !16
  %373 = add nsw i32 %mul.i.7, %368
  %374 = sext i32 %373 to i64
  %375 = getelementptr inbounds float, float* %1, i64 %374
  %376 = bitcast float* %375 to <8 x float>*
  %wide.load72.2 = load <8 x float>, <8 x float>* %376, align 4, !tbaa !12, !llvm.access.group !16
  %377 = fsub <8 x float> %wide.load72.2, %wide.load71.2
  %378 = bitcast float* %375 to <8 x float>*
  store <8 x float> %377, <8 x float>* %378, align 4, !tbaa !12, !llvm.access.group !16
  %379 = or i64 %mul.i.i, 24
  %380 = trunc i64 %379 to i32
  %381 = shl i64 %379, 32
  %382 = ashr exact i64 %381, 32
  %383 = getelementptr inbounds float, float* %0, i64 %382
  %384 = bitcast float* %383 to <8 x float>*
  %wide.load71.3 = load <8 x float>, <8 x float>* %384, align 4, !tbaa !12, !llvm.access.group !16
  %385 = add nsw i32 %mul.i.7, %380
  %386 = sext i32 %385 to i64
  %387 = getelementptr inbounds float, float* %1, i64 %386
  %388 = bitcast float* %387 to <8 x float>*
  %wide.load72.3 = load <8 x float>, <8 x float>* %388, align 4, !tbaa !12, !llvm.access.group !16
  %389 = fsub <8 x float> %wide.load72.3, %wide.load71.3
  %390 = bitcast float* %387 to <8 x float>*
  store <8 x float> %389, <8 x float>* %390, align 4, !tbaa !12, !llvm.access.group !16
  ret void
}

; Function Attrs: nofree norecurse nounwind
define void @_pocl_kernel_reduce_kernel_workgroup(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #1 {
pregion_for_entry.pregion_for_init.i.i:
  %5 = bitcast i8** %0 to float***
  %6 = load float**, float*** %5, align 8
  %7 = load float*, float** %6, align 8
  %8 = getelementptr i8*, i8** %0, i64 1
  %9 = bitcast i8** %8 to float***
  %10 = load float**, float*** %9, align 8
  %11 = load float*, float** %10, align 8
  %12 = getelementptr i8*, i8** %0, i64 2
  %13 = bitcast i8** %12 to i32**
  %14 = load i32*, i32** %13, align 8
  %15 = load i32, i32* %14, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %conv2.i.i = trunc i64 %mul3.i.i.i to i32
  %mul.i.i = mul nsw i32 %15, %conv2.i.i
  %16 = trunc i64 %mul.i.i.i to i32
  %17 = shl i64 %2, 37
  %18 = ashr exact i64 %17, 32
  %19 = getelementptr inbounds float, float* %7, i64 %18
  %20 = bitcast float* %19 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %20, align 4, !tbaa !12, !llvm.access.group !16
  %21 = add nsw i32 %mul.i.i, %16
  %22 = sext i32 %21 to i64
  %23 = getelementptr inbounds float, float* %11, i64 %22
  %24 = bitcast float* %23 to <8 x float>*
  %wide.load2 = load <8 x float>, <8 x float>* %24, align 4, !tbaa !12, !llvm.access.group !16
  %25 = fsub <8 x float> %wide.load2, %wide.load
  %26 = bitcast float* %23 to <8 x float>*
  store <8 x float> %25, <8 x float>* %26, align 4, !tbaa !12, !llvm.access.group !16
  %27 = or i64 %mul.i.i.i, 8
  %28 = trunc i64 %27 to i32
  %29 = shl i64 %27, 32
  %30 = ashr exact i64 %29, 32
  %31 = getelementptr inbounds float, float* %7, i64 %30
  %32 = bitcast float* %31 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %32, align 4, !tbaa !12, !llvm.access.group !16
  %33 = add nsw i32 %mul.i.i, %28
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds float, float* %11, i64 %34
  %36 = bitcast float* %35 to <8 x float>*
  %wide.load2.1 = load <8 x float>, <8 x float>* %36, align 4, !tbaa !12, !llvm.access.group !16
  %37 = fsub <8 x float> %wide.load2.1, %wide.load.1
  %38 = bitcast float* %35 to <8 x float>*
  store <8 x float> %37, <8 x float>* %38, align 4, !tbaa !12, !llvm.access.group !16
  %39 = or i64 %mul.i.i.i, 16
  %40 = trunc i64 %39 to i32
  %41 = shl i64 %39, 32
  %42 = ashr exact i64 %41, 32
  %43 = getelementptr inbounds float, float* %7, i64 %42
  %44 = bitcast float* %43 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %44, align 4, !tbaa !12, !llvm.access.group !16
  %45 = add nsw i32 %mul.i.i, %40
  %46 = sext i32 %45 to i64
  %47 = getelementptr inbounds float, float* %11, i64 %46
  %48 = bitcast float* %47 to <8 x float>*
  %wide.load2.2 = load <8 x float>, <8 x float>* %48, align 4, !tbaa !12, !llvm.access.group !16
  %49 = fsub <8 x float> %wide.load2.2, %wide.load.2
  %50 = bitcast float* %47 to <8 x float>*
  store <8 x float> %49, <8 x float>* %50, align 4, !tbaa !12, !llvm.access.group !16
  %51 = or i64 %mul.i.i.i, 24
  %52 = trunc i64 %51 to i32
  %53 = shl i64 %51, 32
  %54 = ashr exact i64 %53, 32
  %55 = getelementptr inbounds float, float* %7, i64 %54
  %56 = bitcast float* %55 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %56, align 4, !tbaa !12, !llvm.access.group !16
  %57 = add nsw i32 %mul.i.i, %52
  %58 = sext i32 %57 to i64
  %59 = getelementptr inbounds float, float* %11, i64 %58
  %60 = bitcast float* %59 to <8 x float>*
  %wide.load2.3 = load <8 x float>, <8 x float>* %60, align 4, !tbaa !12, !llvm.access.group !16
  %61 = fsub <8 x float> %wide.load2.3, %wide.load.3
  %62 = bitcast float* %59 to <8 x float>*
  store <8 x float> %61, <8 x float>* %62, align 4, !tbaa !12, !llvm.access.group !16
  %63 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.1 = or i32 %63, 1
  %mul.i.i.1 = mul nsw i32 %15, %conv2.i.i.1
  %64 = trunc i64 %mul.i.i.i to i32
  %65 = shl i64 %2, 37
  %66 = ashr exact i64 %65, 32
  %67 = getelementptr inbounds float, float* %7, i64 %66
  %68 = bitcast float* %67 to <8 x float>*
  %wide.load11 = load <8 x float>, <8 x float>* %68, align 4, !tbaa !12, !llvm.access.group !16
  %69 = add nsw i32 %mul.i.i.1, %64
  %70 = sext i32 %69 to i64
  %71 = getelementptr inbounds float, float* %11, i64 %70
  %72 = bitcast float* %71 to <8 x float>*
  %wide.load12 = load <8 x float>, <8 x float>* %72, align 4, !tbaa !12, !llvm.access.group !16
  %73 = fsub <8 x float> %wide.load12, %wide.load11
  %74 = bitcast float* %71 to <8 x float>*
  store <8 x float> %73, <8 x float>* %74, align 4, !tbaa !12, !llvm.access.group !16
  %75 = or i64 %mul.i.i.i, 8
  %76 = trunc i64 %75 to i32
  %77 = shl i64 %75, 32
  %78 = ashr exact i64 %77, 32
  %79 = getelementptr inbounds float, float* %7, i64 %78
  %80 = bitcast float* %79 to <8 x float>*
  %wide.load11.1 = load <8 x float>, <8 x float>* %80, align 4, !tbaa !12, !llvm.access.group !16
  %81 = add nsw i32 %mul.i.i.1, %76
  %82 = sext i32 %81 to i64
  %83 = getelementptr inbounds float, float* %11, i64 %82
  %84 = bitcast float* %83 to <8 x float>*
  %wide.load12.1 = load <8 x float>, <8 x float>* %84, align 4, !tbaa !12, !llvm.access.group !16
  %85 = fsub <8 x float> %wide.load12.1, %wide.load11.1
  %86 = bitcast float* %83 to <8 x float>*
  store <8 x float> %85, <8 x float>* %86, align 4, !tbaa !12, !llvm.access.group !16
  %87 = or i64 %mul.i.i.i, 16
  %88 = trunc i64 %87 to i32
  %89 = shl i64 %87, 32
  %90 = ashr exact i64 %89, 32
  %91 = getelementptr inbounds float, float* %7, i64 %90
  %92 = bitcast float* %91 to <8 x float>*
  %wide.load11.2 = load <8 x float>, <8 x float>* %92, align 4, !tbaa !12, !llvm.access.group !16
  %93 = add nsw i32 %mul.i.i.1, %88
  %94 = sext i32 %93 to i64
  %95 = getelementptr inbounds float, float* %11, i64 %94
  %96 = bitcast float* %95 to <8 x float>*
  %wide.load12.2 = load <8 x float>, <8 x float>* %96, align 4, !tbaa !12, !llvm.access.group !16
  %97 = fsub <8 x float> %wide.load12.2, %wide.load11.2
  %98 = bitcast float* %95 to <8 x float>*
  store <8 x float> %97, <8 x float>* %98, align 4, !tbaa !12, !llvm.access.group !16
  %99 = or i64 %mul.i.i.i, 24
  %100 = trunc i64 %99 to i32
  %101 = shl i64 %99, 32
  %102 = ashr exact i64 %101, 32
  %103 = getelementptr inbounds float, float* %7, i64 %102
  %104 = bitcast float* %103 to <8 x float>*
  %wide.load11.3 = load <8 x float>, <8 x float>* %104, align 4, !tbaa !12, !llvm.access.group !16
  %105 = add nsw i32 %mul.i.i.1, %100
  %106 = sext i32 %105 to i64
  %107 = getelementptr inbounds float, float* %11, i64 %106
  %108 = bitcast float* %107 to <8 x float>*
  %wide.load12.3 = load <8 x float>, <8 x float>* %108, align 4, !tbaa !12, !llvm.access.group !16
  %109 = fsub <8 x float> %wide.load12.3, %wide.load11.3
  %110 = bitcast float* %107 to <8 x float>*
  store <8 x float> %109, <8 x float>* %110, align 4, !tbaa !12, !llvm.access.group !16
  %111 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.2 = or i32 %111, 2
  %mul.i.i.2 = mul nsw i32 %15, %conv2.i.i.2
  %112 = trunc i64 %mul.i.i.i to i32
  %113 = shl i64 %2, 37
  %114 = ashr exact i64 %113, 32
  %115 = getelementptr inbounds float, float* %7, i64 %114
  %116 = bitcast float* %115 to <8 x float>*
  %wide.load21 = load <8 x float>, <8 x float>* %116, align 4, !tbaa !12, !llvm.access.group !16
  %117 = add nsw i32 %mul.i.i.2, %112
  %118 = sext i32 %117 to i64
  %119 = getelementptr inbounds float, float* %11, i64 %118
  %120 = bitcast float* %119 to <8 x float>*
  %wide.load22 = load <8 x float>, <8 x float>* %120, align 4, !tbaa !12, !llvm.access.group !16
  %121 = fsub <8 x float> %wide.load22, %wide.load21
  %122 = bitcast float* %119 to <8 x float>*
  store <8 x float> %121, <8 x float>* %122, align 4, !tbaa !12, !llvm.access.group !16
  %123 = or i64 %mul.i.i.i, 8
  %124 = trunc i64 %123 to i32
  %125 = shl i64 %123, 32
  %126 = ashr exact i64 %125, 32
  %127 = getelementptr inbounds float, float* %7, i64 %126
  %128 = bitcast float* %127 to <8 x float>*
  %wide.load21.1 = load <8 x float>, <8 x float>* %128, align 4, !tbaa !12, !llvm.access.group !16
  %129 = add nsw i32 %mul.i.i.2, %124
  %130 = sext i32 %129 to i64
  %131 = getelementptr inbounds float, float* %11, i64 %130
  %132 = bitcast float* %131 to <8 x float>*
  %wide.load22.1 = load <8 x float>, <8 x float>* %132, align 4, !tbaa !12, !llvm.access.group !16
  %133 = fsub <8 x float> %wide.load22.1, %wide.load21.1
  %134 = bitcast float* %131 to <8 x float>*
  store <8 x float> %133, <8 x float>* %134, align 4, !tbaa !12, !llvm.access.group !16
  %135 = or i64 %mul.i.i.i, 16
  %136 = trunc i64 %135 to i32
  %137 = shl i64 %135, 32
  %138 = ashr exact i64 %137, 32
  %139 = getelementptr inbounds float, float* %7, i64 %138
  %140 = bitcast float* %139 to <8 x float>*
  %wide.load21.2 = load <8 x float>, <8 x float>* %140, align 4, !tbaa !12, !llvm.access.group !16
  %141 = add nsw i32 %mul.i.i.2, %136
  %142 = sext i32 %141 to i64
  %143 = getelementptr inbounds float, float* %11, i64 %142
  %144 = bitcast float* %143 to <8 x float>*
  %wide.load22.2 = load <8 x float>, <8 x float>* %144, align 4, !tbaa !12, !llvm.access.group !16
  %145 = fsub <8 x float> %wide.load22.2, %wide.load21.2
  %146 = bitcast float* %143 to <8 x float>*
  store <8 x float> %145, <8 x float>* %146, align 4, !tbaa !12, !llvm.access.group !16
  %147 = or i64 %mul.i.i.i, 24
  %148 = trunc i64 %147 to i32
  %149 = shl i64 %147, 32
  %150 = ashr exact i64 %149, 32
  %151 = getelementptr inbounds float, float* %7, i64 %150
  %152 = bitcast float* %151 to <8 x float>*
  %wide.load21.3 = load <8 x float>, <8 x float>* %152, align 4, !tbaa !12, !llvm.access.group !16
  %153 = add nsw i32 %mul.i.i.2, %148
  %154 = sext i32 %153 to i64
  %155 = getelementptr inbounds float, float* %11, i64 %154
  %156 = bitcast float* %155 to <8 x float>*
  %wide.load22.3 = load <8 x float>, <8 x float>* %156, align 4, !tbaa !12, !llvm.access.group !16
  %157 = fsub <8 x float> %wide.load22.3, %wide.load21.3
  %158 = bitcast float* %155 to <8 x float>*
  store <8 x float> %157, <8 x float>* %158, align 4, !tbaa !12, !llvm.access.group !16
  %159 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.3 = or i32 %159, 3
  %mul.i.i.3 = mul nsw i32 %15, %conv2.i.i.3
  %160 = trunc i64 %mul.i.i.i to i32
  %161 = shl i64 %2, 37
  %162 = ashr exact i64 %161, 32
  %163 = getelementptr inbounds float, float* %7, i64 %162
  %164 = bitcast float* %163 to <8 x float>*
  %wide.load31 = load <8 x float>, <8 x float>* %164, align 4, !tbaa !12, !llvm.access.group !16
  %165 = add nsw i32 %mul.i.i.3, %160
  %166 = sext i32 %165 to i64
  %167 = getelementptr inbounds float, float* %11, i64 %166
  %168 = bitcast float* %167 to <8 x float>*
  %wide.load32 = load <8 x float>, <8 x float>* %168, align 4, !tbaa !12, !llvm.access.group !16
  %169 = fsub <8 x float> %wide.load32, %wide.load31
  %170 = bitcast float* %167 to <8 x float>*
  store <8 x float> %169, <8 x float>* %170, align 4, !tbaa !12, !llvm.access.group !16
  %171 = or i64 %mul.i.i.i, 8
  %172 = trunc i64 %171 to i32
  %173 = shl i64 %171, 32
  %174 = ashr exact i64 %173, 32
  %175 = getelementptr inbounds float, float* %7, i64 %174
  %176 = bitcast float* %175 to <8 x float>*
  %wide.load31.1 = load <8 x float>, <8 x float>* %176, align 4, !tbaa !12, !llvm.access.group !16
  %177 = add nsw i32 %mul.i.i.3, %172
  %178 = sext i32 %177 to i64
  %179 = getelementptr inbounds float, float* %11, i64 %178
  %180 = bitcast float* %179 to <8 x float>*
  %wide.load32.1 = load <8 x float>, <8 x float>* %180, align 4, !tbaa !12, !llvm.access.group !16
  %181 = fsub <8 x float> %wide.load32.1, %wide.load31.1
  %182 = bitcast float* %179 to <8 x float>*
  store <8 x float> %181, <8 x float>* %182, align 4, !tbaa !12, !llvm.access.group !16
  %183 = or i64 %mul.i.i.i, 16
  %184 = trunc i64 %183 to i32
  %185 = shl i64 %183, 32
  %186 = ashr exact i64 %185, 32
  %187 = getelementptr inbounds float, float* %7, i64 %186
  %188 = bitcast float* %187 to <8 x float>*
  %wide.load31.2 = load <8 x float>, <8 x float>* %188, align 4, !tbaa !12, !llvm.access.group !16
  %189 = add nsw i32 %mul.i.i.3, %184
  %190 = sext i32 %189 to i64
  %191 = getelementptr inbounds float, float* %11, i64 %190
  %192 = bitcast float* %191 to <8 x float>*
  %wide.load32.2 = load <8 x float>, <8 x float>* %192, align 4, !tbaa !12, !llvm.access.group !16
  %193 = fsub <8 x float> %wide.load32.2, %wide.load31.2
  %194 = bitcast float* %191 to <8 x float>*
  store <8 x float> %193, <8 x float>* %194, align 4, !tbaa !12, !llvm.access.group !16
  %195 = or i64 %mul.i.i.i, 24
  %196 = trunc i64 %195 to i32
  %197 = shl i64 %195, 32
  %198 = ashr exact i64 %197, 32
  %199 = getelementptr inbounds float, float* %7, i64 %198
  %200 = bitcast float* %199 to <8 x float>*
  %wide.load31.3 = load <8 x float>, <8 x float>* %200, align 4, !tbaa !12, !llvm.access.group !16
  %201 = add nsw i32 %mul.i.i.3, %196
  %202 = sext i32 %201 to i64
  %203 = getelementptr inbounds float, float* %11, i64 %202
  %204 = bitcast float* %203 to <8 x float>*
  %wide.load32.3 = load <8 x float>, <8 x float>* %204, align 4, !tbaa !12, !llvm.access.group !16
  %205 = fsub <8 x float> %wide.load32.3, %wide.load31.3
  %206 = bitcast float* %203 to <8 x float>*
  store <8 x float> %205, <8 x float>* %206, align 4, !tbaa !12, !llvm.access.group !16
  %207 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.4 = or i32 %207, 4
  %mul.i.i.4 = mul nsw i32 %15, %conv2.i.i.4
  %208 = trunc i64 %mul.i.i.i to i32
  %209 = shl i64 %2, 37
  %210 = ashr exact i64 %209, 32
  %211 = getelementptr inbounds float, float* %7, i64 %210
  %212 = bitcast float* %211 to <8 x float>*
  %wide.load41 = load <8 x float>, <8 x float>* %212, align 4, !tbaa !12, !llvm.access.group !16
  %213 = add nsw i32 %mul.i.i.4, %208
  %214 = sext i32 %213 to i64
  %215 = getelementptr inbounds float, float* %11, i64 %214
  %216 = bitcast float* %215 to <8 x float>*
  %wide.load42 = load <8 x float>, <8 x float>* %216, align 4, !tbaa !12, !llvm.access.group !16
  %217 = fsub <8 x float> %wide.load42, %wide.load41
  %218 = bitcast float* %215 to <8 x float>*
  store <8 x float> %217, <8 x float>* %218, align 4, !tbaa !12, !llvm.access.group !16
  %219 = or i64 %mul.i.i.i, 8
  %220 = trunc i64 %219 to i32
  %221 = shl i64 %219, 32
  %222 = ashr exact i64 %221, 32
  %223 = getelementptr inbounds float, float* %7, i64 %222
  %224 = bitcast float* %223 to <8 x float>*
  %wide.load41.1 = load <8 x float>, <8 x float>* %224, align 4, !tbaa !12, !llvm.access.group !16
  %225 = add nsw i32 %mul.i.i.4, %220
  %226 = sext i32 %225 to i64
  %227 = getelementptr inbounds float, float* %11, i64 %226
  %228 = bitcast float* %227 to <8 x float>*
  %wide.load42.1 = load <8 x float>, <8 x float>* %228, align 4, !tbaa !12, !llvm.access.group !16
  %229 = fsub <8 x float> %wide.load42.1, %wide.load41.1
  %230 = bitcast float* %227 to <8 x float>*
  store <8 x float> %229, <8 x float>* %230, align 4, !tbaa !12, !llvm.access.group !16
  %231 = or i64 %mul.i.i.i, 16
  %232 = trunc i64 %231 to i32
  %233 = shl i64 %231, 32
  %234 = ashr exact i64 %233, 32
  %235 = getelementptr inbounds float, float* %7, i64 %234
  %236 = bitcast float* %235 to <8 x float>*
  %wide.load41.2 = load <8 x float>, <8 x float>* %236, align 4, !tbaa !12, !llvm.access.group !16
  %237 = add nsw i32 %mul.i.i.4, %232
  %238 = sext i32 %237 to i64
  %239 = getelementptr inbounds float, float* %11, i64 %238
  %240 = bitcast float* %239 to <8 x float>*
  %wide.load42.2 = load <8 x float>, <8 x float>* %240, align 4, !tbaa !12, !llvm.access.group !16
  %241 = fsub <8 x float> %wide.load42.2, %wide.load41.2
  %242 = bitcast float* %239 to <8 x float>*
  store <8 x float> %241, <8 x float>* %242, align 4, !tbaa !12, !llvm.access.group !16
  %243 = or i64 %mul.i.i.i, 24
  %244 = trunc i64 %243 to i32
  %245 = shl i64 %243, 32
  %246 = ashr exact i64 %245, 32
  %247 = getelementptr inbounds float, float* %7, i64 %246
  %248 = bitcast float* %247 to <8 x float>*
  %wide.load41.3 = load <8 x float>, <8 x float>* %248, align 4, !tbaa !12, !llvm.access.group !16
  %249 = add nsw i32 %mul.i.i.4, %244
  %250 = sext i32 %249 to i64
  %251 = getelementptr inbounds float, float* %11, i64 %250
  %252 = bitcast float* %251 to <8 x float>*
  %wide.load42.3 = load <8 x float>, <8 x float>* %252, align 4, !tbaa !12, !llvm.access.group !16
  %253 = fsub <8 x float> %wide.load42.3, %wide.load41.3
  %254 = bitcast float* %251 to <8 x float>*
  store <8 x float> %253, <8 x float>* %254, align 4, !tbaa !12, !llvm.access.group !16
  %255 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.5 = or i32 %255, 5
  %mul.i.i.5 = mul nsw i32 %15, %conv2.i.i.5
  %256 = trunc i64 %mul.i.i.i to i32
  %257 = shl i64 %2, 37
  %258 = ashr exact i64 %257, 32
  %259 = getelementptr inbounds float, float* %7, i64 %258
  %260 = bitcast float* %259 to <8 x float>*
  %wide.load51 = load <8 x float>, <8 x float>* %260, align 4, !tbaa !12, !llvm.access.group !16
  %261 = add nsw i32 %mul.i.i.5, %256
  %262 = sext i32 %261 to i64
  %263 = getelementptr inbounds float, float* %11, i64 %262
  %264 = bitcast float* %263 to <8 x float>*
  %wide.load52 = load <8 x float>, <8 x float>* %264, align 4, !tbaa !12, !llvm.access.group !16
  %265 = fsub <8 x float> %wide.load52, %wide.load51
  %266 = bitcast float* %263 to <8 x float>*
  store <8 x float> %265, <8 x float>* %266, align 4, !tbaa !12, !llvm.access.group !16
  %267 = or i64 %mul.i.i.i, 8
  %268 = trunc i64 %267 to i32
  %269 = shl i64 %267, 32
  %270 = ashr exact i64 %269, 32
  %271 = getelementptr inbounds float, float* %7, i64 %270
  %272 = bitcast float* %271 to <8 x float>*
  %wide.load51.1 = load <8 x float>, <8 x float>* %272, align 4, !tbaa !12, !llvm.access.group !16
  %273 = add nsw i32 %mul.i.i.5, %268
  %274 = sext i32 %273 to i64
  %275 = getelementptr inbounds float, float* %11, i64 %274
  %276 = bitcast float* %275 to <8 x float>*
  %wide.load52.1 = load <8 x float>, <8 x float>* %276, align 4, !tbaa !12, !llvm.access.group !16
  %277 = fsub <8 x float> %wide.load52.1, %wide.load51.1
  %278 = bitcast float* %275 to <8 x float>*
  store <8 x float> %277, <8 x float>* %278, align 4, !tbaa !12, !llvm.access.group !16
  %279 = or i64 %mul.i.i.i, 16
  %280 = trunc i64 %279 to i32
  %281 = shl i64 %279, 32
  %282 = ashr exact i64 %281, 32
  %283 = getelementptr inbounds float, float* %7, i64 %282
  %284 = bitcast float* %283 to <8 x float>*
  %wide.load51.2 = load <8 x float>, <8 x float>* %284, align 4, !tbaa !12, !llvm.access.group !16
  %285 = add nsw i32 %mul.i.i.5, %280
  %286 = sext i32 %285 to i64
  %287 = getelementptr inbounds float, float* %11, i64 %286
  %288 = bitcast float* %287 to <8 x float>*
  %wide.load52.2 = load <8 x float>, <8 x float>* %288, align 4, !tbaa !12, !llvm.access.group !16
  %289 = fsub <8 x float> %wide.load52.2, %wide.load51.2
  %290 = bitcast float* %287 to <8 x float>*
  store <8 x float> %289, <8 x float>* %290, align 4, !tbaa !12, !llvm.access.group !16
  %291 = or i64 %mul.i.i.i, 24
  %292 = trunc i64 %291 to i32
  %293 = shl i64 %291, 32
  %294 = ashr exact i64 %293, 32
  %295 = getelementptr inbounds float, float* %7, i64 %294
  %296 = bitcast float* %295 to <8 x float>*
  %wide.load51.3 = load <8 x float>, <8 x float>* %296, align 4, !tbaa !12, !llvm.access.group !16
  %297 = add nsw i32 %mul.i.i.5, %292
  %298 = sext i32 %297 to i64
  %299 = getelementptr inbounds float, float* %11, i64 %298
  %300 = bitcast float* %299 to <8 x float>*
  %wide.load52.3 = load <8 x float>, <8 x float>* %300, align 4, !tbaa !12, !llvm.access.group !16
  %301 = fsub <8 x float> %wide.load52.3, %wide.load51.3
  %302 = bitcast float* %299 to <8 x float>*
  store <8 x float> %301, <8 x float>* %302, align 4, !tbaa !12, !llvm.access.group !16
  %303 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.6 = or i32 %303, 6
  %mul.i.i.6 = mul nsw i32 %15, %conv2.i.i.6
  %304 = trunc i64 %mul.i.i.i to i32
  %305 = shl i64 %2, 37
  %306 = ashr exact i64 %305, 32
  %307 = getelementptr inbounds float, float* %7, i64 %306
  %308 = bitcast float* %307 to <8 x float>*
  %wide.load61 = load <8 x float>, <8 x float>* %308, align 4, !tbaa !12, !llvm.access.group !16
  %309 = add nsw i32 %mul.i.i.6, %304
  %310 = sext i32 %309 to i64
  %311 = getelementptr inbounds float, float* %11, i64 %310
  %312 = bitcast float* %311 to <8 x float>*
  %wide.load62 = load <8 x float>, <8 x float>* %312, align 4, !tbaa !12, !llvm.access.group !16
  %313 = fsub <8 x float> %wide.load62, %wide.load61
  %314 = bitcast float* %311 to <8 x float>*
  store <8 x float> %313, <8 x float>* %314, align 4, !tbaa !12, !llvm.access.group !16
  %315 = or i64 %mul.i.i.i, 8
  %316 = trunc i64 %315 to i32
  %317 = shl i64 %315, 32
  %318 = ashr exact i64 %317, 32
  %319 = getelementptr inbounds float, float* %7, i64 %318
  %320 = bitcast float* %319 to <8 x float>*
  %wide.load61.1 = load <8 x float>, <8 x float>* %320, align 4, !tbaa !12, !llvm.access.group !16
  %321 = add nsw i32 %mul.i.i.6, %316
  %322 = sext i32 %321 to i64
  %323 = getelementptr inbounds float, float* %11, i64 %322
  %324 = bitcast float* %323 to <8 x float>*
  %wide.load62.1 = load <8 x float>, <8 x float>* %324, align 4, !tbaa !12, !llvm.access.group !16
  %325 = fsub <8 x float> %wide.load62.1, %wide.load61.1
  %326 = bitcast float* %323 to <8 x float>*
  store <8 x float> %325, <8 x float>* %326, align 4, !tbaa !12, !llvm.access.group !16
  %327 = or i64 %mul.i.i.i, 16
  %328 = trunc i64 %327 to i32
  %329 = shl i64 %327, 32
  %330 = ashr exact i64 %329, 32
  %331 = getelementptr inbounds float, float* %7, i64 %330
  %332 = bitcast float* %331 to <8 x float>*
  %wide.load61.2 = load <8 x float>, <8 x float>* %332, align 4, !tbaa !12, !llvm.access.group !16
  %333 = add nsw i32 %mul.i.i.6, %328
  %334 = sext i32 %333 to i64
  %335 = getelementptr inbounds float, float* %11, i64 %334
  %336 = bitcast float* %335 to <8 x float>*
  %wide.load62.2 = load <8 x float>, <8 x float>* %336, align 4, !tbaa !12, !llvm.access.group !16
  %337 = fsub <8 x float> %wide.load62.2, %wide.load61.2
  %338 = bitcast float* %335 to <8 x float>*
  store <8 x float> %337, <8 x float>* %338, align 4, !tbaa !12, !llvm.access.group !16
  %339 = or i64 %mul.i.i.i, 24
  %340 = trunc i64 %339 to i32
  %341 = shl i64 %339, 32
  %342 = ashr exact i64 %341, 32
  %343 = getelementptr inbounds float, float* %7, i64 %342
  %344 = bitcast float* %343 to <8 x float>*
  %wide.load61.3 = load <8 x float>, <8 x float>* %344, align 4, !tbaa !12, !llvm.access.group !16
  %345 = add nsw i32 %mul.i.i.6, %340
  %346 = sext i32 %345 to i64
  %347 = getelementptr inbounds float, float* %11, i64 %346
  %348 = bitcast float* %347 to <8 x float>*
  %wide.load62.3 = load <8 x float>, <8 x float>* %348, align 4, !tbaa !12, !llvm.access.group !16
  %349 = fsub <8 x float> %wide.load62.3, %wide.load61.3
  %350 = bitcast float* %347 to <8 x float>*
  store <8 x float> %349, <8 x float>* %350, align 4, !tbaa !12, !llvm.access.group !16
  %351 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.7 = or i32 %351, 7
  %mul.i.i.7 = mul nsw i32 %15, %conv2.i.i.7
  %352 = trunc i64 %mul.i.i.i to i32
  %353 = shl i64 %2, 37
  %354 = ashr exact i64 %353, 32
  %355 = getelementptr inbounds float, float* %7, i64 %354
  %356 = bitcast float* %355 to <8 x float>*
  %wide.load71 = load <8 x float>, <8 x float>* %356, align 4, !tbaa !12, !llvm.access.group !16
  %357 = add nsw i32 %mul.i.i.7, %352
  %358 = sext i32 %357 to i64
  %359 = getelementptr inbounds float, float* %11, i64 %358
  %360 = bitcast float* %359 to <8 x float>*
  %wide.load72 = load <8 x float>, <8 x float>* %360, align 4, !tbaa !12, !llvm.access.group !16
  %361 = fsub <8 x float> %wide.load72, %wide.load71
  %362 = bitcast float* %359 to <8 x float>*
  store <8 x float> %361, <8 x float>* %362, align 4, !tbaa !12, !llvm.access.group !16
  %363 = or i64 %mul.i.i.i, 8
  %364 = trunc i64 %363 to i32
  %365 = shl i64 %363, 32
  %366 = ashr exact i64 %365, 32
  %367 = getelementptr inbounds float, float* %7, i64 %366
  %368 = bitcast float* %367 to <8 x float>*
  %wide.load71.1 = load <8 x float>, <8 x float>* %368, align 4, !tbaa !12, !llvm.access.group !16
  %369 = add nsw i32 %mul.i.i.7, %364
  %370 = sext i32 %369 to i64
  %371 = getelementptr inbounds float, float* %11, i64 %370
  %372 = bitcast float* %371 to <8 x float>*
  %wide.load72.1 = load <8 x float>, <8 x float>* %372, align 4, !tbaa !12, !llvm.access.group !16
  %373 = fsub <8 x float> %wide.load72.1, %wide.load71.1
  %374 = bitcast float* %371 to <8 x float>*
  store <8 x float> %373, <8 x float>* %374, align 4, !tbaa !12, !llvm.access.group !16
  %375 = or i64 %mul.i.i.i, 16
  %376 = trunc i64 %375 to i32
  %377 = shl i64 %375, 32
  %378 = ashr exact i64 %377, 32
  %379 = getelementptr inbounds float, float* %7, i64 %378
  %380 = bitcast float* %379 to <8 x float>*
  %wide.load71.2 = load <8 x float>, <8 x float>* %380, align 4, !tbaa !12, !llvm.access.group !16
  %381 = add nsw i32 %mul.i.i.7, %376
  %382 = sext i32 %381 to i64
  %383 = getelementptr inbounds float, float* %11, i64 %382
  %384 = bitcast float* %383 to <8 x float>*
  %wide.load72.2 = load <8 x float>, <8 x float>* %384, align 4, !tbaa !12, !llvm.access.group !16
  %385 = fsub <8 x float> %wide.load72.2, %wide.load71.2
  %386 = bitcast float* %383 to <8 x float>*
  store <8 x float> %385, <8 x float>* %386, align 4, !tbaa !12, !llvm.access.group !16
  %387 = or i64 %mul.i.i.i, 24
  %388 = trunc i64 %387 to i32
  %389 = shl i64 %387, 32
  %390 = ashr exact i64 %389, 32
  %391 = getelementptr inbounds float, float* %7, i64 %390
  %392 = bitcast float* %391 to <8 x float>*
  %wide.load71.3 = load <8 x float>, <8 x float>* %392, align 4, !tbaa !12, !llvm.access.group !16
  %393 = add nsw i32 %mul.i.i.7, %388
  %394 = sext i32 %393 to i64
  %395 = getelementptr inbounds float, float* %11, i64 %394
  %396 = bitcast float* %395 to <8 x float>*
  %wide.load72.3 = load <8 x float>, <8 x float>* %396, align 4, !tbaa !12, !llvm.access.group !16
  %397 = fsub <8 x float> %wide.load72.3, %wide.load71.3
  %398 = bitcast float* %395 to <8 x float>*
  store <8 x float> %397, <8 x float>* %398, align 4, !tbaa !12, !llvm.access.group !16
  ret void
}

; Function Attrs: nofree norecurse nounwind
define void @_pocl_kernel_reduce_kernel_workgroup_fast(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #1 {
pregion_for_entry.pregion_for_init.i.i:
  %5 = bitcast i8** %0 to float**
  %6 = load float*, float** %5, align 8
  %7 = getelementptr i8*, i8** %0, i64 1
  %8 = bitcast i8** %7 to float**
  %9 = load float*, float** %8, align 8
  %10 = getelementptr i8*, i8** %0, i64 2
  %11 = bitcast i8** %10 to i32**
  %12 = load i32*, i32** %11, align 8
  %13 = load i32, i32* %12, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %conv2.i.i = trunc i64 %mul3.i.i.i to i32
  %mul.i.i = mul nsw i32 %13, %conv2.i.i
  %14 = trunc i64 %mul.i.i.i to i32
  %15 = shl i64 %2, 37
  %16 = ashr exact i64 %15, 32
  %17 = getelementptr inbounds float, float* %6, i64 %16
  %18 = bitcast float* %17 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %18, align 4, !tbaa !12, !llvm.access.group !16
  %19 = add nsw i32 %mul.i.i, %14
  %20 = sext i32 %19 to i64
  %21 = getelementptr inbounds float, float* %9, i64 %20
  %22 = bitcast float* %21 to <8 x float>*
  %wide.load2 = load <8 x float>, <8 x float>* %22, align 4, !tbaa !12, !llvm.access.group !16
  %23 = fsub <8 x float> %wide.load2, %wide.load
  %24 = bitcast float* %21 to <8 x float>*
  store <8 x float> %23, <8 x float>* %24, align 4, !tbaa !12, !llvm.access.group !16
  %25 = or i64 %mul.i.i.i, 8
  %26 = trunc i64 %25 to i32
  %27 = shl i64 %25, 32
  %28 = ashr exact i64 %27, 32
  %29 = getelementptr inbounds float, float* %6, i64 %28
  %30 = bitcast float* %29 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %30, align 4, !tbaa !12, !llvm.access.group !16
  %31 = add nsw i32 %mul.i.i, %26
  %32 = sext i32 %31 to i64
  %33 = getelementptr inbounds float, float* %9, i64 %32
  %34 = bitcast float* %33 to <8 x float>*
  %wide.load2.1 = load <8 x float>, <8 x float>* %34, align 4, !tbaa !12, !llvm.access.group !16
  %35 = fsub <8 x float> %wide.load2.1, %wide.load.1
  %36 = bitcast float* %33 to <8 x float>*
  store <8 x float> %35, <8 x float>* %36, align 4, !tbaa !12, !llvm.access.group !16
  %37 = or i64 %mul.i.i.i, 16
  %38 = trunc i64 %37 to i32
  %39 = shl i64 %37, 32
  %40 = ashr exact i64 %39, 32
  %41 = getelementptr inbounds float, float* %6, i64 %40
  %42 = bitcast float* %41 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %42, align 4, !tbaa !12, !llvm.access.group !16
  %43 = add nsw i32 %mul.i.i, %38
  %44 = sext i32 %43 to i64
  %45 = getelementptr inbounds float, float* %9, i64 %44
  %46 = bitcast float* %45 to <8 x float>*
  %wide.load2.2 = load <8 x float>, <8 x float>* %46, align 4, !tbaa !12, !llvm.access.group !16
  %47 = fsub <8 x float> %wide.load2.2, %wide.load.2
  %48 = bitcast float* %45 to <8 x float>*
  store <8 x float> %47, <8 x float>* %48, align 4, !tbaa !12, !llvm.access.group !16
  %49 = or i64 %mul.i.i.i, 24
  %50 = trunc i64 %49 to i32
  %51 = shl i64 %49, 32
  %52 = ashr exact i64 %51, 32
  %53 = getelementptr inbounds float, float* %6, i64 %52
  %54 = bitcast float* %53 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %54, align 4, !tbaa !12, !llvm.access.group !16
  %55 = add nsw i32 %mul.i.i, %50
  %56 = sext i32 %55 to i64
  %57 = getelementptr inbounds float, float* %9, i64 %56
  %58 = bitcast float* %57 to <8 x float>*
  %wide.load2.3 = load <8 x float>, <8 x float>* %58, align 4, !tbaa !12, !llvm.access.group !16
  %59 = fsub <8 x float> %wide.load2.3, %wide.load.3
  %60 = bitcast float* %57 to <8 x float>*
  store <8 x float> %59, <8 x float>* %60, align 4, !tbaa !12, !llvm.access.group !16
  %61 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.1 = or i32 %61, 1
  %mul.i.i.1 = mul nsw i32 %13, %conv2.i.i.1
  %62 = trunc i64 %mul.i.i.i to i32
  %63 = shl i64 %2, 37
  %64 = ashr exact i64 %63, 32
  %65 = getelementptr inbounds float, float* %6, i64 %64
  %66 = bitcast float* %65 to <8 x float>*
  %wide.load11 = load <8 x float>, <8 x float>* %66, align 4, !tbaa !12, !llvm.access.group !16
  %67 = add nsw i32 %mul.i.i.1, %62
  %68 = sext i32 %67 to i64
  %69 = getelementptr inbounds float, float* %9, i64 %68
  %70 = bitcast float* %69 to <8 x float>*
  %wide.load12 = load <8 x float>, <8 x float>* %70, align 4, !tbaa !12, !llvm.access.group !16
  %71 = fsub <8 x float> %wide.load12, %wide.load11
  %72 = bitcast float* %69 to <8 x float>*
  store <8 x float> %71, <8 x float>* %72, align 4, !tbaa !12, !llvm.access.group !16
  %73 = or i64 %mul.i.i.i, 8
  %74 = trunc i64 %73 to i32
  %75 = shl i64 %73, 32
  %76 = ashr exact i64 %75, 32
  %77 = getelementptr inbounds float, float* %6, i64 %76
  %78 = bitcast float* %77 to <8 x float>*
  %wide.load11.1 = load <8 x float>, <8 x float>* %78, align 4, !tbaa !12, !llvm.access.group !16
  %79 = add nsw i32 %mul.i.i.1, %74
  %80 = sext i32 %79 to i64
  %81 = getelementptr inbounds float, float* %9, i64 %80
  %82 = bitcast float* %81 to <8 x float>*
  %wide.load12.1 = load <8 x float>, <8 x float>* %82, align 4, !tbaa !12, !llvm.access.group !16
  %83 = fsub <8 x float> %wide.load12.1, %wide.load11.1
  %84 = bitcast float* %81 to <8 x float>*
  store <8 x float> %83, <8 x float>* %84, align 4, !tbaa !12, !llvm.access.group !16
  %85 = or i64 %mul.i.i.i, 16
  %86 = trunc i64 %85 to i32
  %87 = shl i64 %85, 32
  %88 = ashr exact i64 %87, 32
  %89 = getelementptr inbounds float, float* %6, i64 %88
  %90 = bitcast float* %89 to <8 x float>*
  %wide.load11.2 = load <8 x float>, <8 x float>* %90, align 4, !tbaa !12, !llvm.access.group !16
  %91 = add nsw i32 %mul.i.i.1, %86
  %92 = sext i32 %91 to i64
  %93 = getelementptr inbounds float, float* %9, i64 %92
  %94 = bitcast float* %93 to <8 x float>*
  %wide.load12.2 = load <8 x float>, <8 x float>* %94, align 4, !tbaa !12, !llvm.access.group !16
  %95 = fsub <8 x float> %wide.load12.2, %wide.load11.2
  %96 = bitcast float* %93 to <8 x float>*
  store <8 x float> %95, <8 x float>* %96, align 4, !tbaa !12, !llvm.access.group !16
  %97 = or i64 %mul.i.i.i, 24
  %98 = trunc i64 %97 to i32
  %99 = shl i64 %97, 32
  %100 = ashr exact i64 %99, 32
  %101 = getelementptr inbounds float, float* %6, i64 %100
  %102 = bitcast float* %101 to <8 x float>*
  %wide.load11.3 = load <8 x float>, <8 x float>* %102, align 4, !tbaa !12, !llvm.access.group !16
  %103 = add nsw i32 %mul.i.i.1, %98
  %104 = sext i32 %103 to i64
  %105 = getelementptr inbounds float, float* %9, i64 %104
  %106 = bitcast float* %105 to <8 x float>*
  %wide.load12.3 = load <8 x float>, <8 x float>* %106, align 4, !tbaa !12, !llvm.access.group !16
  %107 = fsub <8 x float> %wide.load12.3, %wide.load11.3
  %108 = bitcast float* %105 to <8 x float>*
  store <8 x float> %107, <8 x float>* %108, align 4, !tbaa !12, !llvm.access.group !16
  %109 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.2 = or i32 %109, 2
  %mul.i.i.2 = mul nsw i32 %13, %conv2.i.i.2
  %110 = trunc i64 %mul.i.i.i to i32
  %111 = shl i64 %2, 37
  %112 = ashr exact i64 %111, 32
  %113 = getelementptr inbounds float, float* %6, i64 %112
  %114 = bitcast float* %113 to <8 x float>*
  %wide.load21 = load <8 x float>, <8 x float>* %114, align 4, !tbaa !12, !llvm.access.group !16
  %115 = add nsw i32 %mul.i.i.2, %110
  %116 = sext i32 %115 to i64
  %117 = getelementptr inbounds float, float* %9, i64 %116
  %118 = bitcast float* %117 to <8 x float>*
  %wide.load22 = load <8 x float>, <8 x float>* %118, align 4, !tbaa !12, !llvm.access.group !16
  %119 = fsub <8 x float> %wide.load22, %wide.load21
  %120 = bitcast float* %117 to <8 x float>*
  store <8 x float> %119, <8 x float>* %120, align 4, !tbaa !12, !llvm.access.group !16
  %121 = or i64 %mul.i.i.i, 8
  %122 = trunc i64 %121 to i32
  %123 = shl i64 %121, 32
  %124 = ashr exact i64 %123, 32
  %125 = getelementptr inbounds float, float* %6, i64 %124
  %126 = bitcast float* %125 to <8 x float>*
  %wide.load21.1 = load <8 x float>, <8 x float>* %126, align 4, !tbaa !12, !llvm.access.group !16
  %127 = add nsw i32 %mul.i.i.2, %122
  %128 = sext i32 %127 to i64
  %129 = getelementptr inbounds float, float* %9, i64 %128
  %130 = bitcast float* %129 to <8 x float>*
  %wide.load22.1 = load <8 x float>, <8 x float>* %130, align 4, !tbaa !12, !llvm.access.group !16
  %131 = fsub <8 x float> %wide.load22.1, %wide.load21.1
  %132 = bitcast float* %129 to <8 x float>*
  store <8 x float> %131, <8 x float>* %132, align 4, !tbaa !12, !llvm.access.group !16
  %133 = or i64 %mul.i.i.i, 16
  %134 = trunc i64 %133 to i32
  %135 = shl i64 %133, 32
  %136 = ashr exact i64 %135, 32
  %137 = getelementptr inbounds float, float* %6, i64 %136
  %138 = bitcast float* %137 to <8 x float>*
  %wide.load21.2 = load <8 x float>, <8 x float>* %138, align 4, !tbaa !12, !llvm.access.group !16
  %139 = add nsw i32 %mul.i.i.2, %134
  %140 = sext i32 %139 to i64
  %141 = getelementptr inbounds float, float* %9, i64 %140
  %142 = bitcast float* %141 to <8 x float>*
  %wide.load22.2 = load <8 x float>, <8 x float>* %142, align 4, !tbaa !12, !llvm.access.group !16
  %143 = fsub <8 x float> %wide.load22.2, %wide.load21.2
  %144 = bitcast float* %141 to <8 x float>*
  store <8 x float> %143, <8 x float>* %144, align 4, !tbaa !12, !llvm.access.group !16
  %145 = or i64 %mul.i.i.i, 24
  %146 = trunc i64 %145 to i32
  %147 = shl i64 %145, 32
  %148 = ashr exact i64 %147, 32
  %149 = getelementptr inbounds float, float* %6, i64 %148
  %150 = bitcast float* %149 to <8 x float>*
  %wide.load21.3 = load <8 x float>, <8 x float>* %150, align 4, !tbaa !12, !llvm.access.group !16
  %151 = add nsw i32 %mul.i.i.2, %146
  %152 = sext i32 %151 to i64
  %153 = getelementptr inbounds float, float* %9, i64 %152
  %154 = bitcast float* %153 to <8 x float>*
  %wide.load22.3 = load <8 x float>, <8 x float>* %154, align 4, !tbaa !12, !llvm.access.group !16
  %155 = fsub <8 x float> %wide.load22.3, %wide.load21.3
  %156 = bitcast float* %153 to <8 x float>*
  store <8 x float> %155, <8 x float>* %156, align 4, !tbaa !12, !llvm.access.group !16
  %157 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.3 = or i32 %157, 3
  %mul.i.i.3 = mul nsw i32 %13, %conv2.i.i.3
  %158 = trunc i64 %mul.i.i.i to i32
  %159 = shl i64 %2, 37
  %160 = ashr exact i64 %159, 32
  %161 = getelementptr inbounds float, float* %6, i64 %160
  %162 = bitcast float* %161 to <8 x float>*
  %wide.load31 = load <8 x float>, <8 x float>* %162, align 4, !tbaa !12, !llvm.access.group !16
  %163 = add nsw i32 %mul.i.i.3, %158
  %164 = sext i32 %163 to i64
  %165 = getelementptr inbounds float, float* %9, i64 %164
  %166 = bitcast float* %165 to <8 x float>*
  %wide.load32 = load <8 x float>, <8 x float>* %166, align 4, !tbaa !12, !llvm.access.group !16
  %167 = fsub <8 x float> %wide.load32, %wide.load31
  %168 = bitcast float* %165 to <8 x float>*
  store <8 x float> %167, <8 x float>* %168, align 4, !tbaa !12, !llvm.access.group !16
  %169 = or i64 %mul.i.i.i, 8
  %170 = trunc i64 %169 to i32
  %171 = shl i64 %169, 32
  %172 = ashr exact i64 %171, 32
  %173 = getelementptr inbounds float, float* %6, i64 %172
  %174 = bitcast float* %173 to <8 x float>*
  %wide.load31.1 = load <8 x float>, <8 x float>* %174, align 4, !tbaa !12, !llvm.access.group !16
  %175 = add nsw i32 %mul.i.i.3, %170
  %176 = sext i32 %175 to i64
  %177 = getelementptr inbounds float, float* %9, i64 %176
  %178 = bitcast float* %177 to <8 x float>*
  %wide.load32.1 = load <8 x float>, <8 x float>* %178, align 4, !tbaa !12, !llvm.access.group !16
  %179 = fsub <8 x float> %wide.load32.1, %wide.load31.1
  %180 = bitcast float* %177 to <8 x float>*
  store <8 x float> %179, <8 x float>* %180, align 4, !tbaa !12, !llvm.access.group !16
  %181 = or i64 %mul.i.i.i, 16
  %182 = trunc i64 %181 to i32
  %183 = shl i64 %181, 32
  %184 = ashr exact i64 %183, 32
  %185 = getelementptr inbounds float, float* %6, i64 %184
  %186 = bitcast float* %185 to <8 x float>*
  %wide.load31.2 = load <8 x float>, <8 x float>* %186, align 4, !tbaa !12, !llvm.access.group !16
  %187 = add nsw i32 %mul.i.i.3, %182
  %188 = sext i32 %187 to i64
  %189 = getelementptr inbounds float, float* %9, i64 %188
  %190 = bitcast float* %189 to <8 x float>*
  %wide.load32.2 = load <8 x float>, <8 x float>* %190, align 4, !tbaa !12, !llvm.access.group !16
  %191 = fsub <8 x float> %wide.load32.2, %wide.load31.2
  %192 = bitcast float* %189 to <8 x float>*
  store <8 x float> %191, <8 x float>* %192, align 4, !tbaa !12, !llvm.access.group !16
  %193 = or i64 %mul.i.i.i, 24
  %194 = trunc i64 %193 to i32
  %195 = shl i64 %193, 32
  %196 = ashr exact i64 %195, 32
  %197 = getelementptr inbounds float, float* %6, i64 %196
  %198 = bitcast float* %197 to <8 x float>*
  %wide.load31.3 = load <8 x float>, <8 x float>* %198, align 4, !tbaa !12, !llvm.access.group !16
  %199 = add nsw i32 %mul.i.i.3, %194
  %200 = sext i32 %199 to i64
  %201 = getelementptr inbounds float, float* %9, i64 %200
  %202 = bitcast float* %201 to <8 x float>*
  %wide.load32.3 = load <8 x float>, <8 x float>* %202, align 4, !tbaa !12, !llvm.access.group !16
  %203 = fsub <8 x float> %wide.load32.3, %wide.load31.3
  %204 = bitcast float* %201 to <8 x float>*
  store <8 x float> %203, <8 x float>* %204, align 4, !tbaa !12, !llvm.access.group !16
  %205 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.4 = or i32 %205, 4
  %mul.i.i.4 = mul nsw i32 %13, %conv2.i.i.4
  %206 = trunc i64 %mul.i.i.i to i32
  %207 = shl i64 %2, 37
  %208 = ashr exact i64 %207, 32
  %209 = getelementptr inbounds float, float* %6, i64 %208
  %210 = bitcast float* %209 to <8 x float>*
  %wide.load41 = load <8 x float>, <8 x float>* %210, align 4, !tbaa !12, !llvm.access.group !16
  %211 = add nsw i32 %mul.i.i.4, %206
  %212 = sext i32 %211 to i64
  %213 = getelementptr inbounds float, float* %9, i64 %212
  %214 = bitcast float* %213 to <8 x float>*
  %wide.load42 = load <8 x float>, <8 x float>* %214, align 4, !tbaa !12, !llvm.access.group !16
  %215 = fsub <8 x float> %wide.load42, %wide.load41
  %216 = bitcast float* %213 to <8 x float>*
  store <8 x float> %215, <8 x float>* %216, align 4, !tbaa !12, !llvm.access.group !16
  %217 = or i64 %mul.i.i.i, 8
  %218 = trunc i64 %217 to i32
  %219 = shl i64 %217, 32
  %220 = ashr exact i64 %219, 32
  %221 = getelementptr inbounds float, float* %6, i64 %220
  %222 = bitcast float* %221 to <8 x float>*
  %wide.load41.1 = load <8 x float>, <8 x float>* %222, align 4, !tbaa !12, !llvm.access.group !16
  %223 = add nsw i32 %mul.i.i.4, %218
  %224 = sext i32 %223 to i64
  %225 = getelementptr inbounds float, float* %9, i64 %224
  %226 = bitcast float* %225 to <8 x float>*
  %wide.load42.1 = load <8 x float>, <8 x float>* %226, align 4, !tbaa !12, !llvm.access.group !16
  %227 = fsub <8 x float> %wide.load42.1, %wide.load41.1
  %228 = bitcast float* %225 to <8 x float>*
  store <8 x float> %227, <8 x float>* %228, align 4, !tbaa !12, !llvm.access.group !16
  %229 = or i64 %mul.i.i.i, 16
  %230 = trunc i64 %229 to i32
  %231 = shl i64 %229, 32
  %232 = ashr exact i64 %231, 32
  %233 = getelementptr inbounds float, float* %6, i64 %232
  %234 = bitcast float* %233 to <8 x float>*
  %wide.load41.2 = load <8 x float>, <8 x float>* %234, align 4, !tbaa !12, !llvm.access.group !16
  %235 = add nsw i32 %mul.i.i.4, %230
  %236 = sext i32 %235 to i64
  %237 = getelementptr inbounds float, float* %9, i64 %236
  %238 = bitcast float* %237 to <8 x float>*
  %wide.load42.2 = load <8 x float>, <8 x float>* %238, align 4, !tbaa !12, !llvm.access.group !16
  %239 = fsub <8 x float> %wide.load42.2, %wide.load41.2
  %240 = bitcast float* %237 to <8 x float>*
  store <8 x float> %239, <8 x float>* %240, align 4, !tbaa !12, !llvm.access.group !16
  %241 = or i64 %mul.i.i.i, 24
  %242 = trunc i64 %241 to i32
  %243 = shl i64 %241, 32
  %244 = ashr exact i64 %243, 32
  %245 = getelementptr inbounds float, float* %6, i64 %244
  %246 = bitcast float* %245 to <8 x float>*
  %wide.load41.3 = load <8 x float>, <8 x float>* %246, align 4, !tbaa !12, !llvm.access.group !16
  %247 = add nsw i32 %mul.i.i.4, %242
  %248 = sext i32 %247 to i64
  %249 = getelementptr inbounds float, float* %9, i64 %248
  %250 = bitcast float* %249 to <8 x float>*
  %wide.load42.3 = load <8 x float>, <8 x float>* %250, align 4, !tbaa !12, !llvm.access.group !16
  %251 = fsub <8 x float> %wide.load42.3, %wide.load41.3
  %252 = bitcast float* %249 to <8 x float>*
  store <8 x float> %251, <8 x float>* %252, align 4, !tbaa !12, !llvm.access.group !16
  %253 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.5 = or i32 %253, 5
  %mul.i.i.5 = mul nsw i32 %13, %conv2.i.i.5
  %254 = trunc i64 %mul.i.i.i to i32
  %255 = shl i64 %2, 37
  %256 = ashr exact i64 %255, 32
  %257 = getelementptr inbounds float, float* %6, i64 %256
  %258 = bitcast float* %257 to <8 x float>*
  %wide.load51 = load <8 x float>, <8 x float>* %258, align 4, !tbaa !12, !llvm.access.group !16
  %259 = add nsw i32 %mul.i.i.5, %254
  %260 = sext i32 %259 to i64
  %261 = getelementptr inbounds float, float* %9, i64 %260
  %262 = bitcast float* %261 to <8 x float>*
  %wide.load52 = load <8 x float>, <8 x float>* %262, align 4, !tbaa !12, !llvm.access.group !16
  %263 = fsub <8 x float> %wide.load52, %wide.load51
  %264 = bitcast float* %261 to <8 x float>*
  store <8 x float> %263, <8 x float>* %264, align 4, !tbaa !12, !llvm.access.group !16
  %265 = or i64 %mul.i.i.i, 8
  %266 = trunc i64 %265 to i32
  %267 = shl i64 %265, 32
  %268 = ashr exact i64 %267, 32
  %269 = getelementptr inbounds float, float* %6, i64 %268
  %270 = bitcast float* %269 to <8 x float>*
  %wide.load51.1 = load <8 x float>, <8 x float>* %270, align 4, !tbaa !12, !llvm.access.group !16
  %271 = add nsw i32 %mul.i.i.5, %266
  %272 = sext i32 %271 to i64
  %273 = getelementptr inbounds float, float* %9, i64 %272
  %274 = bitcast float* %273 to <8 x float>*
  %wide.load52.1 = load <8 x float>, <8 x float>* %274, align 4, !tbaa !12, !llvm.access.group !16
  %275 = fsub <8 x float> %wide.load52.1, %wide.load51.1
  %276 = bitcast float* %273 to <8 x float>*
  store <8 x float> %275, <8 x float>* %276, align 4, !tbaa !12, !llvm.access.group !16
  %277 = or i64 %mul.i.i.i, 16
  %278 = trunc i64 %277 to i32
  %279 = shl i64 %277, 32
  %280 = ashr exact i64 %279, 32
  %281 = getelementptr inbounds float, float* %6, i64 %280
  %282 = bitcast float* %281 to <8 x float>*
  %wide.load51.2 = load <8 x float>, <8 x float>* %282, align 4, !tbaa !12, !llvm.access.group !16
  %283 = add nsw i32 %mul.i.i.5, %278
  %284 = sext i32 %283 to i64
  %285 = getelementptr inbounds float, float* %9, i64 %284
  %286 = bitcast float* %285 to <8 x float>*
  %wide.load52.2 = load <8 x float>, <8 x float>* %286, align 4, !tbaa !12, !llvm.access.group !16
  %287 = fsub <8 x float> %wide.load52.2, %wide.load51.2
  %288 = bitcast float* %285 to <8 x float>*
  store <8 x float> %287, <8 x float>* %288, align 4, !tbaa !12, !llvm.access.group !16
  %289 = or i64 %mul.i.i.i, 24
  %290 = trunc i64 %289 to i32
  %291 = shl i64 %289, 32
  %292 = ashr exact i64 %291, 32
  %293 = getelementptr inbounds float, float* %6, i64 %292
  %294 = bitcast float* %293 to <8 x float>*
  %wide.load51.3 = load <8 x float>, <8 x float>* %294, align 4, !tbaa !12, !llvm.access.group !16
  %295 = add nsw i32 %mul.i.i.5, %290
  %296 = sext i32 %295 to i64
  %297 = getelementptr inbounds float, float* %9, i64 %296
  %298 = bitcast float* %297 to <8 x float>*
  %wide.load52.3 = load <8 x float>, <8 x float>* %298, align 4, !tbaa !12, !llvm.access.group !16
  %299 = fsub <8 x float> %wide.load52.3, %wide.load51.3
  %300 = bitcast float* %297 to <8 x float>*
  store <8 x float> %299, <8 x float>* %300, align 4, !tbaa !12, !llvm.access.group !16
  %301 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.6 = or i32 %301, 6
  %mul.i.i.6 = mul nsw i32 %13, %conv2.i.i.6
  %302 = trunc i64 %mul.i.i.i to i32
  %303 = shl i64 %2, 37
  %304 = ashr exact i64 %303, 32
  %305 = getelementptr inbounds float, float* %6, i64 %304
  %306 = bitcast float* %305 to <8 x float>*
  %wide.load61 = load <8 x float>, <8 x float>* %306, align 4, !tbaa !12, !llvm.access.group !16
  %307 = add nsw i32 %mul.i.i.6, %302
  %308 = sext i32 %307 to i64
  %309 = getelementptr inbounds float, float* %9, i64 %308
  %310 = bitcast float* %309 to <8 x float>*
  %wide.load62 = load <8 x float>, <8 x float>* %310, align 4, !tbaa !12, !llvm.access.group !16
  %311 = fsub <8 x float> %wide.load62, %wide.load61
  %312 = bitcast float* %309 to <8 x float>*
  store <8 x float> %311, <8 x float>* %312, align 4, !tbaa !12, !llvm.access.group !16
  %313 = or i64 %mul.i.i.i, 8
  %314 = trunc i64 %313 to i32
  %315 = shl i64 %313, 32
  %316 = ashr exact i64 %315, 32
  %317 = getelementptr inbounds float, float* %6, i64 %316
  %318 = bitcast float* %317 to <8 x float>*
  %wide.load61.1 = load <8 x float>, <8 x float>* %318, align 4, !tbaa !12, !llvm.access.group !16
  %319 = add nsw i32 %mul.i.i.6, %314
  %320 = sext i32 %319 to i64
  %321 = getelementptr inbounds float, float* %9, i64 %320
  %322 = bitcast float* %321 to <8 x float>*
  %wide.load62.1 = load <8 x float>, <8 x float>* %322, align 4, !tbaa !12, !llvm.access.group !16
  %323 = fsub <8 x float> %wide.load62.1, %wide.load61.1
  %324 = bitcast float* %321 to <8 x float>*
  store <8 x float> %323, <8 x float>* %324, align 4, !tbaa !12, !llvm.access.group !16
  %325 = or i64 %mul.i.i.i, 16
  %326 = trunc i64 %325 to i32
  %327 = shl i64 %325, 32
  %328 = ashr exact i64 %327, 32
  %329 = getelementptr inbounds float, float* %6, i64 %328
  %330 = bitcast float* %329 to <8 x float>*
  %wide.load61.2 = load <8 x float>, <8 x float>* %330, align 4, !tbaa !12, !llvm.access.group !16
  %331 = add nsw i32 %mul.i.i.6, %326
  %332 = sext i32 %331 to i64
  %333 = getelementptr inbounds float, float* %9, i64 %332
  %334 = bitcast float* %333 to <8 x float>*
  %wide.load62.2 = load <8 x float>, <8 x float>* %334, align 4, !tbaa !12, !llvm.access.group !16
  %335 = fsub <8 x float> %wide.load62.2, %wide.load61.2
  %336 = bitcast float* %333 to <8 x float>*
  store <8 x float> %335, <8 x float>* %336, align 4, !tbaa !12, !llvm.access.group !16
  %337 = or i64 %mul.i.i.i, 24
  %338 = trunc i64 %337 to i32
  %339 = shl i64 %337, 32
  %340 = ashr exact i64 %339, 32
  %341 = getelementptr inbounds float, float* %6, i64 %340
  %342 = bitcast float* %341 to <8 x float>*
  %wide.load61.3 = load <8 x float>, <8 x float>* %342, align 4, !tbaa !12, !llvm.access.group !16
  %343 = add nsw i32 %mul.i.i.6, %338
  %344 = sext i32 %343 to i64
  %345 = getelementptr inbounds float, float* %9, i64 %344
  %346 = bitcast float* %345 to <8 x float>*
  %wide.load62.3 = load <8 x float>, <8 x float>* %346, align 4, !tbaa !12, !llvm.access.group !16
  %347 = fsub <8 x float> %wide.load62.3, %wide.load61.3
  %348 = bitcast float* %345 to <8 x float>*
  store <8 x float> %347, <8 x float>* %348, align 4, !tbaa !12, !llvm.access.group !16
  %349 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.7 = or i32 %349, 7
  %mul.i.i.7 = mul nsw i32 %13, %conv2.i.i.7
  %350 = trunc i64 %mul.i.i.i to i32
  %351 = shl i64 %2, 37
  %352 = ashr exact i64 %351, 32
  %353 = getelementptr inbounds float, float* %6, i64 %352
  %354 = bitcast float* %353 to <8 x float>*
  %wide.load71 = load <8 x float>, <8 x float>* %354, align 4, !tbaa !12, !llvm.access.group !16
  %355 = add nsw i32 %mul.i.i.7, %350
  %356 = sext i32 %355 to i64
  %357 = getelementptr inbounds float, float* %9, i64 %356
  %358 = bitcast float* %357 to <8 x float>*
  %wide.load72 = load <8 x float>, <8 x float>* %358, align 4, !tbaa !12, !llvm.access.group !16
  %359 = fsub <8 x float> %wide.load72, %wide.load71
  %360 = bitcast float* %357 to <8 x float>*
  store <8 x float> %359, <8 x float>* %360, align 4, !tbaa !12, !llvm.access.group !16
  %361 = or i64 %mul.i.i.i, 8
  %362 = trunc i64 %361 to i32
  %363 = shl i64 %361, 32
  %364 = ashr exact i64 %363, 32
  %365 = getelementptr inbounds float, float* %6, i64 %364
  %366 = bitcast float* %365 to <8 x float>*
  %wide.load71.1 = load <8 x float>, <8 x float>* %366, align 4, !tbaa !12, !llvm.access.group !16
  %367 = add nsw i32 %mul.i.i.7, %362
  %368 = sext i32 %367 to i64
  %369 = getelementptr inbounds float, float* %9, i64 %368
  %370 = bitcast float* %369 to <8 x float>*
  %wide.load72.1 = load <8 x float>, <8 x float>* %370, align 4, !tbaa !12, !llvm.access.group !16
  %371 = fsub <8 x float> %wide.load72.1, %wide.load71.1
  %372 = bitcast float* %369 to <8 x float>*
  store <8 x float> %371, <8 x float>* %372, align 4, !tbaa !12, !llvm.access.group !16
  %373 = or i64 %mul.i.i.i, 16
  %374 = trunc i64 %373 to i32
  %375 = shl i64 %373, 32
  %376 = ashr exact i64 %375, 32
  %377 = getelementptr inbounds float, float* %6, i64 %376
  %378 = bitcast float* %377 to <8 x float>*
  %wide.load71.2 = load <8 x float>, <8 x float>* %378, align 4, !tbaa !12, !llvm.access.group !16
  %379 = add nsw i32 %mul.i.i.7, %374
  %380 = sext i32 %379 to i64
  %381 = getelementptr inbounds float, float* %9, i64 %380
  %382 = bitcast float* %381 to <8 x float>*
  %wide.load72.2 = load <8 x float>, <8 x float>* %382, align 4, !tbaa !12, !llvm.access.group !16
  %383 = fsub <8 x float> %wide.load72.2, %wide.load71.2
  %384 = bitcast float* %381 to <8 x float>*
  store <8 x float> %383, <8 x float>* %384, align 4, !tbaa !12, !llvm.access.group !16
  %385 = or i64 %mul.i.i.i, 24
  %386 = trunc i64 %385 to i32
  %387 = shl i64 %385, 32
  %388 = ashr exact i64 %387, 32
  %389 = getelementptr inbounds float, float* %6, i64 %388
  %390 = bitcast float* %389 to <8 x float>*
  %wide.load71.3 = load <8 x float>, <8 x float>* %390, align 4, !tbaa !12, !llvm.access.group !16
  %391 = add nsw i32 %mul.i.i.7, %386
  %392 = sext i32 %391 to i64
  %393 = getelementptr inbounds float, float* %9, i64 %392
  %394 = bitcast float* %393 to <8 x float>*
  %wide.load72.3 = load <8 x float>, <8 x float>* %394, align 4, !tbaa !12, !llvm.access.group !16
  %395 = fsub <8 x float> %wide.load72.3, %wide.load71.3
  %396 = bitcast float* %393 to <8 x float>*
  store <8 x float> %395, <8 x float>* %396, align 4, !tbaa !12, !llvm.access.group !16
  ret void
}

attributes #0 = { alwaysinline nofree norecurse nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "stackrealign" "target-cpu"="skylake" "target-features"="+adx,+aes,+avx,+avx2,+bmi,+bmi2,+clflushopt,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+prfchw,+rdrnd,+rdseed,+sahf,+sgx,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "uniform-work-group-size"="true" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nofree norecurse nounwind }

!llvm.module.flags = !{!0, !1, !2}
!opencl.ocl.version = !{!3}
!llvm.ident = !{!4}
!opencl.spir.version = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 1, i32 2}
!4 = !{!"clang version 11.0.0 (git@github.com:llvm/llvm-project.git 91e89f9a5115b0f83b8f026e1ad0e6d1f885fa9b)"}
!5 = !{i32 1, i32 1, i32 0, i32 0}
!6 = !{!"none", !"none", !"none", !"none"}
!7 = !{!"DATA_TYPE*", !"DATA_TYPE*", !"int", !"int"}
!8 = !{!"float*", !"float*", !"int", !"int"}
!9 = !{!"", !"", !"", !""}
!10 = !{!"mean", !"data", !"m", !"n"}
!11 = !{i32 1}
!12 = !{!13, !13, i64 0}
!13 = !{!"float", !14, i64 0}
!14 = !{!"omnipotent char", !15, i64 0}
!15 = !{!"Simple C/C++ TBAA"}
!16 = !{!17, !18}
!17 = distinct !{}
!18 = distinct !{}
