; ModuleID = './FB/PNIGKMJLGHGOIJGIHNJGPCKLDMNDLLDCLEFHL/Convolution3D_kernel/32-8-1-goffs0-smallgrid/parallel.bc'
source_filename = "parallel_bc"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind readnone speculatable willreturn
declare float @llvm.fmuladd.f32(float, float, float) #0

; Function Attrs: alwaysinline nofree norecurse nounwind
define void @_pocl_kernel_Convolution3D_kernel(float* nocapture readonly %0, float* nocapture %1, i32 %2, i32 %3, i32 %4, i32 %5, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %6, i64 %7, i64 %8, i64 %9) local_unnamed_addr #1 !kernel_arg_addr_space !5 !kernel_arg_access_qual !6 !kernel_arg_type !7 !kernel_arg_base_type !8 !kernel_arg_type_qual !9 !kernel_arg_name !10 !pocl_generated !11 {
  %mul.i.i = shl i64 %7, 5
  %mul3.i.i = shl i64 %8, 3
  %sub.i = add nsw i32 %5, -1
  %mul.i = mul nsw i32 %4, %3
  %mul3.i = mul nsw i32 %sub.i, %mul.i
  %add9.i = add nsw i32 %5, 1
  %mul11.i = mul nsw i32 %add9.i, %mul.i
  %mul66.i = mul nsw i32 %mul.i, %5
  %11 = add i32 %5, -1
  %12 = mul i32 %11, %3
  %13 = trunc i64 %8 to i32
  %14 = shl i32 %13, 3
  %15 = add i32 %12, %14
  %16 = trunc i64 %7 to i32
  %17 = shl i32 %16, 5
  %18 = zext i32 %4 to i64
  %19 = add i32 %5, 1
  %20 = mul i32 %19, %3
  %21 = add i32 %20, %14
  %22 = mul i32 %5, %3
  %23 = add i32 %22, %14
  %24 = add i32 %23, -1
  %25 = insertelement <4 x i32> undef, i32 %22, i32 0
  %26 = insertelement <4 x i32> %25, i32 %15, i32 1
  %27 = insertelement <4 x i32> %26, i32 %21, i32 2
  %28 = insertelement <4 x i32> %27, i32 %12, i32 3
  %29 = insertelement <4 x i32> <i32 undef, i32 -1, i32 -1, i32 undef>, i32 %14, i32 0
  %30 = insertelement <4 x i32> %29, i32 %14, i32 3
  %31 = add <4 x i32> %28, %30
  %32 = insertelement <2 x i32> <i32 1, i32 undef>, i32 %4, i32 1
  %shuffle42 = shufflevector <2 x i32> %32, <2 x i32> undef, <4 x i32> <i32 0, i32 1, i32 1, i32 1>
  %33 = add <4 x i32> %31, %shuffle42
  %34 = shufflevector <4 x i32> %33, <4 x i32> undef, <8 x i32> <i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %35 = mul <4 x i32> %31, %shuffle42
  %36 = shufflevector <4 x i32> %35, <4 x i32> undef, <8 x i32> <i32 undef, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %37 = extractelement <4 x i32> %35, i32 1
  %38 = insertelement <8 x i32> undef, i32 %37, i32 0
  %39 = extractelement <4 x i32> %35, i32 2
  %40 = insertelement <8 x i32> %38, i32 %39, i32 1
  %41 = insertelement <8 x i32> %40, i32 %24, i32 2
  %42 = insertelement <8 x i32> %41, i32 %23, i32 3
  %43 = shufflevector <8 x i32> %42, <8 x i32> %34, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 undef, i32 undef, i32 undef>
  %44 = shufflevector <8 x i32> %43, <8 x i32> %36, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 9, i32 10, i32 11>
  %45 = insertelement <2 x i32> undef, i32 %17, i32 0
  %46 = insertelement <2 x i32> %45, i32 %4, i32 1
  %shuffle = shufflevector <2 x i32> %46, <2 x i32> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 1, i32 0, i32 0, i32 0>
  %47 = add <8 x i32> %44, %shuffle
  %48 = mul <8 x i32> %44, %shuffle
  %49 = shufflevector <8 x i32> %47, <8 x i32> %48, <8 x i32> <i32 0, i32 1, i32 10, i32 11, i32 12, i32 5, i32 6, i32 7>
  %50 = insertelement <8 x i32> <i32 -1, i32 -1, i32 undef, i32 undef, i32 undef, i32 1, i32 1, i32 1>, i32 %17, i32 2
  %51 = insertelement <8 x i32> %50, i32 %17, i32 3
  %52 = insertelement <8 x i32> %51, i32 %17, i32 4
  %53 = add <8 x i32> %49, %52
  %54 = add i32 %20, %14
  %55 = mul i32 %54, %4
  %56 = add i32 %55, %17
  %57 = add i32 %56, 1
  %58 = add i32 %12, %14
  %59 = add i32 %58, 1
  %60 = mul i32 %59, %4
  %61 = add i32 %60, %17
  %62 = add i32 %61, 1
  %63 = add i32 %20, %14
  %64 = add i32 %63, 1
  %65 = mul i32 %64, %4
  %66 = add i32 %65, %17
  %67 = add i32 %66, 1
  %68 = trunc i64 %mul.i.i to i32
  %69 = add nsw i32 %68, -1
  %70 = add i32 %mul66.i, %68
  %71 = or i32 %68, 1
  %72 = add i32 %71, %mul3.i
  %73 = add i32 %71, %mul11.i
  %74 = trunc i64 %mul.i.i to i32
  %75 = or i32 %74, 8
  %76 = add nsw i32 %75, -1
  %77 = add i32 %mul66.i, %75
  %78 = or i32 %74, 9
  %79 = add i32 %78, %mul3.i
  %80 = add i32 %78, %mul11.i
  %81 = trunc i64 %mul.i.i to i32
  %82 = or i32 %81, 16
  %83 = add nsw i32 %82, -1
  %84 = add i32 %mul66.i, %82
  %85 = or i32 %81, 17
  %86 = add i32 %85, %mul3.i
  %87 = add i32 %85, %mul11.i
  %88 = trunc i64 %mul.i.i to i32
  %89 = or i32 %88, 24
  %90 = add nsw i32 %89, -1
  %91 = add i32 %mul66.i, %89
  %92 = or i32 %88, 25
  %93 = add i32 %92, %mul3.i
  %94 = add i32 %92, %mul11.i
  br label %pregion_for_entry.pregion_for_init.i

pregion_for_entry.pregion_for_init.i:             ; preds = %pregion_for_end.i, %10
  %_local_id_y.0 = phi i64 [ 0, %10 ], [ %358, %pregion_for_end.i ]
  %95 = mul i64 %_local_id_y.0, %18
  %add6.i.i = add nuw nsw i64 %_local_id_y.0, %mul3.i.i
  %conv2.i = trunc i64 %add6.i.i to i32
  %sub4.i = add nsw i32 %conv2.i, -1
  %mul5.i = mul nsw i32 %sub4.i, %4
  %add.i = add nsw i32 %mul5.i, %mul3.i
  %add14.i = add nsw i32 %mul5.i, %mul11.i
  %mul79.i = mul nsw i32 %conv2.i, %4
  %add89.i = add nsw i32 %conv2.i, 1
  %mul90.i = mul nsw i32 %add89.i, %4
  %96 = trunc i64 %95 to i32
  %97 = add i32 %67, %96
  %98 = trunc i64 %95 to i32
  %99 = add i32 %62, %98
  %100 = trunc i64 %95 to i32
  %101 = add i32 %57, %100
  %102 = trunc i64 %95 to i32
  %103 = insertelement <8 x i32> undef, i32 %102, i32 0
  %104 = shufflevector <8 x i32> %103, <8 x i32> undef, <8 x i32> zeroinitializer
  %105 = add <8 x i32> %53, %104
  %106 = icmp sgt <8 x i32> %105, <i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616>
  %107 = icmp sgt i32 %101, 2147483616
  %108 = icmp sgt i32 %99, 2147483616
  %109 = icmp sgt i32 %97, 2147483616
  %110 = call i1 @llvm.experimental.vector.reduce.or.v8i1(<8 x i1> %106)
  %111 = or i1 %110, %107
  %112 = or i1 %111, %108
  %113 = or i1 %112, %109
  br i1 %113, label %pregion_for_entry.entry.i.preheader, label %vector.body

pregion_for_entry.entry.i.preheader:              ; preds = %pregion_for_entry.pregion_for_init.i
  br label %pregion_for_entry.entry.i

vector.body:                                      ; preds = %pregion_for_entry.pregion_for_init.i
  %114 = add nsw i32 %69, %add.i
  %115 = sext i32 %114 to i64
  %116 = getelementptr inbounds float, float* %0, i64 %115
  %117 = bitcast float* %116 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %117, align 4, !tbaa !12, !llvm.access.group !16
  %118 = add nsw i32 %69, %add14.i
  %119 = sext i32 %118 to i64
  %120 = getelementptr inbounds float, float* %0, i64 %119
  %121 = bitcast float* %120 to <8 x float>*
  %wide.load32 = load <8 x float>, <8 x float>* %121, align 4, !tbaa !12, !llvm.access.group !16
  %122 = fmul <8 x float> %wide.load32, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %123 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %122)
  %124 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %123)
  %125 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %124)
  %126 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %125)
  %127 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %126)
  %128 = add i32 %70, %mul5.i
  %129 = sext i32 %128 to i64
  %130 = getelementptr inbounds float, float* %0, i64 %129
  %131 = bitcast float* %130 to <8 x float>*
  %wide.load33 = load <8 x float>, <8 x float>* %131, align 4, !tbaa !12, !llvm.access.group !16
  %132 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %127)
  %133 = add i32 %70, %mul79.i
  %134 = sext i32 %133 to i64
  %135 = getelementptr inbounds float, float* %0, i64 %134
  %136 = bitcast float* %135 to <8 x float>*
  %wide.load34 = load <8 x float>, <8 x float>* %136, align 4, !tbaa !12, !llvm.access.group !16
  %137 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %132)
  %138 = add i32 %70, %mul90.i
  %139 = sext i32 %138 to i64
  %140 = getelementptr inbounds float, float* %0, i64 %139
  %141 = bitcast float* %140 to <8 x float>*
  %wide.load35 = load <8 x float>, <8 x float>* %141, align 4, !tbaa !12, !llvm.access.group !16
  %142 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %137)
  %143 = add nsw i32 %71, %add.i
  %144 = sext i32 %143 to i64
  %145 = getelementptr inbounds float, float* %0, i64 %144
  %146 = bitcast float* %145 to <8 x float>*
  %wide.load36 = load <8 x float>, <8 x float>* %146, align 4, !tbaa !12, !llvm.access.group !16
  %147 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %142)
  %148 = add nsw i32 %71, %add14.i
  %149 = sext i32 %148 to i64
  %150 = getelementptr inbounds float, float* %0, i64 %149
  %151 = bitcast float* %150 to <8 x float>*
  %wide.load37 = load <8 x float>, <8 x float>* %151, align 4, !tbaa !12, !llvm.access.group !16
  %152 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %147)
  %153 = add i32 %72, %mul79.i
  %154 = sext i32 %153 to i64
  %155 = getelementptr inbounds float, float* %0, i64 %154
  %156 = bitcast float* %155 to <8 x float>*
  %wide.load38 = load <8 x float>, <8 x float>* %156, align 4, !tbaa !12, !llvm.access.group !16
  %157 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %152)
  %158 = add i32 %73, %mul79.i
  %159 = sext i32 %158 to i64
  %160 = getelementptr inbounds float, float* %0, i64 %159
  %161 = bitcast float* %160 to <8 x float>*
  %wide.load39 = load <8 x float>, <8 x float>* %161, align 4, !tbaa !12, !llvm.access.group !16
  %162 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %157)
  %163 = add i32 %72, %mul90.i
  %164 = sext i32 %163 to i64
  %165 = getelementptr inbounds float, float* %0, i64 %164
  %166 = bitcast float* %165 to <8 x float>*
  %wide.load40 = load <8 x float>, <8 x float>* %166, align 4, !tbaa !12, !llvm.access.group !16
  %167 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %162)
  %168 = add i32 %73, %mul90.i
  %169 = sext i32 %168 to i64
  %170 = getelementptr inbounds float, float* %0, i64 %169
  %171 = bitcast float* %170 to <8 x float>*
  %wide.load41 = load <8 x float>, <8 x float>* %171, align 4, !tbaa !12, !llvm.access.group !16
  %172 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %167)
  %173 = getelementptr inbounds float, float* %1, i64 %134
  %174 = bitcast float* %173 to <8 x float>*
  store <8 x float> %172, <8 x float>* %174, align 4, !tbaa !12, !llvm.access.group !16
  %175 = add nsw i32 %76, %add.i
  %176 = sext i32 %175 to i64
  %177 = getelementptr inbounds float, float* %0, i64 %176
  %178 = bitcast float* %177 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %178, align 4, !tbaa !12, !llvm.access.group !16
  %179 = add nsw i32 %76, %add14.i
  %180 = sext i32 %179 to i64
  %181 = getelementptr inbounds float, float* %0, i64 %180
  %182 = bitcast float* %181 to <8 x float>*
  %wide.load32.1 = load <8 x float>, <8 x float>* %182, align 4, !tbaa !12, !llvm.access.group !16
  %183 = fmul <8 x float> %wide.load32.1, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %184 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.1, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %183)
  %185 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.1, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %184)
  %186 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.1, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %185)
  %187 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.1, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %186)
  %188 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.1, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %187)
  %189 = add i32 %77, %mul5.i
  %190 = sext i32 %189 to i64
  %191 = getelementptr inbounds float, float* %0, i64 %190
  %192 = bitcast float* %191 to <8 x float>*
  %wide.load33.1 = load <8 x float>, <8 x float>* %192, align 4, !tbaa !12, !llvm.access.group !16
  %193 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33.1, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %188)
  %194 = add i32 %77, %mul79.i
  %195 = sext i32 %194 to i64
  %196 = getelementptr inbounds float, float* %0, i64 %195
  %197 = bitcast float* %196 to <8 x float>*
  %wide.load34.1 = load <8 x float>, <8 x float>* %197, align 4, !tbaa !12, !llvm.access.group !16
  %198 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34.1, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %193)
  %199 = add i32 %77, %mul90.i
  %200 = sext i32 %199 to i64
  %201 = getelementptr inbounds float, float* %0, i64 %200
  %202 = bitcast float* %201 to <8 x float>*
  %wide.load35.1 = load <8 x float>, <8 x float>* %202, align 4, !tbaa !12, !llvm.access.group !16
  %203 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35.1, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %198)
  %204 = add nsw i32 %78, %add.i
  %205 = sext i32 %204 to i64
  %206 = getelementptr inbounds float, float* %0, i64 %205
  %207 = bitcast float* %206 to <8 x float>*
  %wide.load36.1 = load <8 x float>, <8 x float>* %207, align 4, !tbaa !12, !llvm.access.group !16
  %208 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36.1, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %203)
  %209 = add nsw i32 %78, %add14.i
  %210 = sext i32 %209 to i64
  %211 = getelementptr inbounds float, float* %0, i64 %210
  %212 = bitcast float* %211 to <8 x float>*
  %wide.load37.1 = load <8 x float>, <8 x float>* %212, align 4, !tbaa !12, !llvm.access.group !16
  %213 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37.1, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %208)
  %214 = add i32 %79, %mul79.i
  %215 = sext i32 %214 to i64
  %216 = getelementptr inbounds float, float* %0, i64 %215
  %217 = bitcast float* %216 to <8 x float>*
  %wide.load38.1 = load <8 x float>, <8 x float>* %217, align 4, !tbaa !12, !llvm.access.group !16
  %218 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38.1, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %213)
  %219 = add i32 %80, %mul79.i
  %220 = sext i32 %219 to i64
  %221 = getelementptr inbounds float, float* %0, i64 %220
  %222 = bitcast float* %221 to <8 x float>*
  %wide.load39.1 = load <8 x float>, <8 x float>* %222, align 4, !tbaa !12, !llvm.access.group !16
  %223 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39.1, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %218)
  %224 = add i32 %79, %mul90.i
  %225 = sext i32 %224 to i64
  %226 = getelementptr inbounds float, float* %0, i64 %225
  %227 = bitcast float* %226 to <8 x float>*
  %wide.load40.1 = load <8 x float>, <8 x float>* %227, align 4, !tbaa !12, !llvm.access.group !16
  %228 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40.1, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %223)
  %229 = add i32 %80, %mul90.i
  %230 = sext i32 %229 to i64
  %231 = getelementptr inbounds float, float* %0, i64 %230
  %232 = bitcast float* %231 to <8 x float>*
  %wide.load41.1 = load <8 x float>, <8 x float>* %232, align 4, !tbaa !12, !llvm.access.group !16
  %233 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41.1, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %228)
  %234 = getelementptr inbounds float, float* %1, i64 %195
  %235 = bitcast float* %234 to <8 x float>*
  store <8 x float> %233, <8 x float>* %235, align 4, !tbaa !12, !llvm.access.group !16
  %236 = add nsw i32 %83, %add.i
  %237 = sext i32 %236 to i64
  %238 = getelementptr inbounds float, float* %0, i64 %237
  %239 = bitcast float* %238 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %239, align 4, !tbaa !12, !llvm.access.group !16
  %240 = add nsw i32 %83, %add14.i
  %241 = sext i32 %240 to i64
  %242 = getelementptr inbounds float, float* %0, i64 %241
  %243 = bitcast float* %242 to <8 x float>*
  %wide.load32.2 = load <8 x float>, <8 x float>* %243, align 4, !tbaa !12, !llvm.access.group !16
  %244 = fmul <8 x float> %wide.load32.2, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %245 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.2, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %244)
  %246 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.2, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %245)
  %247 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.2, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %246)
  %248 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.2, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %247)
  %249 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.2, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %248)
  %250 = add i32 %84, %mul5.i
  %251 = sext i32 %250 to i64
  %252 = getelementptr inbounds float, float* %0, i64 %251
  %253 = bitcast float* %252 to <8 x float>*
  %wide.load33.2 = load <8 x float>, <8 x float>* %253, align 4, !tbaa !12, !llvm.access.group !16
  %254 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33.2, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %249)
  %255 = add i32 %84, %mul79.i
  %256 = sext i32 %255 to i64
  %257 = getelementptr inbounds float, float* %0, i64 %256
  %258 = bitcast float* %257 to <8 x float>*
  %wide.load34.2 = load <8 x float>, <8 x float>* %258, align 4, !tbaa !12, !llvm.access.group !16
  %259 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34.2, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %254)
  %260 = add i32 %84, %mul90.i
  %261 = sext i32 %260 to i64
  %262 = getelementptr inbounds float, float* %0, i64 %261
  %263 = bitcast float* %262 to <8 x float>*
  %wide.load35.2 = load <8 x float>, <8 x float>* %263, align 4, !tbaa !12, !llvm.access.group !16
  %264 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35.2, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %259)
  %265 = add nsw i32 %85, %add.i
  %266 = sext i32 %265 to i64
  %267 = getelementptr inbounds float, float* %0, i64 %266
  %268 = bitcast float* %267 to <8 x float>*
  %wide.load36.2 = load <8 x float>, <8 x float>* %268, align 4, !tbaa !12, !llvm.access.group !16
  %269 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36.2, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %264)
  %270 = add nsw i32 %85, %add14.i
  %271 = sext i32 %270 to i64
  %272 = getelementptr inbounds float, float* %0, i64 %271
  %273 = bitcast float* %272 to <8 x float>*
  %wide.load37.2 = load <8 x float>, <8 x float>* %273, align 4, !tbaa !12, !llvm.access.group !16
  %274 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37.2, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %269)
  %275 = add i32 %86, %mul79.i
  %276 = sext i32 %275 to i64
  %277 = getelementptr inbounds float, float* %0, i64 %276
  %278 = bitcast float* %277 to <8 x float>*
  %wide.load38.2 = load <8 x float>, <8 x float>* %278, align 4, !tbaa !12, !llvm.access.group !16
  %279 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38.2, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %274)
  %280 = add i32 %87, %mul79.i
  %281 = sext i32 %280 to i64
  %282 = getelementptr inbounds float, float* %0, i64 %281
  %283 = bitcast float* %282 to <8 x float>*
  %wide.load39.2 = load <8 x float>, <8 x float>* %283, align 4, !tbaa !12, !llvm.access.group !16
  %284 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39.2, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %279)
  %285 = add i32 %86, %mul90.i
  %286 = sext i32 %285 to i64
  %287 = getelementptr inbounds float, float* %0, i64 %286
  %288 = bitcast float* %287 to <8 x float>*
  %wide.load40.2 = load <8 x float>, <8 x float>* %288, align 4, !tbaa !12, !llvm.access.group !16
  %289 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40.2, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %284)
  %290 = add i32 %87, %mul90.i
  %291 = sext i32 %290 to i64
  %292 = getelementptr inbounds float, float* %0, i64 %291
  %293 = bitcast float* %292 to <8 x float>*
  %wide.load41.2 = load <8 x float>, <8 x float>* %293, align 4, !tbaa !12, !llvm.access.group !16
  %294 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41.2, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %289)
  %295 = getelementptr inbounds float, float* %1, i64 %256
  %296 = bitcast float* %295 to <8 x float>*
  store <8 x float> %294, <8 x float>* %296, align 4, !tbaa !12, !llvm.access.group !16
  %297 = add nsw i32 %90, %add.i
  %298 = sext i32 %297 to i64
  %299 = getelementptr inbounds float, float* %0, i64 %298
  %300 = bitcast float* %299 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %300, align 4, !tbaa !12, !llvm.access.group !16
  %301 = add nsw i32 %90, %add14.i
  %302 = sext i32 %301 to i64
  %303 = getelementptr inbounds float, float* %0, i64 %302
  %304 = bitcast float* %303 to <8 x float>*
  %wide.load32.3 = load <8 x float>, <8 x float>* %304, align 4, !tbaa !12, !llvm.access.group !16
  %305 = fmul <8 x float> %wide.load32.3, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %306 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.3, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %305)
  %307 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.3, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %306)
  %308 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.3, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %307)
  %309 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.3, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %308)
  %310 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.3, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %309)
  %311 = add i32 %91, %mul5.i
  %312 = sext i32 %311 to i64
  %313 = getelementptr inbounds float, float* %0, i64 %312
  %314 = bitcast float* %313 to <8 x float>*
  %wide.load33.3 = load <8 x float>, <8 x float>* %314, align 4, !tbaa !12, !llvm.access.group !16
  %315 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33.3, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %310)
  %316 = add i32 %91, %mul79.i
  %317 = sext i32 %316 to i64
  %318 = getelementptr inbounds float, float* %0, i64 %317
  %319 = bitcast float* %318 to <8 x float>*
  %wide.load34.3 = load <8 x float>, <8 x float>* %319, align 4, !tbaa !12, !llvm.access.group !16
  %320 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34.3, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %315)
  %321 = add i32 %91, %mul90.i
  %322 = sext i32 %321 to i64
  %323 = getelementptr inbounds float, float* %0, i64 %322
  %324 = bitcast float* %323 to <8 x float>*
  %wide.load35.3 = load <8 x float>, <8 x float>* %324, align 4, !tbaa !12, !llvm.access.group !16
  %325 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35.3, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %320)
  %326 = add nsw i32 %92, %add.i
  %327 = sext i32 %326 to i64
  %328 = getelementptr inbounds float, float* %0, i64 %327
  %329 = bitcast float* %328 to <8 x float>*
  %wide.load36.3 = load <8 x float>, <8 x float>* %329, align 4, !tbaa !12, !llvm.access.group !16
  %330 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36.3, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %325)
  %331 = add nsw i32 %92, %add14.i
  %332 = sext i32 %331 to i64
  %333 = getelementptr inbounds float, float* %0, i64 %332
  %334 = bitcast float* %333 to <8 x float>*
  %wide.load37.3 = load <8 x float>, <8 x float>* %334, align 4, !tbaa !12, !llvm.access.group !16
  %335 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37.3, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %330)
  %336 = add i32 %93, %mul79.i
  %337 = sext i32 %336 to i64
  %338 = getelementptr inbounds float, float* %0, i64 %337
  %339 = bitcast float* %338 to <8 x float>*
  %wide.load38.3 = load <8 x float>, <8 x float>* %339, align 4, !tbaa !12, !llvm.access.group !16
  %340 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38.3, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %335)
  %341 = add i32 %94, %mul79.i
  %342 = sext i32 %341 to i64
  %343 = getelementptr inbounds float, float* %0, i64 %342
  %344 = bitcast float* %343 to <8 x float>*
  %wide.load39.3 = load <8 x float>, <8 x float>* %344, align 4, !tbaa !12, !llvm.access.group !16
  %345 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39.3, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %340)
  %346 = add i32 %93, %mul90.i
  %347 = sext i32 %346 to i64
  %348 = getelementptr inbounds float, float* %0, i64 %347
  %349 = bitcast float* %348 to <8 x float>*
  %wide.load40.3 = load <8 x float>, <8 x float>* %349, align 4, !tbaa !12, !llvm.access.group !16
  %350 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40.3, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %345)
  %351 = add i32 %94, %mul90.i
  %352 = sext i32 %351 to i64
  %353 = getelementptr inbounds float, float* %0, i64 %352
  %354 = bitcast float* %353 to <8 x float>*
  %wide.load41.3 = load <8 x float>, <8 x float>* %354, align 4, !tbaa !12, !llvm.access.group !16
  %355 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41.3, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %350)
  %356 = getelementptr inbounds float, float* %1, i64 %317
  %357 = bitcast float* %356 to <8 x float>*
  store <8 x float> %355, <8 x float>* %357, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i

pregion_for_end.i.loopexit:                       ; preds = %pregion_for_entry.entry.i
  br label %pregion_for_end.i

pregion_for_end.i:                                ; preds = %pregion_for_end.i.loopexit, %vector.body
  %358 = add nuw nsw i64 %_local_id_y.0, 1
  %exitcond1.not = icmp eq i64 %358, 8
  br i1 %exitcond1.not, label %Convolution3D_kernel.exit, label %pregion_for_entry.pregion_for_init.i, !llvm.loop !19

pregion_for_entry.entry.i:                        ; preds = %pregion_for_entry.entry.i, %pregion_for_entry.entry.i.preheader
  %_local_id_x.0 = phi i64 [ %384, %pregion_for_entry.entry.i ], [ 0, %pregion_for_entry.entry.i.preheader ]
  %add1.i.i = add nuw nsw i64 %_local_id_x.0, %mul.i.i
  %conv.i = trunc i64 %add1.i.i to i32
  %sub6.i = add nsw i32 %conv.i, -1
  %add7.i = add nsw i32 %sub6.i, %add.i
  %idxprom.i = sext i32 %add7.i to i64
  %arrayidx.i = getelementptr inbounds float, float* %0, i64 %idxprom.i
  %359 = load float, float* %arrayidx.i, align 4, !tbaa !12, !llvm.access.group !16
  %add16.i = add nsw i32 %sub6.i, %add14.i
  %idxprom17.i = sext i32 %add16.i to i64
  %arrayidx18.i = getelementptr inbounds float, float* %0, i64 %idxprom17.i
  %360 = load float, float* %arrayidx18.i, align 4, !tbaa !12, !llvm.access.group !16
  %mul19.i = fmul float %360, 4.000000e+00
  %361 = tail call float @llvm.fmuladd.f32(float %359, float 2.000000e+00, float %mul19.i) #4
  %362 = tail call float @llvm.fmuladd.f32(float %359, float 5.000000e+00, float %361) #4
  %363 = tail call float @llvm.fmuladd.f32(float %360, float 7.000000e+00, float %362) #4
  %364 = tail call float @llvm.fmuladd.f32(float %359, float -8.000000e+00, float %363) #4
  %365 = tail call float @llvm.fmuladd.f32(float %360, float 1.000000e+01, float %364) #4
  %add69.i = add i32 %mul66.i, %conv.i
  %add71.i = add i32 %add69.i, %mul5.i
  %idxprom72.i = sext i32 %add71.i to i64
  %arrayidx73.i = getelementptr inbounds float, float* %0, i64 %idxprom72.i
  %366 = load float, float* %arrayidx73.i, align 4, !tbaa !12, !llvm.access.group !16
  %367 = tail call float @llvm.fmuladd.f32(float %366, float -3.000000e+00, float %365) #4
  %add82.i = add i32 %add69.i, %mul79.i
  %idxprom83.i = sext i32 %add82.i to i64
  %arrayidx84.i = getelementptr inbounds float, float* %0, i64 %idxprom83.i
  %368 = load float, float* %arrayidx84.i, align 4, !tbaa !12, !llvm.access.group !16
  %369 = tail call float @llvm.fmuladd.f32(float %368, float 6.000000e+00, float %367) #4
  %add93.i = add i32 %add69.i, %mul90.i
  %idxprom94.i = sext i32 %add93.i to i64
  %arrayidx95.i = getelementptr inbounds float, float* %0, i64 %idxprom94.i
  %370 = load float, float* %arrayidx95.i, align 4, !tbaa !12, !llvm.access.group !16
  %371 = tail call float @llvm.fmuladd.f32(float %370, float -9.000000e+00, float %369) #4
  %add103.i = add nsw i32 %conv.i, 1
  %add104.i = add nsw i32 %add103.i, %add.i
  %idxprom105.i = sext i32 %add104.i to i64
  %arrayidx106.i = getelementptr inbounds float, float* %0, i64 %idxprom105.i
  %372 = load float, float* %arrayidx106.i, align 4, !tbaa !12, !llvm.access.group !16
  %373 = tail call float @llvm.fmuladd.f32(float %372, float 2.000000e+00, float %371) #4
  %add115.i = add nsw i32 %add103.i, %add14.i
  %idxprom116.i = sext i32 %add115.i to i64
  %arrayidx117.i = getelementptr inbounds float, float* %0, i64 %idxprom116.i
  %374 = load float, float* %arrayidx117.i, align 4, !tbaa !12, !llvm.access.group !16
  %375 = tail call float @llvm.fmuladd.f32(float %374, float 4.000000e+00, float %373) #4
  %add124.i = add i32 %add103.i, %mul3.i
  %add126.i = add i32 %add124.i, %mul79.i
  %idxprom127.i = sext i32 %add126.i to i64
  %arrayidx128.i = getelementptr inbounds float, float* %0, i64 %idxprom127.i
  %376 = load float, float* %arrayidx128.i, align 4, !tbaa !12, !llvm.access.group !16
  %377 = tail call float @llvm.fmuladd.f32(float %376, float 5.000000e+00, float %375) #4
  %add135.i = add i32 %add103.i, %mul11.i
  %add137.i = add i32 %add135.i, %mul79.i
  %idxprom138.i = sext i32 %add137.i to i64
  %arrayidx139.i = getelementptr inbounds float, float* %0, i64 %idxprom138.i
  %378 = load float, float* %arrayidx139.i, align 4, !tbaa !12, !llvm.access.group !16
  %379 = tail call float @llvm.fmuladd.f32(float %378, float 7.000000e+00, float %377) #4
  %add148.i = add i32 %add124.i, %mul90.i
  %idxprom149.i = sext i32 %add148.i to i64
  %arrayidx150.i = getelementptr inbounds float, float* %0, i64 %idxprom149.i
  %380 = load float, float* %arrayidx150.i, align 4, !tbaa !12, !llvm.access.group !16
  %381 = tail call float @llvm.fmuladd.f32(float %380, float -8.000000e+00, float %379) #4
  %add159.i = add i32 %add135.i, %mul90.i
  %idxprom160.i = sext i32 %add159.i to i64
  %arrayidx161.i = getelementptr inbounds float, float* %0, i64 %idxprom160.i
  %382 = load float, float* %arrayidx161.i, align 4, !tbaa !12, !llvm.access.group !16
  %383 = tail call float @llvm.fmuladd.f32(float %382, float 1.000000e+01, float %381) #4
  %arrayidx169.i = getelementptr inbounds float, float* %1, i64 %idxprom83.i
  store float %383, float* %arrayidx169.i, align 4, !tbaa !12, !llvm.access.group !16
  %384 = add nuw nsw i64 %_local_id_x.0, 1
  %exitcond.not = icmp eq i64 %384, 32
  br i1 %exitcond.not, label %pregion_for_end.i.loopexit, label %pregion_for_entry.entry.i, !llvm.loop !21

Convolution3D_kernel.exit:                        ; preds = %pregion_for_end.i
  ret void
}

; Function Attrs: nofree nounwind
define void @_pocl_kernel_Convolution3D_kernel_workgroup(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #2 {
  %6 = bitcast i8** %0 to float***
  %7 = load float**, float*** %6, align 8
  %8 = load float*, float** %7, align 8
  %9 = getelementptr i8*, i8** %0, i64 1
  %10 = bitcast i8** %9 to float***
  %11 = load float**, float*** %10, align 8
  %12 = load float*, float** %11, align 8
  %13 = getelementptr i8*, i8** %0, i64 3
  %14 = bitcast i8** %13 to i32**
  %15 = load i32*, i32** %14, align 8
  %16 = load i32, i32* %15, align 4
  %17 = getelementptr i8*, i8** %0, i64 4
  %18 = bitcast i8** %17 to i32**
  %19 = load i32*, i32** %18, align 8
  %20 = load i32, i32* %19, align 4
  %21 = getelementptr i8*, i8** %0, i64 5
  %22 = bitcast i8** %21 to i32**
  %23 = load i32*, i32** %22, align 8
  %24 = load i32, i32* %23, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %sub.i.i = add nsw i32 %24, -1
  %mul.i.i = mul nsw i32 %20, %16
  %mul3.i.i = mul nsw i32 %sub.i.i, %mul.i.i
  %add9.i.i = add nsw i32 %24, 1
  %mul11.i.i = mul nsw i32 %add9.i.i, %mul.i.i
  %mul66.i.i = mul nsw i32 %24, %mul.i.i
  %25 = add i32 %24, -1
  %26 = mul i32 %16, %25
  %27 = trunc i64 %3 to i32
  %28 = shl i32 %27, 3
  %29 = add i32 %26, %28
  %30 = add i32 %29, -1
  %31 = mul i32 %20, %30
  %32 = trunc i64 %2 to i32
  %33 = shl i32 %32, 5
  %34 = zext i32 %20 to i64
  %35 = add i32 %24, 1
  %36 = mul i32 %16, %35
  %37 = add i32 %36, %28
  %38 = add i32 %37, -1
  %39 = mul i32 %20, %38
  %40 = mul i32 %16, %24
  %41 = add i32 %40, %28
  %42 = add i32 %41, -1
  %43 = add i32 %40, %28
  %44 = add i32 %43, 1
  %45 = add i32 %26, %28
  %46 = mul i32 %20, %45
  %47 = insertelement <4 x i32> undef, i32 %31, i32 0
  %48 = insertelement <4 x i32> %47, i32 %39, i32 1
  %49 = insertelement <4 x i32> %48, i32 %20, i32 2
  %50 = insertelement <4 x i32> %49, i32 %46, i32 3
  %shuffle = shufflevector <4 x i32> %50, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 2, i32 2, i32 0, i32 1, i32 3>
  %51 = insertelement <4 x i32> undef, i32 %33, i32 0
  %52 = insertelement <4 x i32> %51, i32 %42, i32 1
  %53 = insertelement <4 x i32> %52, i32 %41, i32 2
  %54 = insertelement <4 x i32> %53, i32 %44, i32 3
  %shuffle42 = shufflevector <4 x i32> %54, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 2, i32 3, i32 0, i32 0, i32 0>
  %55 = add <8 x i32> %shuffle, %shuffle42
  %56 = mul <8 x i32> %shuffle, %shuffle42
  %57 = shufflevector <8 x i32> %55, <8 x i32> %56, <8 x i32> <i32 0, i32 1, i32 10, i32 11, i32 12, i32 5, i32 6, i32 7>
  %58 = insertelement <8 x i32> <i32 -1, i32 -1, i32 undef, i32 undef, i32 undef, i32 1, i32 1, i32 1>, i32 %33, i32 2
  %59 = insertelement <8 x i32> %58, i32 %33, i32 3
  %60 = insertelement <8 x i32> %59, i32 %33, i32 4
  %61 = add <8 x i32> %57, %60
  %62 = add i32 %36, %28
  %63 = mul i32 %20, %62
  %64 = add i32 %63, %33
  %65 = add i32 %64, 1
  %66 = add i32 %26, %28
  %67 = add i32 %66, 1
  %68 = mul i32 %20, %67
  %69 = add i32 %68, %33
  %70 = add i32 %69, 1
  %71 = add i32 %36, %28
  %72 = add i32 %71, 1
  %73 = mul i32 %20, %72
  %74 = add i32 %73, %33
  %75 = add i32 %74, 1
  %76 = trunc i64 %mul.i.i.i to i32
  %77 = add nsw i32 %76, -1
  %78 = add i32 %mul66.i.i, %76
  %79 = or i32 %76, 1
  %80 = add i32 %79, %mul3.i.i
  %81 = add i32 %79, %mul11.i.i
  %82 = trunc i64 %mul.i.i.i to i32
  %83 = or i32 %82, 8
  %84 = add nsw i32 %83, -1
  %85 = add i32 %mul66.i.i, %83
  %86 = or i32 %82, 9
  %87 = add i32 %86, %mul3.i.i
  %88 = add i32 %86, %mul11.i.i
  %89 = trunc i64 %mul.i.i.i to i32
  %90 = or i32 %89, 16
  %91 = add nsw i32 %90, -1
  %92 = add i32 %mul66.i.i, %90
  %93 = or i32 %89, 17
  %94 = add i32 %93, %mul3.i.i
  %95 = add i32 %93, %mul11.i.i
  %96 = trunc i64 %mul.i.i.i to i32
  %97 = or i32 %96, 24
  %98 = add nsw i32 %97, -1
  %99 = add i32 %mul66.i.i, %97
  %100 = or i32 %96, 25
  %101 = add i32 %100, %mul3.i.i
  %102 = add i32 %100, %mul11.i.i
  br label %pregion_for_entry.pregion_for_init.i.i

pregion_for_entry.pregion_for_init.i.i:           ; preds = %pregion_for_end.i.i, %5
  %_local_id_y.i.0 = phi i64 [ 0, %5 ], [ %366, %pregion_for_end.i.i ]
  %103 = mul i64 %_local_id_y.i.0, %34
  %add6.i.i.i = add nuw nsw i64 %_local_id_y.i.0, %mul3.i.i.i
  %conv2.i.i = trunc i64 %add6.i.i.i to i32
  %sub4.i.i = add nsw i32 %conv2.i.i, -1
  %mul5.i.i = mul nsw i32 %sub4.i.i, %20
  %add.i.i = add nsw i32 %mul5.i.i, %mul3.i.i
  %add14.i.i = add nsw i32 %mul5.i.i, %mul11.i.i
  %mul79.i.i = mul nsw i32 %20, %conv2.i.i
  %add89.i.i = add nsw i32 %conv2.i.i, 1
  %mul90.i.i = mul nsw i32 %add89.i.i, %20
  %104 = trunc i64 %103 to i32
  %105 = add i32 %75, %104
  %106 = trunc i64 %103 to i32
  %107 = add i32 %70, %106
  %108 = trunc i64 %103 to i32
  %109 = add i32 %65, %108
  %110 = trunc i64 %103 to i32
  %111 = insertelement <8 x i32> undef, i32 %110, i32 0
  %112 = shufflevector <8 x i32> %111, <8 x i32> undef, <8 x i32> zeroinitializer
  %113 = add <8 x i32> %61, %112
  %114 = icmp sgt <8 x i32> %113, <i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616>
  %115 = icmp sgt i32 %109, 2147483616
  %116 = icmp sgt i32 %107, 2147483616
  %117 = icmp sgt i32 %105, 2147483616
  %118 = call i1 @llvm.experimental.vector.reduce.or.v8i1(<8 x i1> %114)
  %119 = or i1 %118, %115
  %120 = or i1 %119, %116
  %121 = or i1 %120, %117
  br i1 %121, label %pregion_for_entry.entry.i.i.preheader, label %vector.body

pregion_for_entry.entry.i.i.preheader:            ; preds = %pregion_for_entry.pregion_for_init.i.i
  br label %pregion_for_entry.entry.i.i

vector.body:                                      ; preds = %pregion_for_entry.pregion_for_init.i.i
  %122 = add nsw i32 %77, %add.i.i
  %123 = sext i32 %122 to i64
  %124 = getelementptr inbounds float, float* %8, i64 %123
  %125 = bitcast float* %124 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %125, align 4, !tbaa !12, !llvm.access.group !16
  %126 = add nsw i32 %77, %add14.i.i
  %127 = sext i32 %126 to i64
  %128 = getelementptr inbounds float, float* %8, i64 %127
  %129 = bitcast float* %128 to <8 x float>*
  %wide.load32 = load <8 x float>, <8 x float>* %129, align 4, !tbaa !12, !llvm.access.group !16
  %130 = fmul <8 x float> %wide.load32, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %131 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %130)
  %132 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %131)
  %133 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %132)
  %134 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %133)
  %135 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %134)
  %136 = add i32 %78, %mul5.i.i
  %137 = sext i32 %136 to i64
  %138 = getelementptr inbounds float, float* %8, i64 %137
  %139 = bitcast float* %138 to <8 x float>*
  %wide.load33 = load <8 x float>, <8 x float>* %139, align 4, !tbaa !12, !llvm.access.group !16
  %140 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %135)
  %141 = add i32 %78, %mul79.i.i
  %142 = sext i32 %141 to i64
  %143 = getelementptr inbounds float, float* %8, i64 %142
  %144 = bitcast float* %143 to <8 x float>*
  %wide.load34 = load <8 x float>, <8 x float>* %144, align 4, !tbaa !12, !llvm.access.group !16
  %145 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %140)
  %146 = add i32 %78, %mul90.i.i
  %147 = sext i32 %146 to i64
  %148 = getelementptr inbounds float, float* %8, i64 %147
  %149 = bitcast float* %148 to <8 x float>*
  %wide.load35 = load <8 x float>, <8 x float>* %149, align 4, !tbaa !12, !llvm.access.group !16
  %150 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %145)
  %151 = add nsw i32 %79, %add.i.i
  %152 = sext i32 %151 to i64
  %153 = getelementptr inbounds float, float* %8, i64 %152
  %154 = bitcast float* %153 to <8 x float>*
  %wide.load36 = load <8 x float>, <8 x float>* %154, align 4, !tbaa !12, !llvm.access.group !16
  %155 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %150)
  %156 = add nsw i32 %79, %add14.i.i
  %157 = sext i32 %156 to i64
  %158 = getelementptr inbounds float, float* %8, i64 %157
  %159 = bitcast float* %158 to <8 x float>*
  %wide.load37 = load <8 x float>, <8 x float>* %159, align 4, !tbaa !12, !llvm.access.group !16
  %160 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %155)
  %161 = add i32 %80, %mul79.i.i
  %162 = sext i32 %161 to i64
  %163 = getelementptr inbounds float, float* %8, i64 %162
  %164 = bitcast float* %163 to <8 x float>*
  %wide.load38 = load <8 x float>, <8 x float>* %164, align 4, !tbaa !12, !llvm.access.group !16
  %165 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %160)
  %166 = add i32 %81, %mul79.i.i
  %167 = sext i32 %166 to i64
  %168 = getelementptr inbounds float, float* %8, i64 %167
  %169 = bitcast float* %168 to <8 x float>*
  %wide.load39 = load <8 x float>, <8 x float>* %169, align 4, !tbaa !12, !llvm.access.group !16
  %170 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %165)
  %171 = add i32 %80, %mul90.i.i
  %172 = sext i32 %171 to i64
  %173 = getelementptr inbounds float, float* %8, i64 %172
  %174 = bitcast float* %173 to <8 x float>*
  %wide.load40 = load <8 x float>, <8 x float>* %174, align 4, !tbaa !12, !llvm.access.group !16
  %175 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %170)
  %176 = add i32 %81, %mul90.i.i
  %177 = sext i32 %176 to i64
  %178 = getelementptr inbounds float, float* %8, i64 %177
  %179 = bitcast float* %178 to <8 x float>*
  %wide.load41 = load <8 x float>, <8 x float>* %179, align 4, !tbaa !12, !llvm.access.group !16
  %180 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %175)
  %181 = getelementptr inbounds float, float* %12, i64 %142
  %182 = bitcast float* %181 to <8 x float>*
  store <8 x float> %180, <8 x float>* %182, align 4, !tbaa !12, !llvm.access.group !16
  %183 = add nsw i32 %84, %add.i.i
  %184 = sext i32 %183 to i64
  %185 = getelementptr inbounds float, float* %8, i64 %184
  %186 = bitcast float* %185 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %186, align 4, !tbaa !12, !llvm.access.group !16
  %187 = add nsw i32 %84, %add14.i.i
  %188 = sext i32 %187 to i64
  %189 = getelementptr inbounds float, float* %8, i64 %188
  %190 = bitcast float* %189 to <8 x float>*
  %wide.load32.1 = load <8 x float>, <8 x float>* %190, align 4, !tbaa !12, !llvm.access.group !16
  %191 = fmul <8 x float> %wide.load32.1, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %192 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.1, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %191)
  %193 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.1, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %192)
  %194 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.1, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %193)
  %195 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.1, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %194)
  %196 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.1, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %195)
  %197 = add i32 %85, %mul5.i.i
  %198 = sext i32 %197 to i64
  %199 = getelementptr inbounds float, float* %8, i64 %198
  %200 = bitcast float* %199 to <8 x float>*
  %wide.load33.1 = load <8 x float>, <8 x float>* %200, align 4, !tbaa !12, !llvm.access.group !16
  %201 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33.1, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %196)
  %202 = add i32 %85, %mul79.i.i
  %203 = sext i32 %202 to i64
  %204 = getelementptr inbounds float, float* %8, i64 %203
  %205 = bitcast float* %204 to <8 x float>*
  %wide.load34.1 = load <8 x float>, <8 x float>* %205, align 4, !tbaa !12, !llvm.access.group !16
  %206 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34.1, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %201)
  %207 = add i32 %85, %mul90.i.i
  %208 = sext i32 %207 to i64
  %209 = getelementptr inbounds float, float* %8, i64 %208
  %210 = bitcast float* %209 to <8 x float>*
  %wide.load35.1 = load <8 x float>, <8 x float>* %210, align 4, !tbaa !12, !llvm.access.group !16
  %211 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35.1, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %206)
  %212 = add nsw i32 %86, %add.i.i
  %213 = sext i32 %212 to i64
  %214 = getelementptr inbounds float, float* %8, i64 %213
  %215 = bitcast float* %214 to <8 x float>*
  %wide.load36.1 = load <8 x float>, <8 x float>* %215, align 4, !tbaa !12, !llvm.access.group !16
  %216 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36.1, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %211)
  %217 = add nsw i32 %86, %add14.i.i
  %218 = sext i32 %217 to i64
  %219 = getelementptr inbounds float, float* %8, i64 %218
  %220 = bitcast float* %219 to <8 x float>*
  %wide.load37.1 = load <8 x float>, <8 x float>* %220, align 4, !tbaa !12, !llvm.access.group !16
  %221 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37.1, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %216)
  %222 = add i32 %87, %mul79.i.i
  %223 = sext i32 %222 to i64
  %224 = getelementptr inbounds float, float* %8, i64 %223
  %225 = bitcast float* %224 to <8 x float>*
  %wide.load38.1 = load <8 x float>, <8 x float>* %225, align 4, !tbaa !12, !llvm.access.group !16
  %226 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38.1, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %221)
  %227 = add i32 %88, %mul79.i.i
  %228 = sext i32 %227 to i64
  %229 = getelementptr inbounds float, float* %8, i64 %228
  %230 = bitcast float* %229 to <8 x float>*
  %wide.load39.1 = load <8 x float>, <8 x float>* %230, align 4, !tbaa !12, !llvm.access.group !16
  %231 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39.1, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %226)
  %232 = add i32 %87, %mul90.i.i
  %233 = sext i32 %232 to i64
  %234 = getelementptr inbounds float, float* %8, i64 %233
  %235 = bitcast float* %234 to <8 x float>*
  %wide.load40.1 = load <8 x float>, <8 x float>* %235, align 4, !tbaa !12, !llvm.access.group !16
  %236 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40.1, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %231)
  %237 = add i32 %88, %mul90.i.i
  %238 = sext i32 %237 to i64
  %239 = getelementptr inbounds float, float* %8, i64 %238
  %240 = bitcast float* %239 to <8 x float>*
  %wide.load41.1 = load <8 x float>, <8 x float>* %240, align 4, !tbaa !12, !llvm.access.group !16
  %241 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41.1, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %236)
  %242 = getelementptr inbounds float, float* %12, i64 %203
  %243 = bitcast float* %242 to <8 x float>*
  store <8 x float> %241, <8 x float>* %243, align 4, !tbaa !12, !llvm.access.group !16
  %244 = add nsw i32 %91, %add.i.i
  %245 = sext i32 %244 to i64
  %246 = getelementptr inbounds float, float* %8, i64 %245
  %247 = bitcast float* %246 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %247, align 4, !tbaa !12, !llvm.access.group !16
  %248 = add nsw i32 %91, %add14.i.i
  %249 = sext i32 %248 to i64
  %250 = getelementptr inbounds float, float* %8, i64 %249
  %251 = bitcast float* %250 to <8 x float>*
  %wide.load32.2 = load <8 x float>, <8 x float>* %251, align 4, !tbaa !12, !llvm.access.group !16
  %252 = fmul <8 x float> %wide.load32.2, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %253 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.2, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %252)
  %254 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.2, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %253)
  %255 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.2, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %254)
  %256 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.2, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %255)
  %257 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.2, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %256)
  %258 = add i32 %92, %mul5.i.i
  %259 = sext i32 %258 to i64
  %260 = getelementptr inbounds float, float* %8, i64 %259
  %261 = bitcast float* %260 to <8 x float>*
  %wide.load33.2 = load <8 x float>, <8 x float>* %261, align 4, !tbaa !12, !llvm.access.group !16
  %262 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33.2, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %257)
  %263 = add i32 %92, %mul79.i.i
  %264 = sext i32 %263 to i64
  %265 = getelementptr inbounds float, float* %8, i64 %264
  %266 = bitcast float* %265 to <8 x float>*
  %wide.load34.2 = load <8 x float>, <8 x float>* %266, align 4, !tbaa !12, !llvm.access.group !16
  %267 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34.2, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %262)
  %268 = add i32 %92, %mul90.i.i
  %269 = sext i32 %268 to i64
  %270 = getelementptr inbounds float, float* %8, i64 %269
  %271 = bitcast float* %270 to <8 x float>*
  %wide.load35.2 = load <8 x float>, <8 x float>* %271, align 4, !tbaa !12, !llvm.access.group !16
  %272 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35.2, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %267)
  %273 = add nsw i32 %93, %add.i.i
  %274 = sext i32 %273 to i64
  %275 = getelementptr inbounds float, float* %8, i64 %274
  %276 = bitcast float* %275 to <8 x float>*
  %wide.load36.2 = load <8 x float>, <8 x float>* %276, align 4, !tbaa !12, !llvm.access.group !16
  %277 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36.2, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %272)
  %278 = add nsw i32 %93, %add14.i.i
  %279 = sext i32 %278 to i64
  %280 = getelementptr inbounds float, float* %8, i64 %279
  %281 = bitcast float* %280 to <8 x float>*
  %wide.load37.2 = load <8 x float>, <8 x float>* %281, align 4, !tbaa !12, !llvm.access.group !16
  %282 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37.2, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %277)
  %283 = add i32 %94, %mul79.i.i
  %284 = sext i32 %283 to i64
  %285 = getelementptr inbounds float, float* %8, i64 %284
  %286 = bitcast float* %285 to <8 x float>*
  %wide.load38.2 = load <8 x float>, <8 x float>* %286, align 4, !tbaa !12, !llvm.access.group !16
  %287 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38.2, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %282)
  %288 = add i32 %95, %mul79.i.i
  %289 = sext i32 %288 to i64
  %290 = getelementptr inbounds float, float* %8, i64 %289
  %291 = bitcast float* %290 to <8 x float>*
  %wide.load39.2 = load <8 x float>, <8 x float>* %291, align 4, !tbaa !12, !llvm.access.group !16
  %292 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39.2, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %287)
  %293 = add i32 %94, %mul90.i.i
  %294 = sext i32 %293 to i64
  %295 = getelementptr inbounds float, float* %8, i64 %294
  %296 = bitcast float* %295 to <8 x float>*
  %wide.load40.2 = load <8 x float>, <8 x float>* %296, align 4, !tbaa !12, !llvm.access.group !16
  %297 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40.2, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %292)
  %298 = add i32 %95, %mul90.i.i
  %299 = sext i32 %298 to i64
  %300 = getelementptr inbounds float, float* %8, i64 %299
  %301 = bitcast float* %300 to <8 x float>*
  %wide.load41.2 = load <8 x float>, <8 x float>* %301, align 4, !tbaa !12, !llvm.access.group !16
  %302 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41.2, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %297)
  %303 = getelementptr inbounds float, float* %12, i64 %264
  %304 = bitcast float* %303 to <8 x float>*
  store <8 x float> %302, <8 x float>* %304, align 4, !tbaa !12, !llvm.access.group !16
  %305 = add nsw i32 %98, %add.i.i
  %306 = sext i32 %305 to i64
  %307 = getelementptr inbounds float, float* %8, i64 %306
  %308 = bitcast float* %307 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %308, align 4, !tbaa !12, !llvm.access.group !16
  %309 = add nsw i32 %98, %add14.i.i
  %310 = sext i32 %309 to i64
  %311 = getelementptr inbounds float, float* %8, i64 %310
  %312 = bitcast float* %311 to <8 x float>*
  %wide.load32.3 = load <8 x float>, <8 x float>* %312, align 4, !tbaa !12, !llvm.access.group !16
  %313 = fmul <8 x float> %wide.load32.3, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %314 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.3, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %313)
  %315 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.3, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %314)
  %316 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.3, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %315)
  %317 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.3, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %316)
  %318 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.3, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %317)
  %319 = add i32 %99, %mul5.i.i
  %320 = sext i32 %319 to i64
  %321 = getelementptr inbounds float, float* %8, i64 %320
  %322 = bitcast float* %321 to <8 x float>*
  %wide.load33.3 = load <8 x float>, <8 x float>* %322, align 4, !tbaa !12, !llvm.access.group !16
  %323 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33.3, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %318)
  %324 = add i32 %99, %mul79.i.i
  %325 = sext i32 %324 to i64
  %326 = getelementptr inbounds float, float* %8, i64 %325
  %327 = bitcast float* %326 to <8 x float>*
  %wide.load34.3 = load <8 x float>, <8 x float>* %327, align 4, !tbaa !12, !llvm.access.group !16
  %328 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34.3, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %323)
  %329 = add i32 %99, %mul90.i.i
  %330 = sext i32 %329 to i64
  %331 = getelementptr inbounds float, float* %8, i64 %330
  %332 = bitcast float* %331 to <8 x float>*
  %wide.load35.3 = load <8 x float>, <8 x float>* %332, align 4, !tbaa !12, !llvm.access.group !16
  %333 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35.3, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %328)
  %334 = add nsw i32 %100, %add.i.i
  %335 = sext i32 %334 to i64
  %336 = getelementptr inbounds float, float* %8, i64 %335
  %337 = bitcast float* %336 to <8 x float>*
  %wide.load36.3 = load <8 x float>, <8 x float>* %337, align 4, !tbaa !12, !llvm.access.group !16
  %338 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36.3, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %333)
  %339 = add nsw i32 %100, %add14.i.i
  %340 = sext i32 %339 to i64
  %341 = getelementptr inbounds float, float* %8, i64 %340
  %342 = bitcast float* %341 to <8 x float>*
  %wide.load37.3 = load <8 x float>, <8 x float>* %342, align 4, !tbaa !12, !llvm.access.group !16
  %343 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37.3, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %338)
  %344 = add i32 %101, %mul79.i.i
  %345 = sext i32 %344 to i64
  %346 = getelementptr inbounds float, float* %8, i64 %345
  %347 = bitcast float* %346 to <8 x float>*
  %wide.load38.3 = load <8 x float>, <8 x float>* %347, align 4, !tbaa !12, !llvm.access.group !16
  %348 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38.3, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %343)
  %349 = add i32 %102, %mul79.i.i
  %350 = sext i32 %349 to i64
  %351 = getelementptr inbounds float, float* %8, i64 %350
  %352 = bitcast float* %351 to <8 x float>*
  %wide.load39.3 = load <8 x float>, <8 x float>* %352, align 4, !tbaa !12, !llvm.access.group !16
  %353 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39.3, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %348)
  %354 = add i32 %101, %mul90.i.i
  %355 = sext i32 %354 to i64
  %356 = getelementptr inbounds float, float* %8, i64 %355
  %357 = bitcast float* %356 to <8 x float>*
  %wide.load40.3 = load <8 x float>, <8 x float>* %357, align 4, !tbaa !12, !llvm.access.group !16
  %358 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40.3, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %353)
  %359 = add i32 %102, %mul90.i.i
  %360 = sext i32 %359 to i64
  %361 = getelementptr inbounds float, float* %8, i64 %360
  %362 = bitcast float* %361 to <8 x float>*
  %wide.load41.3 = load <8 x float>, <8 x float>* %362, align 4, !tbaa !12, !llvm.access.group !16
  %363 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41.3, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %358)
  %364 = getelementptr inbounds float, float* %12, i64 %325
  %365 = bitcast float* %364 to <8 x float>*
  store <8 x float> %363, <8 x float>* %365, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i

pregion_for_end.i.i.loopexit:                     ; preds = %pregion_for_entry.entry.i.i
  br label %pregion_for_end.i.i

pregion_for_end.i.i:                              ; preds = %pregion_for_end.i.i.loopexit, %vector.body
  %366 = add nuw nsw i64 %_local_id_y.i.0, 1
  %exitcond1.not = icmp eq i64 %366, 8
  br i1 %exitcond1.not, label %_pocl_kernel_Convolution3D_kernel.exit, label %pregion_for_entry.pregion_for_init.i.i, !llvm.loop !19

pregion_for_entry.entry.i.i:                      ; preds = %pregion_for_entry.entry.i.i, %pregion_for_entry.entry.i.i.preheader
  %_local_id_x.i.0 = phi i64 [ %392, %pregion_for_entry.entry.i.i ], [ 0, %pregion_for_entry.entry.i.i.preheader ]
  %add1.i.i.i = add nuw nsw i64 %_local_id_x.i.0, %mul.i.i.i
  %conv.i.i = trunc i64 %add1.i.i.i to i32
  %sub6.i.i = add nsw i32 %conv.i.i, -1
  %add7.i.i = add nsw i32 %sub6.i.i, %add.i.i
  %idxprom.i.i = sext i32 %add7.i.i to i64
  %arrayidx.i.i = getelementptr inbounds float, float* %8, i64 %idxprom.i.i
  %367 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %add16.i.i = add nsw i32 %sub6.i.i, %add14.i.i
  %idxprom17.i.i = sext i32 %add16.i.i to i64
  %arrayidx18.i.i = getelementptr inbounds float, float* %8, i64 %idxprom17.i.i
  %368 = load float, float* %arrayidx18.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %mul19.i.i = fmul float %368, 4.000000e+00
  %369 = tail call float @llvm.fmuladd.f32(float %367, float 2.000000e+00, float %mul19.i.i) #4
  %370 = tail call float @llvm.fmuladd.f32(float %367, float 5.000000e+00, float %369) #4
  %371 = tail call float @llvm.fmuladd.f32(float %368, float 7.000000e+00, float %370) #4
  %372 = tail call float @llvm.fmuladd.f32(float %367, float -8.000000e+00, float %371) #4
  %373 = tail call float @llvm.fmuladd.f32(float %368, float 1.000000e+01, float %372) #4
  %add69.i.i = add i32 %mul66.i.i, %conv.i.i
  %add71.i.i = add i32 %add69.i.i, %mul5.i.i
  %idxprom72.i.i = sext i32 %add71.i.i to i64
  %arrayidx73.i.i = getelementptr inbounds float, float* %8, i64 %idxprom72.i.i
  %374 = load float, float* %arrayidx73.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %375 = tail call float @llvm.fmuladd.f32(float %374, float -3.000000e+00, float %373) #4
  %add82.i.i = add i32 %add69.i.i, %mul79.i.i
  %idxprom83.i.i = sext i32 %add82.i.i to i64
  %arrayidx84.i.i = getelementptr inbounds float, float* %8, i64 %idxprom83.i.i
  %376 = load float, float* %arrayidx84.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %377 = tail call float @llvm.fmuladd.f32(float %376, float 6.000000e+00, float %375) #4
  %add93.i.i = add i32 %add69.i.i, %mul90.i.i
  %idxprom94.i.i = sext i32 %add93.i.i to i64
  %arrayidx95.i.i = getelementptr inbounds float, float* %8, i64 %idxprom94.i.i
  %378 = load float, float* %arrayidx95.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %379 = tail call float @llvm.fmuladd.f32(float %378, float -9.000000e+00, float %377) #4
  %add103.i.i = add nsw i32 %conv.i.i, 1
  %add104.i.i = add nsw i32 %add103.i.i, %add.i.i
  %idxprom105.i.i = sext i32 %add104.i.i to i64
  %arrayidx106.i.i = getelementptr inbounds float, float* %8, i64 %idxprom105.i.i
  %380 = load float, float* %arrayidx106.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %381 = tail call float @llvm.fmuladd.f32(float %380, float 2.000000e+00, float %379) #4
  %add115.i.i = add nsw i32 %add103.i.i, %add14.i.i
  %idxprom116.i.i = sext i32 %add115.i.i to i64
  %arrayidx117.i.i = getelementptr inbounds float, float* %8, i64 %idxprom116.i.i
  %382 = load float, float* %arrayidx117.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %383 = tail call float @llvm.fmuladd.f32(float %382, float 4.000000e+00, float %381) #4
  %add124.i.i = add i32 %add103.i.i, %mul3.i.i
  %add126.i.i = add i32 %add124.i.i, %mul79.i.i
  %idxprom127.i.i = sext i32 %add126.i.i to i64
  %arrayidx128.i.i = getelementptr inbounds float, float* %8, i64 %idxprom127.i.i
  %384 = load float, float* %arrayidx128.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %385 = tail call float @llvm.fmuladd.f32(float %384, float 5.000000e+00, float %383) #4
  %add135.i.i = add i32 %add103.i.i, %mul11.i.i
  %add137.i.i = add i32 %add135.i.i, %mul79.i.i
  %idxprom138.i.i = sext i32 %add137.i.i to i64
  %arrayidx139.i.i = getelementptr inbounds float, float* %8, i64 %idxprom138.i.i
  %386 = load float, float* %arrayidx139.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %387 = tail call float @llvm.fmuladd.f32(float %386, float 7.000000e+00, float %385) #4
  %add148.i.i = add i32 %add124.i.i, %mul90.i.i
  %idxprom149.i.i = sext i32 %add148.i.i to i64
  %arrayidx150.i.i = getelementptr inbounds float, float* %8, i64 %idxprom149.i.i
  %388 = load float, float* %arrayidx150.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %389 = tail call float @llvm.fmuladd.f32(float %388, float -8.000000e+00, float %387) #4
  %add159.i.i = add i32 %add135.i.i, %mul90.i.i
  %idxprom160.i.i = sext i32 %add159.i.i to i64
  %arrayidx161.i.i = getelementptr inbounds float, float* %8, i64 %idxprom160.i.i
  %390 = load float, float* %arrayidx161.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %391 = tail call float @llvm.fmuladd.f32(float %390, float 1.000000e+01, float %389) #4
  %arrayidx169.i.i = getelementptr inbounds float, float* %12, i64 %idxprom83.i.i
  store float %391, float* %arrayidx169.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %392 = add nuw nsw i64 %_local_id_x.i.0, 1
  %exitcond.not = icmp eq i64 %392, 32
  br i1 %exitcond.not, label %pregion_for_end.i.i.loopexit, label %pregion_for_entry.entry.i.i, !llvm.loop !24

_pocl_kernel_Convolution3D_kernel.exit:           ; preds = %pregion_for_end.i.i
  ret void
}

; Function Attrs: nofree nounwind
define void @_pocl_kernel_Convolution3D_kernel_workgroup_fast(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #2 {
  %6 = bitcast i8** %0 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr i8*, i8** %0, i64 1
  %9 = bitcast i8** %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr i8*, i8** %0, i64 3
  %12 = bitcast i8** %11 to i32**
  %13 = load i32*, i32** %12, align 8
  %14 = load i32, i32* %13, align 4
  %15 = getelementptr i8*, i8** %0, i64 4
  %16 = bitcast i8** %15 to i32**
  %17 = load i32*, i32** %16, align 8
  %18 = load i32, i32* %17, align 4
  %19 = getelementptr i8*, i8** %0, i64 5
  %20 = bitcast i8** %19 to i32**
  %21 = load i32*, i32** %20, align 8
  %22 = load i32, i32* %21, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %sub.i.i = add nsw i32 %22, -1
  %mul.i.i = mul nsw i32 %18, %14
  %mul3.i.i = mul nsw i32 %sub.i.i, %mul.i.i
  %add9.i.i = add nsw i32 %22, 1
  %mul11.i.i = mul nsw i32 %add9.i.i, %mul.i.i
  %mul66.i.i = mul nsw i32 %22, %mul.i.i
  %23 = add i32 %22, -1
  %24 = mul i32 %14, %23
  %25 = trunc i64 %3 to i32
  %26 = shl i32 %25, 3
  %27 = add i32 %24, %26
  %28 = add i32 %27, -1
  %29 = mul i32 %18, %28
  %30 = trunc i64 %2 to i32
  %31 = shl i32 %30, 5
  %32 = zext i32 %18 to i64
  %33 = add i32 %22, 1
  %34 = mul i32 %14, %33
  %35 = add i32 %34, %26
  %36 = add i32 %35, -1
  %37 = mul i32 %18, %36
  %38 = mul i32 %14, %22
  %39 = add i32 %38, %26
  %40 = add i32 %39, -1
  %41 = add i32 %38, %26
  %42 = add i32 %41, 1
  %43 = add i32 %24, %26
  %44 = mul i32 %18, %43
  %45 = insertelement <4 x i32> undef, i32 %29, i32 0
  %46 = insertelement <4 x i32> %45, i32 %37, i32 1
  %47 = insertelement <4 x i32> %46, i32 %18, i32 2
  %48 = insertelement <4 x i32> %47, i32 %44, i32 3
  %shuffle = shufflevector <4 x i32> %48, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 2, i32 2, i32 0, i32 1, i32 3>
  %49 = insertelement <4 x i32> undef, i32 %31, i32 0
  %50 = insertelement <4 x i32> %49, i32 %40, i32 1
  %51 = insertelement <4 x i32> %50, i32 %39, i32 2
  %52 = insertelement <4 x i32> %51, i32 %42, i32 3
  %shuffle42 = shufflevector <4 x i32> %52, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 2, i32 3, i32 0, i32 0, i32 0>
  %53 = add <8 x i32> %shuffle, %shuffle42
  %54 = mul <8 x i32> %shuffle, %shuffle42
  %55 = shufflevector <8 x i32> %53, <8 x i32> %54, <8 x i32> <i32 0, i32 1, i32 10, i32 11, i32 12, i32 5, i32 6, i32 7>
  %56 = insertelement <8 x i32> <i32 -1, i32 -1, i32 undef, i32 undef, i32 undef, i32 1, i32 1, i32 1>, i32 %31, i32 2
  %57 = insertelement <8 x i32> %56, i32 %31, i32 3
  %58 = insertelement <8 x i32> %57, i32 %31, i32 4
  %59 = add <8 x i32> %55, %58
  %60 = add i32 %34, %26
  %61 = mul i32 %18, %60
  %62 = add i32 %61, %31
  %63 = add i32 %62, 1
  %64 = add i32 %24, %26
  %65 = add i32 %64, 1
  %66 = mul i32 %18, %65
  %67 = add i32 %66, %31
  %68 = add i32 %67, 1
  %69 = add i32 %34, %26
  %70 = add i32 %69, 1
  %71 = mul i32 %18, %70
  %72 = add i32 %71, %31
  %73 = add i32 %72, 1
  %74 = trunc i64 %mul.i.i.i to i32
  %75 = add nsw i32 %74, -1
  %76 = add i32 %mul66.i.i, %74
  %77 = or i32 %74, 1
  %78 = add i32 %77, %mul3.i.i
  %79 = add i32 %77, %mul11.i.i
  %80 = trunc i64 %mul.i.i.i to i32
  %81 = or i32 %80, 8
  %82 = add nsw i32 %81, -1
  %83 = add i32 %mul66.i.i, %81
  %84 = or i32 %80, 9
  %85 = add i32 %84, %mul3.i.i
  %86 = add i32 %84, %mul11.i.i
  %87 = trunc i64 %mul.i.i.i to i32
  %88 = or i32 %87, 16
  %89 = add nsw i32 %88, -1
  %90 = add i32 %mul66.i.i, %88
  %91 = or i32 %87, 17
  %92 = add i32 %91, %mul3.i.i
  %93 = add i32 %91, %mul11.i.i
  %94 = trunc i64 %mul.i.i.i to i32
  %95 = or i32 %94, 24
  %96 = add nsw i32 %95, -1
  %97 = add i32 %mul66.i.i, %95
  %98 = or i32 %94, 25
  %99 = add i32 %98, %mul3.i.i
  %100 = add i32 %98, %mul11.i.i
  br label %pregion_for_entry.pregion_for_init.i.i

pregion_for_entry.pregion_for_init.i.i:           ; preds = %pregion_for_end.i.i, %5
  %_local_id_y.i.0 = phi i64 [ 0, %5 ], [ %364, %pregion_for_end.i.i ]
  %101 = mul i64 %_local_id_y.i.0, %32
  %add6.i.i.i = add nuw nsw i64 %_local_id_y.i.0, %mul3.i.i.i
  %conv2.i.i = trunc i64 %add6.i.i.i to i32
  %sub4.i.i = add nsw i32 %conv2.i.i, -1
  %mul5.i.i = mul nsw i32 %sub4.i.i, %18
  %add.i.i = add nsw i32 %mul5.i.i, %mul3.i.i
  %add14.i.i = add nsw i32 %mul5.i.i, %mul11.i.i
  %mul79.i.i = mul nsw i32 %18, %conv2.i.i
  %add89.i.i = add nsw i32 %conv2.i.i, 1
  %mul90.i.i = mul nsw i32 %add89.i.i, %18
  %102 = trunc i64 %101 to i32
  %103 = add i32 %73, %102
  %104 = trunc i64 %101 to i32
  %105 = add i32 %68, %104
  %106 = trunc i64 %101 to i32
  %107 = add i32 %63, %106
  %108 = trunc i64 %101 to i32
  %109 = insertelement <8 x i32> undef, i32 %108, i32 0
  %110 = shufflevector <8 x i32> %109, <8 x i32> undef, <8 x i32> zeroinitializer
  %111 = add <8 x i32> %59, %110
  %112 = icmp sgt <8 x i32> %111, <i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616, i32 2147483616>
  %113 = icmp sgt i32 %107, 2147483616
  %114 = icmp sgt i32 %105, 2147483616
  %115 = icmp sgt i32 %103, 2147483616
  %116 = call i1 @llvm.experimental.vector.reduce.or.v8i1(<8 x i1> %112)
  %117 = or i1 %116, %113
  %118 = or i1 %117, %114
  %119 = or i1 %118, %115
  br i1 %119, label %pregion_for_entry.entry.i.i.preheader, label %vector.body

pregion_for_entry.entry.i.i.preheader:            ; preds = %pregion_for_entry.pregion_for_init.i.i
  br label %pregion_for_entry.entry.i.i

vector.body:                                      ; preds = %pregion_for_entry.pregion_for_init.i.i
  %120 = add nsw i32 %75, %add.i.i
  %121 = sext i32 %120 to i64
  %122 = getelementptr inbounds float, float* %7, i64 %121
  %123 = bitcast float* %122 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %123, align 4, !tbaa !12, !llvm.access.group !16
  %124 = add nsw i32 %75, %add14.i.i
  %125 = sext i32 %124 to i64
  %126 = getelementptr inbounds float, float* %7, i64 %125
  %127 = bitcast float* %126 to <8 x float>*
  %wide.load32 = load <8 x float>, <8 x float>* %127, align 4, !tbaa !12, !llvm.access.group !16
  %128 = fmul <8 x float> %wide.load32, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %129 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %128)
  %130 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %129)
  %131 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %130)
  %132 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %131)
  %133 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %132)
  %134 = add i32 %76, %mul5.i.i
  %135 = sext i32 %134 to i64
  %136 = getelementptr inbounds float, float* %7, i64 %135
  %137 = bitcast float* %136 to <8 x float>*
  %wide.load33 = load <8 x float>, <8 x float>* %137, align 4, !tbaa !12, !llvm.access.group !16
  %138 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %133)
  %139 = add i32 %76, %mul79.i.i
  %140 = sext i32 %139 to i64
  %141 = getelementptr inbounds float, float* %7, i64 %140
  %142 = bitcast float* %141 to <8 x float>*
  %wide.load34 = load <8 x float>, <8 x float>* %142, align 4, !tbaa !12, !llvm.access.group !16
  %143 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %138)
  %144 = add i32 %76, %mul90.i.i
  %145 = sext i32 %144 to i64
  %146 = getelementptr inbounds float, float* %7, i64 %145
  %147 = bitcast float* %146 to <8 x float>*
  %wide.load35 = load <8 x float>, <8 x float>* %147, align 4, !tbaa !12, !llvm.access.group !16
  %148 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %143)
  %149 = add nsw i32 %77, %add.i.i
  %150 = sext i32 %149 to i64
  %151 = getelementptr inbounds float, float* %7, i64 %150
  %152 = bitcast float* %151 to <8 x float>*
  %wide.load36 = load <8 x float>, <8 x float>* %152, align 4, !tbaa !12, !llvm.access.group !16
  %153 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %148)
  %154 = add nsw i32 %77, %add14.i.i
  %155 = sext i32 %154 to i64
  %156 = getelementptr inbounds float, float* %7, i64 %155
  %157 = bitcast float* %156 to <8 x float>*
  %wide.load37 = load <8 x float>, <8 x float>* %157, align 4, !tbaa !12, !llvm.access.group !16
  %158 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %153)
  %159 = add i32 %78, %mul79.i.i
  %160 = sext i32 %159 to i64
  %161 = getelementptr inbounds float, float* %7, i64 %160
  %162 = bitcast float* %161 to <8 x float>*
  %wide.load38 = load <8 x float>, <8 x float>* %162, align 4, !tbaa !12, !llvm.access.group !16
  %163 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %158)
  %164 = add i32 %79, %mul79.i.i
  %165 = sext i32 %164 to i64
  %166 = getelementptr inbounds float, float* %7, i64 %165
  %167 = bitcast float* %166 to <8 x float>*
  %wide.load39 = load <8 x float>, <8 x float>* %167, align 4, !tbaa !12, !llvm.access.group !16
  %168 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %163)
  %169 = add i32 %78, %mul90.i.i
  %170 = sext i32 %169 to i64
  %171 = getelementptr inbounds float, float* %7, i64 %170
  %172 = bitcast float* %171 to <8 x float>*
  %wide.load40 = load <8 x float>, <8 x float>* %172, align 4, !tbaa !12, !llvm.access.group !16
  %173 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %168)
  %174 = add i32 %79, %mul90.i.i
  %175 = sext i32 %174 to i64
  %176 = getelementptr inbounds float, float* %7, i64 %175
  %177 = bitcast float* %176 to <8 x float>*
  %wide.load41 = load <8 x float>, <8 x float>* %177, align 4, !tbaa !12, !llvm.access.group !16
  %178 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %173)
  %179 = getelementptr inbounds float, float* %10, i64 %140
  %180 = bitcast float* %179 to <8 x float>*
  store <8 x float> %178, <8 x float>* %180, align 4, !tbaa !12, !llvm.access.group !16
  %181 = add nsw i32 %82, %add.i.i
  %182 = sext i32 %181 to i64
  %183 = getelementptr inbounds float, float* %7, i64 %182
  %184 = bitcast float* %183 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %184, align 4, !tbaa !12, !llvm.access.group !16
  %185 = add nsw i32 %82, %add14.i.i
  %186 = sext i32 %185 to i64
  %187 = getelementptr inbounds float, float* %7, i64 %186
  %188 = bitcast float* %187 to <8 x float>*
  %wide.load32.1 = load <8 x float>, <8 x float>* %188, align 4, !tbaa !12, !llvm.access.group !16
  %189 = fmul <8 x float> %wide.load32.1, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %190 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.1, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %189)
  %191 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.1, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %190)
  %192 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.1, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %191)
  %193 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.1, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %192)
  %194 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.1, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %193)
  %195 = add i32 %83, %mul5.i.i
  %196 = sext i32 %195 to i64
  %197 = getelementptr inbounds float, float* %7, i64 %196
  %198 = bitcast float* %197 to <8 x float>*
  %wide.load33.1 = load <8 x float>, <8 x float>* %198, align 4, !tbaa !12, !llvm.access.group !16
  %199 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33.1, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %194)
  %200 = add i32 %83, %mul79.i.i
  %201 = sext i32 %200 to i64
  %202 = getelementptr inbounds float, float* %7, i64 %201
  %203 = bitcast float* %202 to <8 x float>*
  %wide.load34.1 = load <8 x float>, <8 x float>* %203, align 4, !tbaa !12, !llvm.access.group !16
  %204 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34.1, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %199)
  %205 = add i32 %83, %mul90.i.i
  %206 = sext i32 %205 to i64
  %207 = getelementptr inbounds float, float* %7, i64 %206
  %208 = bitcast float* %207 to <8 x float>*
  %wide.load35.1 = load <8 x float>, <8 x float>* %208, align 4, !tbaa !12, !llvm.access.group !16
  %209 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35.1, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %204)
  %210 = add nsw i32 %84, %add.i.i
  %211 = sext i32 %210 to i64
  %212 = getelementptr inbounds float, float* %7, i64 %211
  %213 = bitcast float* %212 to <8 x float>*
  %wide.load36.1 = load <8 x float>, <8 x float>* %213, align 4, !tbaa !12, !llvm.access.group !16
  %214 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36.1, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %209)
  %215 = add nsw i32 %84, %add14.i.i
  %216 = sext i32 %215 to i64
  %217 = getelementptr inbounds float, float* %7, i64 %216
  %218 = bitcast float* %217 to <8 x float>*
  %wide.load37.1 = load <8 x float>, <8 x float>* %218, align 4, !tbaa !12, !llvm.access.group !16
  %219 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37.1, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %214)
  %220 = add i32 %85, %mul79.i.i
  %221 = sext i32 %220 to i64
  %222 = getelementptr inbounds float, float* %7, i64 %221
  %223 = bitcast float* %222 to <8 x float>*
  %wide.load38.1 = load <8 x float>, <8 x float>* %223, align 4, !tbaa !12, !llvm.access.group !16
  %224 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38.1, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %219)
  %225 = add i32 %86, %mul79.i.i
  %226 = sext i32 %225 to i64
  %227 = getelementptr inbounds float, float* %7, i64 %226
  %228 = bitcast float* %227 to <8 x float>*
  %wide.load39.1 = load <8 x float>, <8 x float>* %228, align 4, !tbaa !12, !llvm.access.group !16
  %229 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39.1, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %224)
  %230 = add i32 %85, %mul90.i.i
  %231 = sext i32 %230 to i64
  %232 = getelementptr inbounds float, float* %7, i64 %231
  %233 = bitcast float* %232 to <8 x float>*
  %wide.load40.1 = load <8 x float>, <8 x float>* %233, align 4, !tbaa !12, !llvm.access.group !16
  %234 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40.1, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %229)
  %235 = add i32 %86, %mul90.i.i
  %236 = sext i32 %235 to i64
  %237 = getelementptr inbounds float, float* %7, i64 %236
  %238 = bitcast float* %237 to <8 x float>*
  %wide.load41.1 = load <8 x float>, <8 x float>* %238, align 4, !tbaa !12, !llvm.access.group !16
  %239 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41.1, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %234)
  %240 = getelementptr inbounds float, float* %10, i64 %201
  %241 = bitcast float* %240 to <8 x float>*
  store <8 x float> %239, <8 x float>* %241, align 4, !tbaa !12, !llvm.access.group !16
  %242 = add nsw i32 %89, %add.i.i
  %243 = sext i32 %242 to i64
  %244 = getelementptr inbounds float, float* %7, i64 %243
  %245 = bitcast float* %244 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %245, align 4, !tbaa !12, !llvm.access.group !16
  %246 = add nsw i32 %89, %add14.i.i
  %247 = sext i32 %246 to i64
  %248 = getelementptr inbounds float, float* %7, i64 %247
  %249 = bitcast float* %248 to <8 x float>*
  %wide.load32.2 = load <8 x float>, <8 x float>* %249, align 4, !tbaa !12, !llvm.access.group !16
  %250 = fmul <8 x float> %wide.load32.2, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %251 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.2, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %250)
  %252 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.2, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %251)
  %253 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.2, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %252)
  %254 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.2, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %253)
  %255 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.2, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %254)
  %256 = add i32 %90, %mul5.i.i
  %257 = sext i32 %256 to i64
  %258 = getelementptr inbounds float, float* %7, i64 %257
  %259 = bitcast float* %258 to <8 x float>*
  %wide.load33.2 = load <8 x float>, <8 x float>* %259, align 4, !tbaa !12, !llvm.access.group !16
  %260 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33.2, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %255)
  %261 = add i32 %90, %mul79.i.i
  %262 = sext i32 %261 to i64
  %263 = getelementptr inbounds float, float* %7, i64 %262
  %264 = bitcast float* %263 to <8 x float>*
  %wide.load34.2 = load <8 x float>, <8 x float>* %264, align 4, !tbaa !12, !llvm.access.group !16
  %265 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34.2, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %260)
  %266 = add i32 %90, %mul90.i.i
  %267 = sext i32 %266 to i64
  %268 = getelementptr inbounds float, float* %7, i64 %267
  %269 = bitcast float* %268 to <8 x float>*
  %wide.load35.2 = load <8 x float>, <8 x float>* %269, align 4, !tbaa !12, !llvm.access.group !16
  %270 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35.2, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %265)
  %271 = add nsw i32 %91, %add.i.i
  %272 = sext i32 %271 to i64
  %273 = getelementptr inbounds float, float* %7, i64 %272
  %274 = bitcast float* %273 to <8 x float>*
  %wide.load36.2 = load <8 x float>, <8 x float>* %274, align 4, !tbaa !12, !llvm.access.group !16
  %275 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36.2, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %270)
  %276 = add nsw i32 %91, %add14.i.i
  %277 = sext i32 %276 to i64
  %278 = getelementptr inbounds float, float* %7, i64 %277
  %279 = bitcast float* %278 to <8 x float>*
  %wide.load37.2 = load <8 x float>, <8 x float>* %279, align 4, !tbaa !12, !llvm.access.group !16
  %280 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37.2, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %275)
  %281 = add i32 %92, %mul79.i.i
  %282 = sext i32 %281 to i64
  %283 = getelementptr inbounds float, float* %7, i64 %282
  %284 = bitcast float* %283 to <8 x float>*
  %wide.load38.2 = load <8 x float>, <8 x float>* %284, align 4, !tbaa !12, !llvm.access.group !16
  %285 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38.2, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %280)
  %286 = add i32 %93, %mul79.i.i
  %287 = sext i32 %286 to i64
  %288 = getelementptr inbounds float, float* %7, i64 %287
  %289 = bitcast float* %288 to <8 x float>*
  %wide.load39.2 = load <8 x float>, <8 x float>* %289, align 4, !tbaa !12, !llvm.access.group !16
  %290 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39.2, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %285)
  %291 = add i32 %92, %mul90.i.i
  %292 = sext i32 %291 to i64
  %293 = getelementptr inbounds float, float* %7, i64 %292
  %294 = bitcast float* %293 to <8 x float>*
  %wide.load40.2 = load <8 x float>, <8 x float>* %294, align 4, !tbaa !12, !llvm.access.group !16
  %295 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40.2, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %290)
  %296 = add i32 %93, %mul90.i.i
  %297 = sext i32 %296 to i64
  %298 = getelementptr inbounds float, float* %7, i64 %297
  %299 = bitcast float* %298 to <8 x float>*
  %wide.load41.2 = load <8 x float>, <8 x float>* %299, align 4, !tbaa !12, !llvm.access.group !16
  %300 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41.2, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %295)
  %301 = getelementptr inbounds float, float* %10, i64 %262
  %302 = bitcast float* %301 to <8 x float>*
  store <8 x float> %300, <8 x float>* %302, align 4, !tbaa !12, !llvm.access.group !16
  %303 = add nsw i32 %96, %add.i.i
  %304 = sext i32 %303 to i64
  %305 = getelementptr inbounds float, float* %7, i64 %304
  %306 = bitcast float* %305 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %306, align 4, !tbaa !12, !llvm.access.group !16
  %307 = add nsw i32 %96, %add14.i.i
  %308 = sext i32 %307 to i64
  %309 = getelementptr inbounds float, float* %7, i64 %308
  %310 = bitcast float* %309 to <8 x float>*
  %wide.load32.3 = load <8 x float>, <8 x float>* %310, align 4, !tbaa !12, !llvm.access.group !16
  %311 = fmul <8 x float> %wide.load32.3, <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>
  %312 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.3, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %311)
  %313 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.3, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %312)
  %314 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.3, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %313)
  %315 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load.3, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %314)
  %316 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load32.3, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %315)
  %317 = add i32 %97, %mul5.i.i
  %318 = sext i32 %317 to i64
  %319 = getelementptr inbounds float, float* %7, i64 %318
  %320 = bitcast float* %319 to <8 x float>*
  %wide.load33.3 = load <8 x float>, <8 x float>* %320, align 4, !tbaa !12, !llvm.access.group !16
  %321 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load33.3, <8 x float> <float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00, float -3.000000e+00>, <8 x float> %316)
  %322 = add i32 %97, %mul79.i.i
  %323 = sext i32 %322 to i64
  %324 = getelementptr inbounds float, float* %7, i64 %323
  %325 = bitcast float* %324 to <8 x float>*
  %wide.load34.3 = load <8 x float>, <8 x float>* %325, align 4, !tbaa !12, !llvm.access.group !16
  %326 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load34.3, <8 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>, <8 x float> %321)
  %327 = add i32 %97, %mul90.i.i
  %328 = sext i32 %327 to i64
  %329 = getelementptr inbounds float, float* %7, i64 %328
  %330 = bitcast float* %329 to <8 x float>*
  %wide.load35.3 = load <8 x float>, <8 x float>* %330, align 4, !tbaa !12, !llvm.access.group !16
  %331 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load35.3, <8 x float> <float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00, float -9.000000e+00>, <8 x float> %326)
  %332 = add nsw i32 %98, %add.i.i
  %333 = sext i32 %332 to i64
  %334 = getelementptr inbounds float, float* %7, i64 %333
  %335 = bitcast float* %334 to <8 x float>*
  %wide.load36.3 = load <8 x float>, <8 x float>* %335, align 4, !tbaa !12, !llvm.access.group !16
  %336 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load36.3, <8 x float> <float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00, float 2.000000e+00>, <8 x float> %331)
  %337 = add nsw i32 %98, %add14.i.i
  %338 = sext i32 %337 to i64
  %339 = getelementptr inbounds float, float* %7, i64 %338
  %340 = bitcast float* %339 to <8 x float>*
  %wide.load37.3 = load <8 x float>, <8 x float>* %340, align 4, !tbaa !12, !llvm.access.group !16
  %341 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load37.3, <8 x float> <float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00, float 4.000000e+00>, <8 x float> %336)
  %342 = add i32 %99, %mul79.i.i
  %343 = sext i32 %342 to i64
  %344 = getelementptr inbounds float, float* %7, i64 %343
  %345 = bitcast float* %344 to <8 x float>*
  %wide.load38.3 = load <8 x float>, <8 x float>* %345, align 4, !tbaa !12, !llvm.access.group !16
  %346 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load38.3, <8 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, <8 x float> %341)
  %347 = add i32 %100, %mul79.i.i
  %348 = sext i32 %347 to i64
  %349 = getelementptr inbounds float, float* %7, i64 %348
  %350 = bitcast float* %349 to <8 x float>*
  %wide.load39.3 = load <8 x float>, <8 x float>* %350, align 4, !tbaa !12, !llvm.access.group !16
  %351 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load39.3, <8 x float> <float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00, float 7.000000e+00>, <8 x float> %346)
  %352 = add i32 %99, %mul90.i.i
  %353 = sext i32 %352 to i64
  %354 = getelementptr inbounds float, float* %7, i64 %353
  %355 = bitcast float* %354 to <8 x float>*
  %wide.load40.3 = load <8 x float>, <8 x float>* %355, align 4, !tbaa !12, !llvm.access.group !16
  %356 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load40.3, <8 x float> <float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00, float -8.000000e+00>, <8 x float> %351)
  %357 = add i32 %100, %mul90.i.i
  %358 = sext i32 %357 to i64
  %359 = getelementptr inbounds float, float* %7, i64 %358
  %360 = bitcast float* %359 to <8 x float>*
  %wide.load41.3 = load <8 x float>, <8 x float>* %360, align 4, !tbaa !12, !llvm.access.group !16
  %361 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %wide.load41.3, <8 x float> <float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01, float 1.000000e+01>, <8 x float> %356)
  %362 = getelementptr inbounds float, float* %10, i64 %323
  %363 = bitcast float* %362 to <8 x float>*
  store <8 x float> %361, <8 x float>* %363, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i

pregion_for_end.i.i.loopexit:                     ; preds = %pregion_for_entry.entry.i.i
  br label %pregion_for_end.i.i

pregion_for_end.i.i:                              ; preds = %pregion_for_end.i.i.loopexit, %vector.body
  %364 = add nuw nsw i64 %_local_id_y.i.0, 1
  %exitcond1.not = icmp eq i64 %364, 8
  br i1 %exitcond1.not, label %_pocl_kernel_Convolution3D_kernel.exit, label %pregion_for_entry.pregion_for_init.i.i, !llvm.loop !19

pregion_for_entry.entry.i.i:                      ; preds = %pregion_for_entry.entry.i.i, %pregion_for_entry.entry.i.i.preheader
  %_local_id_x.i.0 = phi i64 [ %390, %pregion_for_entry.entry.i.i ], [ 0, %pregion_for_entry.entry.i.i.preheader ]
  %add1.i.i.i = add nuw nsw i64 %_local_id_x.i.0, %mul.i.i.i
  %conv.i.i = trunc i64 %add1.i.i.i to i32
  %sub6.i.i = add nsw i32 %conv.i.i, -1
  %add7.i.i = add nsw i32 %sub6.i.i, %add.i.i
  %idxprom.i.i = sext i32 %add7.i.i to i64
  %arrayidx.i.i = getelementptr inbounds float, float* %7, i64 %idxprom.i.i
  %365 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %add16.i.i = add nsw i32 %sub6.i.i, %add14.i.i
  %idxprom17.i.i = sext i32 %add16.i.i to i64
  %arrayidx18.i.i = getelementptr inbounds float, float* %7, i64 %idxprom17.i.i
  %366 = load float, float* %arrayidx18.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %mul19.i.i = fmul float %366, 4.000000e+00
  %367 = tail call float @llvm.fmuladd.f32(float %365, float 2.000000e+00, float %mul19.i.i) #4
  %368 = tail call float @llvm.fmuladd.f32(float %365, float 5.000000e+00, float %367) #4
  %369 = tail call float @llvm.fmuladd.f32(float %366, float 7.000000e+00, float %368) #4
  %370 = tail call float @llvm.fmuladd.f32(float %365, float -8.000000e+00, float %369) #4
  %371 = tail call float @llvm.fmuladd.f32(float %366, float 1.000000e+01, float %370) #4
  %add69.i.i = add i32 %mul66.i.i, %conv.i.i
  %add71.i.i = add i32 %add69.i.i, %mul5.i.i
  %idxprom72.i.i = sext i32 %add71.i.i to i64
  %arrayidx73.i.i = getelementptr inbounds float, float* %7, i64 %idxprom72.i.i
  %372 = load float, float* %arrayidx73.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %373 = tail call float @llvm.fmuladd.f32(float %372, float -3.000000e+00, float %371) #4
  %add82.i.i = add i32 %add69.i.i, %mul79.i.i
  %idxprom83.i.i = sext i32 %add82.i.i to i64
  %arrayidx84.i.i = getelementptr inbounds float, float* %7, i64 %idxprom83.i.i
  %374 = load float, float* %arrayidx84.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %375 = tail call float @llvm.fmuladd.f32(float %374, float 6.000000e+00, float %373) #4
  %add93.i.i = add i32 %add69.i.i, %mul90.i.i
  %idxprom94.i.i = sext i32 %add93.i.i to i64
  %arrayidx95.i.i = getelementptr inbounds float, float* %7, i64 %idxprom94.i.i
  %376 = load float, float* %arrayidx95.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %377 = tail call float @llvm.fmuladd.f32(float %376, float -9.000000e+00, float %375) #4
  %add103.i.i = add nsw i32 %conv.i.i, 1
  %add104.i.i = add nsw i32 %add103.i.i, %add.i.i
  %idxprom105.i.i = sext i32 %add104.i.i to i64
  %arrayidx106.i.i = getelementptr inbounds float, float* %7, i64 %idxprom105.i.i
  %378 = load float, float* %arrayidx106.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %379 = tail call float @llvm.fmuladd.f32(float %378, float 2.000000e+00, float %377) #4
  %add115.i.i = add nsw i32 %add103.i.i, %add14.i.i
  %idxprom116.i.i = sext i32 %add115.i.i to i64
  %arrayidx117.i.i = getelementptr inbounds float, float* %7, i64 %idxprom116.i.i
  %380 = load float, float* %arrayidx117.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %381 = tail call float @llvm.fmuladd.f32(float %380, float 4.000000e+00, float %379) #4
  %add124.i.i = add i32 %add103.i.i, %mul3.i.i
  %add126.i.i = add i32 %add124.i.i, %mul79.i.i
  %idxprom127.i.i = sext i32 %add126.i.i to i64
  %arrayidx128.i.i = getelementptr inbounds float, float* %7, i64 %idxprom127.i.i
  %382 = load float, float* %arrayidx128.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %383 = tail call float @llvm.fmuladd.f32(float %382, float 5.000000e+00, float %381) #4
  %add135.i.i = add i32 %add103.i.i, %mul11.i.i
  %add137.i.i = add i32 %add135.i.i, %mul79.i.i
  %idxprom138.i.i = sext i32 %add137.i.i to i64
  %arrayidx139.i.i = getelementptr inbounds float, float* %7, i64 %idxprom138.i.i
  %384 = load float, float* %arrayidx139.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %385 = tail call float @llvm.fmuladd.f32(float %384, float 7.000000e+00, float %383) #4
  %add148.i.i = add i32 %add124.i.i, %mul90.i.i
  %idxprom149.i.i = sext i32 %add148.i.i to i64
  %arrayidx150.i.i = getelementptr inbounds float, float* %7, i64 %idxprom149.i.i
  %386 = load float, float* %arrayidx150.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %387 = tail call float @llvm.fmuladd.f32(float %386, float -8.000000e+00, float %385) #4
  %add159.i.i = add i32 %add135.i.i, %mul90.i.i
  %idxprom160.i.i = sext i32 %add159.i.i to i64
  %arrayidx161.i.i = getelementptr inbounds float, float* %7, i64 %idxprom160.i.i
  %388 = load float, float* %arrayidx161.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %389 = tail call float @llvm.fmuladd.f32(float %388, float 1.000000e+01, float %387) #4
  %arrayidx169.i.i = getelementptr inbounds float, float* %10, i64 %idxprom83.i.i
  store float %389, float* %arrayidx169.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %390 = add nuw nsw i64 %_local_id_x.i.0, 1
  %exitcond.not = icmp eq i64 %390, 32
  br i1 %exitcond.not, label %pregion_for_end.i.i.loopexit, label %pregion_for_entry.entry.i.i, !llvm.loop !25

_pocl_kernel_Convolution3D_kernel.exit:           ; preds = %pregion_for_end.i.i
  ret void
}

; Function Attrs: nounwind readnone speculatable willreturn
declare <8 x float> @llvm.fmuladd.v8f32(<8 x float>, <8 x float>, <8 x float>) #0

; Function Attrs: nounwind readnone willreturn
declare i1 @llvm.experimental.vector.reduce.or.v8i1(<8 x i1>) #3

attributes #0 = { nounwind readnone speculatable willreturn }
attributes #1 = { alwaysinline nofree norecurse nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "stackrealign" "target-cpu"="skylake" "target-features"="+adx,+aes,+avx,+avx2,+bmi,+bmi2,+clflushopt,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+prfchw,+rdrnd,+rdseed,+sahf,+sgx,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "uniform-work-group-size"="true" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { nofree nounwind }
attributes #3 = { nounwind readnone willreturn }
attributes #4 = { nounwind }

!llvm.module.flags = !{!0, !1, !2}
!opencl.ocl.version = !{!3}
!llvm.ident = !{!4}
!opencl.spir.version = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 1, i32 2}
!4 = !{!"clang version 11.0.0 (git@github.com:llvm/llvm-project.git 91e89f9a5115b0f83b8f026e1ad0e6d1f885fa9b)"}
!5 = !{i32 1, i32 1, i32 0, i32 0, i32 0, i32 0}
!6 = !{!"none", !"none", !"none", !"none", !"none", !"none"}
!7 = !{!"DATA_TYPE*", !"DATA_TYPE*", !"int", !"int", !"int", !"int"}
!8 = !{!"float*", !"float*", !"int", !"int", !"int", !"int"}
!9 = !{!"", !"", !"", !"", !"", !""}
!10 = !{!"A", !"B", !"ni", !"nj", !"nk", !"i"}
!11 = !{i32 1}
!12 = !{!13, !13, i64 0}
!13 = !{!"float", !14, i64 0}
!14 = !{!"omnipotent char", !15, i64 0}
!15 = !{!"Simple C/C++ TBAA"}
!16 = !{!17, !18}
!17 = distinct !{}
!18 = distinct !{}
!19 = distinct !{!19, !20}
!20 = !{!"llvm.loop.parallel_accesses", !18}
!21 = distinct !{!21, !22, !23}
!22 = !{!"llvm.loop.parallel_accesses", !17}
!23 = !{!"llvm.loop.isvectorized", i32 1}
!24 = distinct !{!24, !22, !23}
!25 = distinct !{!25, !22, !23}
