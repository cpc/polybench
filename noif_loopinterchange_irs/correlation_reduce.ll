; ModuleID = './MG/JFHMMJMIFKMDPIMHNAHNJMKBLGBGENDPNOHAD/reduce_kernel/32-8-1-goffs0-smallgrid/parallel.bc'
source_filename = "parallel_bc"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind readnone speculatable willreturn
declare float @llvm.sqrt.f32(float) #0

; Function Attrs: alwaysinline nofree norecurse nounwind
define void @_pocl_kernel_reduce_kernel(float* nocapture readonly %0, float* nocapture readonly %1, float* nocapture %2, float %3, i32 %4, i32 %5, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %6, i64 %7, i64 %8, i64 %9) local_unnamed_addr #1 !kernel_arg_addr_space !5 !kernel_arg_access_qual !6 !kernel_arg_type !7 !kernel_arg_base_type !8 !kernel_arg_type_qual !9 !kernel_arg_name !10 !pocl_generated !11 {
pregion_for_entry.pregion_for_init.i:
  %mul.i.i = shl i64 %7, 5
  %mul3.i.i = shl i64 %8, 3
  %10 = tail call float @llvm.sqrt.f32(float %3) #3
  %conv2.i = trunc i64 %mul3.i.i to i32
  %mul.i = mul nsw i32 %conv2.i, %4
  %broadcast.splatinsert = insertelement <8 x float> undef, float %10, i32 0
  %broadcast.splat = shufflevector <8 x float> %broadcast.splatinsert, <8 x float> undef, <8 x i32> zeroinitializer
  %11 = trunc i64 %mul.i.i to i32
  %12 = shl i64 %7, 37
  %13 = ashr exact i64 %12, 32
  %14 = getelementptr inbounds float, float* %0, i64 %13
  %15 = bitcast float* %14 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %15, align 4, !tbaa !12, !llvm.access.group !16
  %16 = add nsw i32 %mul.i, %11
  %17 = sext i32 %16 to i64
  %18 = getelementptr inbounds float, float* %2, i64 %17
  %19 = bitcast float* %18 to <8 x float>*
  %wide.load2 = load <8 x float>, <8 x float>* %19, align 4, !tbaa !12, !llvm.access.group !16
  %20 = fsub <8 x float> %wide.load2, %wide.load
  %21 = bitcast float* %18 to <8 x float>*
  store <8 x float> %20, <8 x float>* %21, align 4, !tbaa !12, !llvm.access.group !16
  %22 = getelementptr inbounds float, float* %1, i64 %13
  %23 = bitcast float* %22 to <8 x float>*
  %wide.load3 = load <8 x float>, <8 x float>* %23, align 4, !tbaa !12, !llvm.access.group !16
  %24 = fmul <8 x float> %broadcast.splat, %wide.load3
  %25 = fdiv <8 x float> %20, %24, !fpmath !19
  %26 = bitcast float* %18 to <8 x float>*
  store <8 x float> %25, <8 x float>* %26, align 4, !tbaa !12, !llvm.access.group !16
  %27 = or i64 %mul.i.i, 8
  %28 = trunc i64 %27 to i32
  %29 = shl i64 %27, 32
  %30 = ashr exact i64 %29, 32
  %31 = getelementptr inbounds float, float* %0, i64 %30
  %32 = bitcast float* %31 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %32, align 4, !tbaa !12, !llvm.access.group !16
  %33 = add nsw i32 %mul.i, %28
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds float, float* %2, i64 %34
  %36 = bitcast float* %35 to <8 x float>*
  %wide.load2.1 = load <8 x float>, <8 x float>* %36, align 4, !tbaa !12, !llvm.access.group !16
  %37 = fsub <8 x float> %wide.load2.1, %wide.load.1
  %38 = bitcast float* %35 to <8 x float>*
  store <8 x float> %37, <8 x float>* %38, align 4, !tbaa !12, !llvm.access.group !16
  %39 = getelementptr inbounds float, float* %1, i64 %30
  %40 = bitcast float* %39 to <8 x float>*
  %wide.load3.1 = load <8 x float>, <8 x float>* %40, align 4, !tbaa !12, !llvm.access.group !16
  %41 = fmul <8 x float> %broadcast.splat, %wide.load3.1
  %42 = fdiv <8 x float> %37, %41, !fpmath !19
  %43 = bitcast float* %35 to <8 x float>*
  store <8 x float> %42, <8 x float>* %43, align 4, !tbaa !12, !llvm.access.group !16
  %44 = or i64 %mul.i.i, 16
  %45 = trunc i64 %44 to i32
  %46 = shl i64 %44, 32
  %47 = ashr exact i64 %46, 32
  %48 = getelementptr inbounds float, float* %0, i64 %47
  %49 = bitcast float* %48 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %49, align 4, !tbaa !12, !llvm.access.group !16
  %50 = add nsw i32 %mul.i, %45
  %51 = sext i32 %50 to i64
  %52 = getelementptr inbounds float, float* %2, i64 %51
  %53 = bitcast float* %52 to <8 x float>*
  %wide.load2.2 = load <8 x float>, <8 x float>* %53, align 4, !tbaa !12, !llvm.access.group !16
  %54 = fsub <8 x float> %wide.load2.2, %wide.load.2
  %55 = bitcast float* %52 to <8 x float>*
  store <8 x float> %54, <8 x float>* %55, align 4, !tbaa !12, !llvm.access.group !16
  %56 = getelementptr inbounds float, float* %1, i64 %47
  %57 = bitcast float* %56 to <8 x float>*
  %wide.load3.2 = load <8 x float>, <8 x float>* %57, align 4, !tbaa !12, !llvm.access.group !16
  %58 = fmul <8 x float> %broadcast.splat, %wide.load3.2
  %59 = fdiv <8 x float> %54, %58, !fpmath !19
  %60 = bitcast float* %52 to <8 x float>*
  store <8 x float> %59, <8 x float>* %60, align 4, !tbaa !12, !llvm.access.group !16
  %61 = or i64 %mul.i.i, 24
  %62 = trunc i64 %61 to i32
  %63 = shl i64 %61, 32
  %64 = ashr exact i64 %63, 32
  %65 = getelementptr inbounds float, float* %0, i64 %64
  %66 = bitcast float* %65 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %66, align 4, !tbaa !12, !llvm.access.group !16
  %67 = add nsw i32 %mul.i, %62
  %68 = sext i32 %67 to i64
  %69 = getelementptr inbounds float, float* %2, i64 %68
  %70 = bitcast float* %69 to <8 x float>*
  %wide.load2.3 = load <8 x float>, <8 x float>* %70, align 4, !tbaa !12, !llvm.access.group !16
  %71 = fsub <8 x float> %wide.load2.3, %wide.load.3
  %72 = bitcast float* %69 to <8 x float>*
  store <8 x float> %71, <8 x float>* %72, align 4, !tbaa !12, !llvm.access.group !16
  %73 = getelementptr inbounds float, float* %1, i64 %64
  %74 = bitcast float* %73 to <8 x float>*
  %wide.load3.3 = load <8 x float>, <8 x float>* %74, align 4, !tbaa !12, !llvm.access.group !16
  %75 = fmul <8 x float> %broadcast.splat, %wide.load3.3
  %76 = fdiv <8 x float> %71, %75, !fpmath !19
  %77 = bitcast float* %69 to <8 x float>*
  store <8 x float> %76, <8 x float>* %77, align 4, !tbaa !12, !llvm.access.group !16
  %78 = trunc i64 %mul3.i.i to i32
  %conv2.i.1 = or i32 %78, 1
  %mul.i.1 = mul nsw i32 %conv2.i.1, %4
  %broadcast.splatinsert15 = insertelement <8 x float> undef, float %10, i32 0
  %broadcast.splat16 = shufflevector <8 x float> %broadcast.splatinsert15, <8 x float> undef, <8 x i32> zeroinitializer
  %79 = trunc i64 %mul.i.i to i32
  %80 = shl i64 %7, 37
  %81 = ashr exact i64 %80, 32
  %82 = getelementptr inbounds float, float* %0, i64 %81
  %83 = bitcast float* %82 to <8 x float>*
  %wide.load12 = load <8 x float>, <8 x float>* %83, align 4, !tbaa !12, !llvm.access.group !16
  %84 = add nsw i32 %mul.i.1, %79
  %85 = sext i32 %84 to i64
  %86 = getelementptr inbounds float, float* %2, i64 %85
  %87 = bitcast float* %86 to <8 x float>*
  %wide.load13 = load <8 x float>, <8 x float>* %87, align 4, !tbaa !12, !llvm.access.group !16
  %88 = fsub <8 x float> %wide.load13, %wide.load12
  %89 = bitcast float* %86 to <8 x float>*
  store <8 x float> %88, <8 x float>* %89, align 4, !tbaa !12, !llvm.access.group !16
  %90 = getelementptr inbounds float, float* %1, i64 %81
  %91 = bitcast float* %90 to <8 x float>*
  %wide.load14 = load <8 x float>, <8 x float>* %91, align 4, !tbaa !12, !llvm.access.group !16
  %92 = fmul <8 x float> %broadcast.splat16, %wide.load14
  %93 = fdiv <8 x float> %88, %92, !fpmath !19
  %94 = bitcast float* %86 to <8 x float>*
  store <8 x float> %93, <8 x float>* %94, align 4, !tbaa !12, !llvm.access.group !16
  %95 = or i64 %mul.i.i, 8
  %96 = trunc i64 %95 to i32
  %97 = shl i64 %95, 32
  %98 = ashr exact i64 %97, 32
  %99 = getelementptr inbounds float, float* %0, i64 %98
  %100 = bitcast float* %99 to <8 x float>*
  %wide.load12.1 = load <8 x float>, <8 x float>* %100, align 4, !tbaa !12, !llvm.access.group !16
  %101 = add nsw i32 %mul.i.1, %96
  %102 = sext i32 %101 to i64
  %103 = getelementptr inbounds float, float* %2, i64 %102
  %104 = bitcast float* %103 to <8 x float>*
  %wide.load13.1 = load <8 x float>, <8 x float>* %104, align 4, !tbaa !12, !llvm.access.group !16
  %105 = fsub <8 x float> %wide.load13.1, %wide.load12.1
  %106 = bitcast float* %103 to <8 x float>*
  store <8 x float> %105, <8 x float>* %106, align 4, !tbaa !12, !llvm.access.group !16
  %107 = getelementptr inbounds float, float* %1, i64 %98
  %108 = bitcast float* %107 to <8 x float>*
  %wide.load14.1 = load <8 x float>, <8 x float>* %108, align 4, !tbaa !12, !llvm.access.group !16
  %109 = fmul <8 x float> %broadcast.splat16, %wide.load14.1
  %110 = fdiv <8 x float> %105, %109, !fpmath !19
  %111 = bitcast float* %103 to <8 x float>*
  store <8 x float> %110, <8 x float>* %111, align 4, !tbaa !12, !llvm.access.group !16
  %112 = or i64 %mul.i.i, 16
  %113 = trunc i64 %112 to i32
  %114 = shl i64 %112, 32
  %115 = ashr exact i64 %114, 32
  %116 = getelementptr inbounds float, float* %0, i64 %115
  %117 = bitcast float* %116 to <8 x float>*
  %wide.load12.2 = load <8 x float>, <8 x float>* %117, align 4, !tbaa !12, !llvm.access.group !16
  %118 = add nsw i32 %mul.i.1, %113
  %119 = sext i32 %118 to i64
  %120 = getelementptr inbounds float, float* %2, i64 %119
  %121 = bitcast float* %120 to <8 x float>*
  %wide.load13.2 = load <8 x float>, <8 x float>* %121, align 4, !tbaa !12, !llvm.access.group !16
  %122 = fsub <8 x float> %wide.load13.2, %wide.load12.2
  %123 = bitcast float* %120 to <8 x float>*
  store <8 x float> %122, <8 x float>* %123, align 4, !tbaa !12, !llvm.access.group !16
  %124 = getelementptr inbounds float, float* %1, i64 %115
  %125 = bitcast float* %124 to <8 x float>*
  %wide.load14.2 = load <8 x float>, <8 x float>* %125, align 4, !tbaa !12, !llvm.access.group !16
  %126 = fmul <8 x float> %broadcast.splat16, %wide.load14.2
  %127 = fdiv <8 x float> %122, %126, !fpmath !19
  %128 = bitcast float* %120 to <8 x float>*
  store <8 x float> %127, <8 x float>* %128, align 4, !tbaa !12, !llvm.access.group !16
  %129 = or i64 %mul.i.i, 24
  %130 = trunc i64 %129 to i32
  %131 = shl i64 %129, 32
  %132 = ashr exact i64 %131, 32
  %133 = getelementptr inbounds float, float* %0, i64 %132
  %134 = bitcast float* %133 to <8 x float>*
  %wide.load12.3 = load <8 x float>, <8 x float>* %134, align 4, !tbaa !12, !llvm.access.group !16
  %135 = add nsw i32 %mul.i.1, %130
  %136 = sext i32 %135 to i64
  %137 = getelementptr inbounds float, float* %2, i64 %136
  %138 = bitcast float* %137 to <8 x float>*
  %wide.load13.3 = load <8 x float>, <8 x float>* %138, align 4, !tbaa !12, !llvm.access.group !16
  %139 = fsub <8 x float> %wide.load13.3, %wide.load12.3
  %140 = bitcast float* %137 to <8 x float>*
  store <8 x float> %139, <8 x float>* %140, align 4, !tbaa !12, !llvm.access.group !16
  %141 = getelementptr inbounds float, float* %1, i64 %132
  %142 = bitcast float* %141 to <8 x float>*
  %wide.load14.3 = load <8 x float>, <8 x float>* %142, align 4, !tbaa !12, !llvm.access.group !16
  %143 = fmul <8 x float> %broadcast.splat16, %wide.load14.3
  %144 = fdiv <8 x float> %139, %143, !fpmath !19
  %145 = bitcast float* %137 to <8 x float>*
  store <8 x float> %144, <8 x float>* %145, align 4, !tbaa !12, !llvm.access.group !16
  %146 = trunc i64 %mul3.i.i to i32
  %conv2.i.2 = or i32 %146, 2
  %mul.i.2 = mul nsw i32 %conv2.i.2, %4
  %broadcast.splatinsert28 = insertelement <8 x float> undef, float %10, i32 0
  %broadcast.splat29 = shufflevector <8 x float> %broadcast.splatinsert28, <8 x float> undef, <8 x i32> zeroinitializer
  %147 = trunc i64 %mul.i.i to i32
  %148 = shl i64 %7, 37
  %149 = ashr exact i64 %148, 32
  %150 = getelementptr inbounds float, float* %0, i64 %149
  %151 = bitcast float* %150 to <8 x float>*
  %wide.load25 = load <8 x float>, <8 x float>* %151, align 4, !tbaa !12, !llvm.access.group !16
  %152 = add nsw i32 %mul.i.2, %147
  %153 = sext i32 %152 to i64
  %154 = getelementptr inbounds float, float* %2, i64 %153
  %155 = bitcast float* %154 to <8 x float>*
  %wide.load26 = load <8 x float>, <8 x float>* %155, align 4, !tbaa !12, !llvm.access.group !16
  %156 = fsub <8 x float> %wide.load26, %wide.load25
  %157 = bitcast float* %154 to <8 x float>*
  store <8 x float> %156, <8 x float>* %157, align 4, !tbaa !12, !llvm.access.group !16
  %158 = getelementptr inbounds float, float* %1, i64 %149
  %159 = bitcast float* %158 to <8 x float>*
  %wide.load27 = load <8 x float>, <8 x float>* %159, align 4, !tbaa !12, !llvm.access.group !16
  %160 = fmul <8 x float> %broadcast.splat29, %wide.load27
  %161 = fdiv <8 x float> %156, %160, !fpmath !19
  %162 = bitcast float* %154 to <8 x float>*
  store <8 x float> %161, <8 x float>* %162, align 4, !tbaa !12, !llvm.access.group !16
  %163 = or i64 %mul.i.i, 8
  %164 = trunc i64 %163 to i32
  %165 = shl i64 %163, 32
  %166 = ashr exact i64 %165, 32
  %167 = getelementptr inbounds float, float* %0, i64 %166
  %168 = bitcast float* %167 to <8 x float>*
  %wide.load25.1 = load <8 x float>, <8 x float>* %168, align 4, !tbaa !12, !llvm.access.group !16
  %169 = add nsw i32 %mul.i.2, %164
  %170 = sext i32 %169 to i64
  %171 = getelementptr inbounds float, float* %2, i64 %170
  %172 = bitcast float* %171 to <8 x float>*
  %wide.load26.1 = load <8 x float>, <8 x float>* %172, align 4, !tbaa !12, !llvm.access.group !16
  %173 = fsub <8 x float> %wide.load26.1, %wide.load25.1
  %174 = bitcast float* %171 to <8 x float>*
  store <8 x float> %173, <8 x float>* %174, align 4, !tbaa !12, !llvm.access.group !16
  %175 = getelementptr inbounds float, float* %1, i64 %166
  %176 = bitcast float* %175 to <8 x float>*
  %wide.load27.1 = load <8 x float>, <8 x float>* %176, align 4, !tbaa !12, !llvm.access.group !16
  %177 = fmul <8 x float> %broadcast.splat29, %wide.load27.1
  %178 = fdiv <8 x float> %173, %177, !fpmath !19
  %179 = bitcast float* %171 to <8 x float>*
  store <8 x float> %178, <8 x float>* %179, align 4, !tbaa !12, !llvm.access.group !16
  %180 = or i64 %mul.i.i, 16
  %181 = trunc i64 %180 to i32
  %182 = shl i64 %180, 32
  %183 = ashr exact i64 %182, 32
  %184 = getelementptr inbounds float, float* %0, i64 %183
  %185 = bitcast float* %184 to <8 x float>*
  %wide.load25.2 = load <8 x float>, <8 x float>* %185, align 4, !tbaa !12, !llvm.access.group !16
  %186 = add nsw i32 %mul.i.2, %181
  %187 = sext i32 %186 to i64
  %188 = getelementptr inbounds float, float* %2, i64 %187
  %189 = bitcast float* %188 to <8 x float>*
  %wide.load26.2 = load <8 x float>, <8 x float>* %189, align 4, !tbaa !12, !llvm.access.group !16
  %190 = fsub <8 x float> %wide.load26.2, %wide.load25.2
  %191 = bitcast float* %188 to <8 x float>*
  store <8 x float> %190, <8 x float>* %191, align 4, !tbaa !12, !llvm.access.group !16
  %192 = getelementptr inbounds float, float* %1, i64 %183
  %193 = bitcast float* %192 to <8 x float>*
  %wide.load27.2 = load <8 x float>, <8 x float>* %193, align 4, !tbaa !12, !llvm.access.group !16
  %194 = fmul <8 x float> %broadcast.splat29, %wide.load27.2
  %195 = fdiv <8 x float> %190, %194, !fpmath !19
  %196 = bitcast float* %188 to <8 x float>*
  store <8 x float> %195, <8 x float>* %196, align 4, !tbaa !12, !llvm.access.group !16
  %197 = or i64 %mul.i.i, 24
  %198 = trunc i64 %197 to i32
  %199 = shl i64 %197, 32
  %200 = ashr exact i64 %199, 32
  %201 = getelementptr inbounds float, float* %0, i64 %200
  %202 = bitcast float* %201 to <8 x float>*
  %wide.load25.3 = load <8 x float>, <8 x float>* %202, align 4, !tbaa !12, !llvm.access.group !16
  %203 = add nsw i32 %mul.i.2, %198
  %204 = sext i32 %203 to i64
  %205 = getelementptr inbounds float, float* %2, i64 %204
  %206 = bitcast float* %205 to <8 x float>*
  %wide.load26.3 = load <8 x float>, <8 x float>* %206, align 4, !tbaa !12, !llvm.access.group !16
  %207 = fsub <8 x float> %wide.load26.3, %wide.load25.3
  %208 = bitcast float* %205 to <8 x float>*
  store <8 x float> %207, <8 x float>* %208, align 4, !tbaa !12, !llvm.access.group !16
  %209 = getelementptr inbounds float, float* %1, i64 %200
  %210 = bitcast float* %209 to <8 x float>*
  %wide.load27.3 = load <8 x float>, <8 x float>* %210, align 4, !tbaa !12, !llvm.access.group !16
  %211 = fmul <8 x float> %broadcast.splat29, %wide.load27.3
  %212 = fdiv <8 x float> %207, %211, !fpmath !19
  %213 = bitcast float* %205 to <8 x float>*
  store <8 x float> %212, <8 x float>* %213, align 4, !tbaa !12, !llvm.access.group !16
  %214 = trunc i64 %mul3.i.i to i32
  %conv2.i.3 = or i32 %214, 3
  %mul.i.3 = mul nsw i32 %conv2.i.3, %4
  %broadcast.splatinsert41 = insertelement <8 x float> undef, float %10, i32 0
  %broadcast.splat42 = shufflevector <8 x float> %broadcast.splatinsert41, <8 x float> undef, <8 x i32> zeroinitializer
  %215 = trunc i64 %mul.i.i to i32
  %216 = shl i64 %7, 37
  %217 = ashr exact i64 %216, 32
  %218 = getelementptr inbounds float, float* %0, i64 %217
  %219 = bitcast float* %218 to <8 x float>*
  %wide.load38 = load <8 x float>, <8 x float>* %219, align 4, !tbaa !12, !llvm.access.group !16
  %220 = add nsw i32 %mul.i.3, %215
  %221 = sext i32 %220 to i64
  %222 = getelementptr inbounds float, float* %2, i64 %221
  %223 = bitcast float* %222 to <8 x float>*
  %wide.load39 = load <8 x float>, <8 x float>* %223, align 4, !tbaa !12, !llvm.access.group !16
  %224 = fsub <8 x float> %wide.load39, %wide.load38
  %225 = bitcast float* %222 to <8 x float>*
  store <8 x float> %224, <8 x float>* %225, align 4, !tbaa !12, !llvm.access.group !16
  %226 = getelementptr inbounds float, float* %1, i64 %217
  %227 = bitcast float* %226 to <8 x float>*
  %wide.load40 = load <8 x float>, <8 x float>* %227, align 4, !tbaa !12, !llvm.access.group !16
  %228 = fmul <8 x float> %broadcast.splat42, %wide.load40
  %229 = fdiv <8 x float> %224, %228, !fpmath !19
  %230 = bitcast float* %222 to <8 x float>*
  store <8 x float> %229, <8 x float>* %230, align 4, !tbaa !12, !llvm.access.group !16
  %231 = or i64 %mul.i.i, 8
  %232 = trunc i64 %231 to i32
  %233 = shl i64 %231, 32
  %234 = ashr exact i64 %233, 32
  %235 = getelementptr inbounds float, float* %0, i64 %234
  %236 = bitcast float* %235 to <8 x float>*
  %wide.load38.1 = load <8 x float>, <8 x float>* %236, align 4, !tbaa !12, !llvm.access.group !16
  %237 = add nsw i32 %mul.i.3, %232
  %238 = sext i32 %237 to i64
  %239 = getelementptr inbounds float, float* %2, i64 %238
  %240 = bitcast float* %239 to <8 x float>*
  %wide.load39.1 = load <8 x float>, <8 x float>* %240, align 4, !tbaa !12, !llvm.access.group !16
  %241 = fsub <8 x float> %wide.load39.1, %wide.load38.1
  %242 = bitcast float* %239 to <8 x float>*
  store <8 x float> %241, <8 x float>* %242, align 4, !tbaa !12, !llvm.access.group !16
  %243 = getelementptr inbounds float, float* %1, i64 %234
  %244 = bitcast float* %243 to <8 x float>*
  %wide.load40.1 = load <8 x float>, <8 x float>* %244, align 4, !tbaa !12, !llvm.access.group !16
  %245 = fmul <8 x float> %broadcast.splat42, %wide.load40.1
  %246 = fdiv <8 x float> %241, %245, !fpmath !19
  %247 = bitcast float* %239 to <8 x float>*
  store <8 x float> %246, <8 x float>* %247, align 4, !tbaa !12, !llvm.access.group !16
  %248 = or i64 %mul.i.i, 16
  %249 = trunc i64 %248 to i32
  %250 = shl i64 %248, 32
  %251 = ashr exact i64 %250, 32
  %252 = getelementptr inbounds float, float* %0, i64 %251
  %253 = bitcast float* %252 to <8 x float>*
  %wide.load38.2 = load <8 x float>, <8 x float>* %253, align 4, !tbaa !12, !llvm.access.group !16
  %254 = add nsw i32 %mul.i.3, %249
  %255 = sext i32 %254 to i64
  %256 = getelementptr inbounds float, float* %2, i64 %255
  %257 = bitcast float* %256 to <8 x float>*
  %wide.load39.2 = load <8 x float>, <8 x float>* %257, align 4, !tbaa !12, !llvm.access.group !16
  %258 = fsub <8 x float> %wide.load39.2, %wide.load38.2
  %259 = bitcast float* %256 to <8 x float>*
  store <8 x float> %258, <8 x float>* %259, align 4, !tbaa !12, !llvm.access.group !16
  %260 = getelementptr inbounds float, float* %1, i64 %251
  %261 = bitcast float* %260 to <8 x float>*
  %wide.load40.2 = load <8 x float>, <8 x float>* %261, align 4, !tbaa !12, !llvm.access.group !16
  %262 = fmul <8 x float> %broadcast.splat42, %wide.load40.2
  %263 = fdiv <8 x float> %258, %262, !fpmath !19
  %264 = bitcast float* %256 to <8 x float>*
  store <8 x float> %263, <8 x float>* %264, align 4, !tbaa !12, !llvm.access.group !16
  %265 = or i64 %mul.i.i, 24
  %266 = trunc i64 %265 to i32
  %267 = shl i64 %265, 32
  %268 = ashr exact i64 %267, 32
  %269 = getelementptr inbounds float, float* %0, i64 %268
  %270 = bitcast float* %269 to <8 x float>*
  %wide.load38.3 = load <8 x float>, <8 x float>* %270, align 4, !tbaa !12, !llvm.access.group !16
  %271 = add nsw i32 %mul.i.3, %266
  %272 = sext i32 %271 to i64
  %273 = getelementptr inbounds float, float* %2, i64 %272
  %274 = bitcast float* %273 to <8 x float>*
  %wide.load39.3 = load <8 x float>, <8 x float>* %274, align 4, !tbaa !12, !llvm.access.group !16
  %275 = fsub <8 x float> %wide.load39.3, %wide.load38.3
  %276 = bitcast float* %273 to <8 x float>*
  store <8 x float> %275, <8 x float>* %276, align 4, !tbaa !12, !llvm.access.group !16
  %277 = getelementptr inbounds float, float* %1, i64 %268
  %278 = bitcast float* %277 to <8 x float>*
  %wide.load40.3 = load <8 x float>, <8 x float>* %278, align 4, !tbaa !12, !llvm.access.group !16
  %279 = fmul <8 x float> %broadcast.splat42, %wide.load40.3
  %280 = fdiv <8 x float> %275, %279, !fpmath !19
  %281 = bitcast float* %273 to <8 x float>*
  store <8 x float> %280, <8 x float>* %281, align 4, !tbaa !12, !llvm.access.group !16
  %282 = trunc i64 %mul3.i.i to i32
  %conv2.i.4 = or i32 %282, 4
  %mul.i.4 = mul nsw i32 %conv2.i.4, %4
  %broadcast.splatinsert54 = insertelement <8 x float> undef, float %10, i32 0
  %broadcast.splat55 = shufflevector <8 x float> %broadcast.splatinsert54, <8 x float> undef, <8 x i32> zeroinitializer
  %283 = trunc i64 %mul.i.i to i32
  %284 = shl i64 %7, 37
  %285 = ashr exact i64 %284, 32
  %286 = getelementptr inbounds float, float* %0, i64 %285
  %287 = bitcast float* %286 to <8 x float>*
  %wide.load51 = load <8 x float>, <8 x float>* %287, align 4, !tbaa !12, !llvm.access.group !16
  %288 = add nsw i32 %mul.i.4, %283
  %289 = sext i32 %288 to i64
  %290 = getelementptr inbounds float, float* %2, i64 %289
  %291 = bitcast float* %290 to <8 x float>*
  %wide.load52 = load <8 x float>, <8 x float>* %291, align 4, !tbaa !12, !llvm.access.group !16
  %292 = fsub <8 x float> %wide.load52, %wide.load51
  %293 = bitcast float* %290 to <8 x float>*
  store <8 x float> %292, <8 x float>* %293, align 4, !tbaa !12, !llvm.access.group !16
  %294 = getelementptr inbounds float, float* %1, i64 %285
  %295 = bitcast float* %294 to <8 x float>*
  %wide.load53 = load <8 x float>, <8 x float>* %295, align 4, !tbaa !12, !llvm.access.group !16
  %296 = fmul <8 x float> %broadcast.splat55, %wide.load53
  %297 = fdiv <8 x float> %292, %296, !fpmath !19
  %298 = bitcast float* %290 to <8 x float>*
  store <8 x float> %297, <8 x float>* %298, align 4, !tbaa !12, !llvm.access.group !16
  %299 = or i64 %mul.i.i, 8
  %300 = trunc i64 %299 to i32
  %301 = shl i64 %299, 32
  %302 = ashr exact i64 %301, 32
  %303 = getelementptr inbounds float, float* %0, i64 %302
  %304 = bitcast float* %303 to <8 x float>*
  %wide.load51.1 = load <8 x float>, <8 x float>* %304, align 4, !tbaa !12, !llvm.access.group !16
  %305 = add nsw i32 %mul.i.4, %300
  %306 = sext i32 %305 to i64
  %307 = getelementptr inbounds float, float* %2, i64 %306
  %308 = bitcast float* %307 to <8 x float>*
  %wide.load52.1 = load <8 x float>, <8 x float>* %308, align 4, !tbaa !12, !llvm.access.group !16
  %309 = fsub <8 x float> %wide.load52.1, %wide.load51.1
  %310 = bitcast float* %307 to <8 x float>*
  store <8 x float> %309, <8 x float>* %310, align 4, !tbaa !12, !llvm.access.group !16
  %311 = getelementptr inbounds float, float* %1, i64 %302
  %312 = bitcast float* %311 to <8 x float>*
  %wide.load53.1 = load <8 x float>, <8 x float>* %312, align 4, !tbaa !12, !llvm.access.group !16
  %313 = fmul <8 x float> %broadcast.splat55, %wide.load53.1
  %314 = fdiv <8 x float> %309, %313, !fpmath !19
  %315 = bitcast float* %307 to <8 x float>*
  store <8 x float> %314, <8 x float>* %315, align 4, !tbaa !12, !llvm.access.group !16
  %316 = or i64 %mul.i.i, 16
  %317 = trunc i64 %316 to i32
  %318 = shl i64 %316, 32
  %319 = ashr exact i64 %318, 32
  %320 = getelementptr inbounds float, float* %0, i64 %319
  %321 = bitcast float* %320 to <8 x float>*
  %wide.load51.2 = load <8 x float>, <8 x float>* %321, align 4, !tbaa !12, !llvm.access.group !16
  %322 = add nsw i32 %mul.i.4, %317
  %323 = sext i32 %322 to i64
  %324 = getelementptr inbounds float, float* %2, i64 %323
  %325 = bitcast float* %324 to <8 x float>*
  %wide.load52.2 = load <8 x float>, <8 x float>* %325, align 4, !tbaa !12, !llvm.access.group !16
  %326 = fsub <8 x float> %wide.load52.2, %wide.load51.2
  %327 = bitcast float* %324 to <8 x float>*
  store <8 x float> %326, <8 x float>* %327, align 4, !tbaa !12, !llvm.access.group !16
  %328 = getelementptr inbounds float, float* %1, i64 %319
  %329 = bitcast float* %328 to <8 x float>*
  %wide.load53.2 = load <8 x float>, <8 x float>* %329, align 4, !tbaa !12, !llvm.access.group !16
  %330 = fmul <8 x float> %broadcast.splat55, %wide.load53.2
  %331 = fdiv <8 x float> %326, %330, !fpmath !19
  %332 = bitcast float* %324 to <8 x float>*
  store <8 x float> %331, <8 x float>* %332, align 4, !tbaa !12, !llvm.access.group !16
  %333 = or i64 %mul.i.i, 24
  %334 = trunc i64 %333 to i32
  %335 = shl i64 %333, 32
  %336 = ashr exact i64 %335, 32
  %337 = getelementptr inbounds float, float* %0, i64 %336
  %338 = bitcast float* %337 to <8 x float>*
  %wide.load51.3 = load <8 x float>, <8 x float>* %338, align 4, !tbaa !12, !llvm.access.group !16
  %339 = add nsw i32 %mul.i.4, %334
  %340 = sext i32 %339 to i64
  %341 = getelementptr inbounds float, float* %2, i64 %340
  %342 = bitcast float* %341 to <8 x float>*
  %wide.load52.3 = load <8 x float>, <8 x float>* %342, align 4, !tbaa !12, !llvm.access.group !16
  %343 = fsub <8 x float> %wide.load52.3, %wide.load51.3
  %344 = bitcast float* %341 to <8 x float>*
  store <8 x float> %343, <8 x float>* %344, align 4, !tbaa !12, !llvm.access.group !16
  %345 = getelementptr inbounds float, float* %1, i64 %336
  %346 = bitcast float* %345 to <8 x float>*
  %wide.load53.3 = load <8 x float>, <8 x float>* %346, align 4, !tbaa !12, !llvm.access.group !16
  %347 = fmul <8 x float> %broadcast.splat55, %wide.load53.3
  %348 = fdiv <8 x float> %343, %347, !fpmath !19
  %349 = bitcast float* %341 to <8 x float>*
  store <8 x float> %348, <8 x float>* %349, align 4, !tbaa !12, !llvm.access.group !16
  %350 = trunc i64 %mul3.i.i to i32
  %conv2.i.5 = or i32 %350, 5
  %mul.i.5 = mul nsw i32 %conv2.i.5, %4
  %broadcast.splatinsert67 = insertelement <8 x float> undef, float %10, i32 0
  %broadcast.splat68 = shufflevector <8 x float> %broadcast.splatinsert67, <8 x float> undef, <8 x i32> zeroinitializer
  %351 = trunc i64 %mul.i.i to i32
  %352 = shl i64 %7, 37
  %353 = ashr exact i64 %352, 32
  %354 = getelementptr inbounds float, float* %0, i64 %353
  %355 = bitcast float* %354 to <8 x float>*
  %wide.load64 = load <8 x float>, <8 x float>* %355, align 4, !tbaa !12, !llvm.access.group !16
  %356 = add nsw i32 %mul.i.5, %351
  %357 = sext i32 %356 to i64
  %358 = getelementptr inbounds float, float* %2, i64 %357
  %359 = bitcast float* %358 to <8 x float>*
  %wide.load65 = load <8 x float>, <8 x float>* %359, align 4, !tbaa !12, !llvm.access.group !16
  %360 = fsub <8 x float> %wide.load65, %wide.load64
  %361 = bitcast float* %358 to <8 x float>*
  store <8 x float> %360, <8 x float>* %361, align 4, !tbaa !12, !llvm.access.group !16
  %362 = getelementptr inbounds float, float* %1, i64 %353
  %363 = bitcast float* %362 to <8 x float>*
  %wide.load66 = load <8 x float>, <8 x float>* %363, align 4, !tbaa !12, !llvm.access.group !16
  %364 = fmul <8 x float> %broadcast.splat68, %wide.load66
  %365 = fdiv <8 x float> %360, %364, !fpmath !19
  %366 = bitcast float* %358 to <8 x float>*
  store <8 x float> %365, <8 x float>* %366, align 4, !tbaa !12, !llvm.access.group !16
  %367 = or i64 %mul.i.i, 8
  %368 = trunc i64 %367 to i32
  %369 = shl i64 %367, 32
  %370 = ashr exact i64 %369, 32
  %371 = getelementptr inbounds float, float* %0, i64 %370
  %372 = bitcast float* %371 to <8 x float>*
  %wide.load64.1 = load <8 x float>, <8 x float>* %372, align 4, !tbaa !12, !llvm.access.group !16
  %373 = add nsw i32 %mul.i.5, %368
  %374 = sext i32 %373 to i64
  %375 = getelementptr inbounds float, float* %2, i64 %374
  %376 = bitcast float* %375 to <8 x float>*
  %wide.load65.1 = load <8 x float>, <8 x float>* %376, align 4, !tbaa !12, !llvm.access.group !16
  %377 = fsub <8 x float> %wide.load65.1, %wide.load64.1
  %378 = bitcast float* %375 to <8 x float>*
  store <8 x float> %377, <8 x float>* %378, align 4, !tbaa !12, !llvm.access.group !16
  %379 = getelementptr inbounds float, float* %1, i64 %370
  %380 = bitcast float* %379 to <8 x float>*
  %wide.load66.1 = load <8 x float>, <8 x float>* %380, align 4, !tbaa !12, !llvm.access.group !16
  %381 = fmul <8 x float> %broadcast.splat68, %wide.load66.1
  %382 = fdiv <8 x float> %377, %381, !fpmath !19
  %383 = bitcast float* %375 to <8 x float>*
  store <8 x float> %382, <8 x float>* %383, align 4, !tbaa !12, !llvm.access.group !16
  %384 = or i64 %mul.i.i, 16
  %385 = trunc i64 %384 to i32
  %386 = shl i64 %384, 32
  %387 = ashr exact i64 %386, 32
  %388 = getelementptr inbounds float, float* %0, i64 %387
  %389 = bitcast float* %388 to <8 x float>*
  %wide.load64.2 = load <8 x float>, <8 x float>* %389, align 4, !tbaa !12, !llvm.access.group !16
  %390 = add nsw i32 %mul.i.5, %385
  %391 = sext i32 %390 to i64
  %392 = getelementptr inbounds float, float* %2, i64 %391
  %393 = bitcast float* %392 to <8 x float>*
  %wide.load65.2 = load <8 x float>, <8 x float>* %393, align 4, !tbaa !12, !llvm.access.group !16
  %394 = fsub <8 x float> %wide.load65.2, %wide.load64.2
  %395 = bitcast float* %392 to <8 x float>*
  store <8 x float> %394, <8 x float>* %395, align 4, !tbaa !12, !llvm.access.group !16
  %396 = getelementptr inbounds float, float* %1, i64 %387
  %397 = bitcast float* %396 to <8 x float>*
  %wide.load66.2 = load <8 x float>, <8 x float>* %397, align 4, !tbaa !12, !llvm.access.group !16
  %398 = fmul <8 x float> %broadcast.splat68, %wide.load66.2
  %399 = fdiv <8 x float> %394, %398, !fpmath !19
  %400 = bitcast float* %392 to <8 x float>*
  store <8 x float> %399, <8 x float>* %400, align 4, !tbaa !12, !llvm.access.group !16
  %401 = or i64 %mul.i.i, 24
  %402 = trunc i64 %401 to i32
  %403 = shl i64 %401, 32
  %404 = ashr exact i64 %403, 32
  %405 = getelementptr inbounds float, float* %0, i64 %404
  %406 = bitcast float* %405 to <8 x float>*
  %wide.load64.3 = load <8 x float>, <8 x float>* %406, align 4, !tbaa !12, !llvm.access.group !16
  %407 = add nsw i32 %mul.i.5, %402
  %408 = sext i32 %407 to i64
  %409 = getelementptr inbounds float, float* %2, i64 %408
  %410 = bitcast float* %409 to <8 x float>*
  %wide.load65.3 = load <8 x float>, <8 x float>* %410, align 4, !tbaa !12, !llvm.access.group !16
  %411 = fsub <8 x float> %wide.load65.3, %wide.load64.3
  %412 = bitcast float* %409 to <8 x float>*
  store <8 x float> %411, <8 x float>* %412, align 4, !tbaa !12, !llvm.access.group !16
  %413 = getelementptr inbounds float, float* %1, i64 %404
  %414 = bitcast float* %413 to <8 x float>*
  %wide.load66.3 = load <8 x float>, <8 x float>* %414, align 4, !tbaa !12, !llvm.access.group !16
  %415 = fmul <8 x float> %broadcast.splat68, %wide.load66.3
  %416 = fdiv <8 x float> %411, %415, !fpmath !19
  %417 = bitcast float* %409 to <8 x float>*
  store <8 x float> %416, <8 x float>* %417, align 4, !tbaa !12, !llvm.access.group !16
  %418 = trunc i64 %mul3.i.i to i32
  %conv2.i.6 = or i32 %418, 6
  %mul.i.6 = mul nsw i32 %conv2.i.6, %4
  %broadcast.splatinsert80 = insertelement <8 x float> undef, float %10, i32 0
  %broadcast.splat81 = shufflevector <8 x float> %broadcast.splatinsert80, <8 x float> undef, <8 x i32> zeroinitializer
  %419 = trunc i64 %mul.i.i to i32
  %420 = shl i64 %7, 37
  %421 = ashr exact i64 %420, 32
  %422 = getelementptr inbounds float, float* %0, i64 %421
  %423 = bitcast float* %422 to <8 x float>*
  %wide.load77 = load <8 x float>, <8 x float>* %423, align 4, !tbaa !12, !llvm.access.group !16
  %424 = add nsw i32 %mul.i.6, %419
  %425 = sext i32 %424 to i64
  %426 = getelementptr inbounds float, float* %2, i64 %425
  %427 = bitcast float* %426 to <8 x float>*
  %wide.load78 = load <8 x float>, <8 x float>* %427, align 4, !tbaa !12, !llvm.access.group !16
  %428 = fsub <8 x float> %wide.load78, %wide.load77
  %429 = bitcast float* %426 to <8 x float>*
  store <8 x float> %428, <8 x float>* %429, align 4, !tbaa !12, !llvm.access.group !16
  %430 = getelementptr inbounds float, float* %1, i64 %421
  %431 = bitcast float* %430 to <8 x float>*
  %wide.load79 = load <8 x float>, <8 x float>* %431, align 4, !tbaa !12, !llvm.access.group !16
  %432 = fmul <8 x float> %broadcast.splat81, %wide.load79
  %433 = fdiv <8 x float> %428, %432, !fpmath !19
  %434 = bitcast float* %426 to <8 x float>*
  store <8 x float> %433, <8 x float>* %434, align 4, !tbaa !12, !llvm.access.group !16
  %435 = or i64 %mul.i.i, 8
  %436 = trunc i64 %435 to i32
  %437 = shl i64 %435, 32
  %438 = ashr exact i64 %437, 32
  %439 = getelementptr inbounds float, float* %0, i64 %438
  %440 = bitcast float* %439 to <8 x float>*
  %wide.load77.1 = load <8 x float>, <8 x float>* %440, align 4, !tbaa !12, !llvm.access.group !16
  %441 = add nsw i32 %mul.i.6, %436
  %442 = sext i32 %441 to i64
  %443 = getelementptr inbounds float, float* %2, i64 %442
  %444 = bitcast float* %443 to <8 x float>*
  %wide.load78.1 = load <8 x float>, <8 x float>* %444, align 4, !tbaa !12, !llvm.access.group !16
  %445 = fsub <8 x float> %wide.load78.1, %wide.load77.1
  %446 = bitcast float* %443 to <8 x float>*
  store <8 x float> %445, <8 x float>* %446, align 4, !tbaa !12, !llvm.access.group !16
  %447 = getelementptr inbounds float, float* %1, i64 %438
  %448 = bitcast float* %447 to <8 x float>*
  %wide.load79.1 = load <8 x float>, <8 x float>* %448, align 4, !tbaa !12, !llvm.access.group !16
  %449 = fmul <8 x float> %broadcast.splat81, %wide.load79.1
  %450 = fdiv <8 x float> %445, %449, !fpmath !19
  %451 = bitcast float* %443 to <8 x float>*
  store <8 x float> %450, <8 x float>* %451, align 4, !tbaa !12, !llvm.access.group !16
  %452 = or i64 %mul.i.i, 16
  %453 = trunc i64 %452 to i32
  %454 = shl i64 %452, 32
  %455 = ashr exact i64 %454, 32
  %456 = getelementptr inbounds float, float* %0, i64 %455
  %457 = bitcast float* %456 to <8 x float>*
  %wide.load77.2 = load <8 x float>, <8 x float>* %457, align 4, !tbaa !12, !llvm.access.group !16
  %458 = add nsw i32 %mul.i.6, %453
  %459 = sext i32 %458 to i64
  %460 = getelementptr inbounds float, float* %2, i64 %459
  %461 = bitcast float* %460 to <8 x float>*
  %wide.load78.2 = load <8 x float>, <8 x float>* %461, align 4, !tbaa !12, !llvm.access.group !16
  %462 = fsub <8 x float> %wide.load78.2, %wide.load77.2
  %463 = bitcast float* %460 to <8 x float>*
  store <8 x float> %462, <8 x float>* %463, align 4, !tbaa !12, !llvm.access.group !16
  %464 = getelementptr inbounds float, float* %1, i64 %455
  %465 = bitcast float* %464 to <8 x float>*
  %wide.load79.2 = load <8 x float>, <8 x float>* %465, align 4, !tbaa !12, !llvm.access.group !16
  %466 = fmul <8 x float> %broadcast.splat81, %wide.load79.2
  %467 = fdiv <8 x float> %462, %466, !fpmath !19
  %468 = bitcast float* %460 to <8 x float>*
  store <8 x float> %467, <8 x float>* %468, align 4, !tbaa !12, !llvm.access.group !16
  %469 = or i64 %mul.i.i, 24
  %470 = trunc i64 %469 to i32
  %471 = shl i64 %469, 32
  %472 = ashr exact i64 %471, 32
  %473 = getelementptr inbounds float, float* %0, i64 %472
  %474 = bitcast float* %473 to <8 x float>*
  %wide.load77.3 = load <8 x float>, <8 x float>* %474, align 4, !tbaa !12, !llvm.access.group !16
  %475 = add nsw i32 %mul.i.6, %470
  %476 = sext i32 %475 to i64
  %477 = getelementptr inbounds float, float* %2, i64 %476
  %478 = bitcast float* %477 to <8 x float>*
  %wide.load78.3 = load <8 x float>, <8 x float>* %478, align 4, !tbaa !12, !llvm.access.group !16
  %479 = fsub <8 x float> %wide.load78.3, %wide.load77.3
  %480 = bitcast float* %477 to <8 x float>*
  store <8 x float> %479, <8 x float>* %480, align 4, !tbaa !12, !llvm.access.group !16
  %481 = getelementptr inbounds float, float* %1, i64 %472
  %482 = bitcast float* %481 to <8 x float>*
  %wide.load79.3 = load <8 x float>, <8 x float>* %482, align 4, !tbaa !12, !llvm.access.group !16
  %483 = fmul <8 x float> %broadcast.splat81, %wide.load79.3
  %484 = fdiv <8 x float> %479, %483, !fpmath !19
  %485 = bitcast float* %477 to <8 x float>*
  store <8 x float> %484, <8 x float>* %485, align 4, !tbaa !12, !llvm.access.group !16
  %486 = trunc i64 %mul3.i.i to i32
  %conv2.i.7 = or i32 %486, 7
  %mul.i.7 = mul nsw i32 %conv2.i.7, %4
  %broadcast.splatinsert93 = insertelement <8 x float> undef, float %10, i32 0
  %broadcast.splat94 = shufflevector <8 x float> %broadcast.splatinsert93, <8 x float> undef, <8 x i32> zeroinitializer
  %487 = trunc i64 %mul.i.i to i32
  %488 = shl i64 %7, 37
  %489 = ashr exact i64 %488, 32
  %490 = getelementptr inbounds float, float* %0, i64 %489
  %491 = bitcast float* %490 to <8 x float>*
  %wide.load90 = load <8 x float>, <8 x float>* %491, align 4, !tbaa !12, !llvm.access.group !16
  %492 = add nsw i32 %mul.i.7, %487
  %493 = sext i32 %492 to i64
  %494 = getelementptr inbounds float, float* %2, i64 %493
  %495 = bitcast float* %494 to <8 x float>*
  %wide.load91 = load <8 x float>, <8 x float>* %495, align 4, !tbaa !12, !llvm.access.group !16
  %496 = fsub <8 x float> %wide.load91, %wide.load90
  %497 = bitcast float* %494 to <8 x float>*
  store <8 x float> %496, <8 x float>* %497, align 4, !tbaa !12, !llvm.access.group !16
  %498 = getelementptr inbounds float, float* %1, i64 %489
  %499 = bitcast float* %498 to <8 x float>*
  %wide.load92 = load <8 x float>, <8 x float>* %499, align 4, !tbaa !12, !llvm.access.group !16
  %500 = fmul <8 x float> %broadcast.splat94, %wide.load92
  %501 = fdiv <8 x float> %496, %500, !fpmath !19
  %502 = bitcast float* %494 to <8 x float>*
  store <8 x float> %501, <8 x float>* %502, align 4, !tbaa !12, !llvm.access.group !16
  %503 = or i64 %mul.i.i, 8
  %504 = trunc i64 %503 to i32
  %505 = shl i64 %503, 32
  %506 = ashr exact i64 %505, 32
  %507 = getelementptr inbounds float, float* %0, i64 %506
  %508 = bitcast float* %507 to <8 x float>*
  %wide.load90.1 = load <8 x float>, <8 x float>* %508, align 4, !tbaa !12, !llvm.access.group !16
  %509 = add nsw i32 %mul.i.7, %504
  %510 = sext i32 %509 to i64
  %511 = getelementptr inbounds float, float* %2, i64 %510
  %512 = bitcast float* %511 to <8 x float>*
  %wide.load91.1 = load <8 x float>, <8 x float>* %512, align 4, !tbaa !12, !llvm.access.group !16
  %513 = fsub <8 x float> %wide.load91.1, %wide.load90.1
  %514 = bitcast float* %511 to <8 x float>*
  store <8 x float> %513, <8 x float>* %514, align 4, !tbaa !12, !llvm.access.group !16
  %515 = getelementptr inbounds float, float* %1, i64 %506
  %516 = bitcast float* %515 to <8 x float>*
  %wide.load92.1 = load <8 x float>, <8 x float>* %516, align 4, !tbaa !12, !llvm.access.group !16
  %517 = fmul <8 x float> %broadcast.splat94, %wide.load92.1
  %518 = fdiv <8 x float> %513, %517, !fpmath !19
  %519 = bitcast float* %511 to <8 x float>*
  store <8 x float> %518, <8 x float>* %519, align 4, !tbaa !12, !llvm.access.group !16
  %520 = or i64 %mul.i.i, 16
  %521 = trunc i64 %520 to i32
  %522 = shl i64 %520, 32
  %523 = ashr exact i64 %522, 32
  %524 = getelementptr inbounds float, float* %0, i64 %523
  %525 = bitcast float* %524 to <8 x float>*
  %wide.load90.2 = load <8 x float>, <8 x float>* %525, align 4, !tbaa !12, !llvm.access.group !16
  %526 = add nsw i32 %mul.i.7, %521
  %527 = sext i32 %526 to i64
  %528 = getelementptr inbounds float, float* %2, i64 %527
  %529 = bitcast float* %528 to <8 x float>*
  %wide.load91.2 = load <8 x float>, <8 x float>* %529, align 4, !tbaa !12, !llvm.access.group !16
  %530 = fsub <8 x float> %wide.load91.2, %wide.load90.2
  %531 = bitcast float* %528 to <8 x float>*
  store <8 x float> %530, <8 x float>* %531, align 4, !tbaa !12, !llvm.access.group !16
  %532 = getelementptr inbounds float, float* %1, i64 %523
  %533 = bitcast float* %532 to <8 x float>*
  %wide.load92.2 = load <8 x float>, <8 x float>* %533, align 4, !tbaa !12, !llvm.access.group !16
  %534 = fmul <8 x float> %broadcast.splat94, %wide.load92.2
  %535 = fdiv <8 x float> %530, %534, !fpmath !19
  %536 = bitcast float* %528 to <8 x float>*
  store <8 x float> %535, <8 x float>* %536, align 4, !tbaa !12, !llvm.access.group !16
  %537 = or i64 %mul.i.i, 24
  %538 = trunc i64 %537 to i32
  %539 = shl i64 %537, 32
  %540 = ashr exact i64 %539, 32
  %541 = getelementptr inbounds float, float* %0, i64 %540
  %542 = bitcast float* %541 to <8 x float>*
  %wide.load90.3 = load <8 x float>, <8 x float>* %542, align 4, !tbaa !12, !llvm.access.group !16
  %543 = add nsw i32 %mul.i.7, %538
  %544 = sext i32 %543 to i64
  %545 = getelementptr inbounds float, float* %2, i64 %544
  %546 = bitcast float* %545 to <8 x float>*
  %wide.load91.3 = load <8 x float>, <8 x float>* %546, align 4, !tbaa !12, !llvm.access.group !16
  %547 = fsub <8 x float> %wide.load91.3, %wide.load90.3
  %548 = bitcast float* %545 to <8 x float>*
  store <8 x float> %547, <8 x float>* %548, align 4, !tbaa !12, !llvm.access.group !16
  %549 = getelementptr inbounds float, float* %1, i64 %540
  %550 = bitcast float* %549 to <8 x float>*
  %wide.load92.3 = load <8 x float>, <8 x float>* %550, align 4, !tbaa !12, !llvm.access.group !16
  %551 = fmul <8 x float> %broadcast.splat94, %wide.load92.3
  %552 = fdiv <8 x float> %547, %551, !fpmath !19
  %553 = bitcast float* %545 to <8 x float>*
  store <8 x float> %552, <8 x float>* %553, align 4, !tbaa !12, !llvm.access.group !16
  ret void
}

; Function Attrs: nofree nounwind
define void @_pocl_kernel_reduce_kernel_workgroup(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #2 {
pregion_for_entry.pregion_for_init.i.i:
  %5 = bitcast i8** %0 to float***
  %6 = load float**, float*** %5, align 8
  %7 = load float*, float** %6, align 8
  %8 = getelementptr i8*, i8** %0, i64 1
  %9 = bitcast i8** %8 to float***
  %10 = load float**, float*** %9, align 8
  %11 = load float*, float** %10, align 8
  %12 = getelementptr i8*, i8** %0, i64 2
  %13 = bitcast i8** %12 to float***
  %14 = load float**, float*** %13, align 8
  %15 = load float*, float** %14, align 8
  %16 = getelementptr i8*, i8** %0, i64 3
  %17 = bitcast i8** %16 to float**
  %18 = load float*, float** %17, align 8
  %19 = load float, float* %18, align 4
  %20 = getelementptr i8*, i8** %0, i64 4
  %21 = bitcast i8** %20 to i32**
  %22 = load i32*, i32** %21, align 8
  %23 = load i32, i32* %22, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %24 = tail call float @llvm.sqrt.f32(float %19) #3
  %conv2.i.i = trunc i64 %mul3.i.i.i to i32
  %mul.i.i = mul nsw i32 %23, %conv2.i.i
  %broadcast.splatinsert = insertelement <8 x float> undef, float %24, i32 0
  %broadcast.splat = shufflevector <8 x float> %broadcast.splatinsert, <8 x float> undef, <8 x i32> zeroinitializer
  %25 = trunc i64 %mul.i.i.i to i32
  %26 = shl i64 %2, 37
  %27 = ashr exact i64 %26, 32
  %28 = getelementptr inbounds float, float* %7, i64 %27
  %29 = bitcast float* %28 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %29, align 4, !tbaa !12, !llvm.access.group !16
  %30 = add nsw i32 %mul.i.i, %25
  %31 = sext i32 %30 to i64
  %32 = getelementptr inbounds float, float* %15, i64 %31
  %33 = bitcast float* %32 to <8 x float>*
  %wide.load2 = load <8 x float>, <8 x float>* %33, align 4, !tbaa !12, !llvm.access.group !16
  %34 = fsub <8 x float> %wide.load2, %wide.load
  %35 = bitcast float* %32 to <8 x float>*
  store <8 x float> %34, <8 x float>* %35, align 4, !tbaa !12, !llvm.access.group !16
  %36 = getelementptr inbounds float, float* %11, i64 %27
  %37 = bitcast float* %36 to <8 x float>*
  %wide.load3 = load <8 x float>, <8 x float>* %37, align 4, !tbaa !12, !llvm.access.group !16
  %38 = fmul <8 x float> %broadcast.splat, %wide.load3
  %39 = fdiv <8 x float> %34, %38, !fpmath !19
  %40 = bitcast float* %32 to <8 x float>*
  store <8 x float> %39, <8 x float>* %40, align 4, !tbaa !12, !llvm.access.group !16
  %41 = or i64 %mul.i.i.i, 8
  %42 = trunc i64 %41 to i32
  %43 = shl i64 %41, 32
  %44 = ashr exact i64 %43, 32
  %45 = getelementptr inbounds float, float* %7, i64 %44
  %46 = bitcast float* %45 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %46, align 4, !tbaa !12, !llvm.access.group !16
  %47 = add nsw i32 %mul.i.i, %42
  %48 = sext i32 %47 to i64
  %49 = getelementptr inbounds float, float* %15, i64 %48
  %50 = bitcast float* %49 to <8 x float>*
  %wide.load2.1 = load <8 x float>, <8 x float>* %50, align 4, !tbaa !12, !llvm.access.group !16
  %51 = fsub <8 x float> %wide.load2.1, %wide.load.1
  %52 = bitcast float* %49 to <8 x float>*
  store <8 x float> %51, <8 x float>* %52, align 4, !tbaa !12, !llvm.access.group !16
  %53 = getelementptr inbounds float, float* %11, i64 %44
  %54 = bitcast float* %53 to <8 x float>*
  %wide.load3.1 = load <8 x float>, <8 x float>* %54, align 4, !tbaa !12, !llvm.access.group !16
  %55 = fmul <8 x float> %broadcast.splat, %wide.load3.1
  %56 = fdiv <8 x float> %51, %55, !fpmath !19
  %57 = bitcast float* %49 to <8 x float>*
  store <8 x float> %56, <8 x float>* %57, align 4, !tbaa !12, !llvm.access.group !16
  %58 = or i64 %mul.i.i.i, 16
  %59 = trunc i64 %58 to i32
  %60 = shl i64 %58, 32
  %61 = ashr exact i64 %60, 32
  %62 = getelementptr inbounds float, float* %7, i64 %61
  %63 = bitcast float* %62 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %63, align 4, !tbaa !12, !llvm.access.group !16
  %64 = add nsw i32 %mul.i.i, %59
  %65 = sext i32 %64 to i64
  %66 = getelementptr inbounds float, float* %15, i64 %65
  %67 = bitcast float* %66 to <8 x float>*
  %wide.load2.2 = load <8 x float>, <8 x float>* %67, align 4, !tbaa !12, !llvm.access.group !16
  %68 = fsub <8 x float> %wide.load2.2, %wide.load.2
  %69 = bitcast float* %66 to <8 x float>*
  store <8 x float> %68, <8 x float>* %69, align 4, !tbaa !12, !llvm.access.group !16
  %70 = getelementptr inbounds float, float* %11, i64 %61
  %71 = bitcast float* %70 to <8 x float>*
  %wide.load3.2 = load <8 x float>, <8 x float>* %71, align 4, !tbaa !12, !llvm.access.group !16
  %72 = fmul <8 x float> %broadcast.splat, %wide.load3.2
  %73 = fdiv <8 x float> %68, %72, !fpmath !19
  %74 = bitcast float* %66 to <8 x float>*
  store <8 x float> %73, <8 x float>* %74, align 4, !tbaa !12, !llvm.access.group !16
  %75 = or i64 %mul.i.i.i, 24
  %76 = trunc i64 %75 to i32
  %77 = shl i64 %75, 32
  %78 = ashr exact i64 %77, 32
  %79 = getelementptr inbounds float, float* %7, i64 %78
  %80 = bitcast float* %79 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %80, align 4, !tbaa !12, !llvm.access.group !16
  %81 = add nsw i32 %mul.i.i, %76
  %82 = sext i32 %81 to i64
  %83 = getelementptr inbounds float, float* %15, i64 %82
  %84 = bitcast float* %83 to <8 x float>*
  %wide.load2.3 = load <8 x float>, <8 x float>* %84, align 4, !tbaa !12, !llvm.access.group !16
  %85 = fsub <8 x float> %wide.load2.3, %wide.load.3
  %86 = bitcast float* %83 to <8 x float>*
  store <8 x float> %85, <8 x float>* %86, align 4, !tbaa !12, !llvm.access.group !16
  %87 = getelementptr inbounds float, float* %11, i64 %78
  %88 = bitcast float* %87 to <8 x float>*
  %wide.load3.3 = load <8 x float>, <8 x float>* %88, align 4, !tbaa !12, !llvm.access.group !16
  %89 = fmul <8 x float> %broadcast.splat, %wide.load3.3
  %90 = fdiv <8 x float> %85, %89, !fpmath !19
  %91 = bitcast float* %83 to <8 x float>*
  store <8 x float> %90, <8 x float>* %91, align 4, !tbaa !12, !llvm.access.group !16
  %92 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.1 = or i32 %92, 1
  %mul.i.i.1 = mul nsw i32 %23, %conv2.i.i.1
  %broadcast.splatinsert15 = insertelement <8 x float> undef, float %24, i32 0
  %broadcast.splat16 = shufflevector <8 x float> %broadcast.splatinsert15, <8 x float> undef, <8 x i32> zeroinitializer
  %93 = trunc i64 %mul.i.i.i to i32
  %94 = shl i64 %2, 37
  %95 = ashr exact i64 %94, 32
  %96 = getelementptr inbounds float, float* %7, i64 %95
  %97 = bitcast float* %96 to <8 x float>*
  %wide.load12 = load <8 x float>, <8 x float>* %97, align 4, !tbaa !12, !llvm.access.group !16
  %98 = add nsw i32 %mul.i.i.1, %93
  %99 = sext i32 %98 to i64
  %100 = getelementptr inbounds float, float* %15, i64 %99
  %101 = bitcast float* %100 to <8 x float>*
  %wide.load13 = load <8 x float>, <8 x float>* %101, align 4, !tbaa !12, !llvm.access.group !16
  %102 = fsub <8 x float> %wide.load13, %wide.load12
  %103 = bitcast float* %100 to <8 x float>*
  store <8 x float> %102, <8 x float>* %103, align 4, !tbaa !12, !llvm.access.group !16
  %104 = getelementptr inbounds float, float* %11, i64 %95
  %105 = bitcast float* %104 to <8 x float>*
  %wide.load14 = load <8 x float>, <8 x float>* %105, align 4, !tbaa !12, !llvm.access.group !16
  %106 = fmul <8 x float> %broadcast.splat16, %wide.load14
  %107 = fdiv <8 x float> %102, %106, !fpmath !19
  %108 = bitcast float* %100 to <8 x float>*
  store <8 x float> %107, <8 x float>* %108, align 4, !tbaa !12, !llvm.access.group !16
  %109 = or i64 %mul.i.i.i, 8
  %110 = trunc i64 %109 to i32
  %111 = shl i64 %109, 32
  %112 = ashr exact i64 %111, 32
  %113 = getelementptr inbounds float, float* %7, i64 %112
  %114 = bitcast float* %113 to <8 x float>*
  %wide.load12.1 = load <8 x float>, <8 x float>* %114, align 4, !tbaa !12, !llvm.access.group !16
  %115 = add nsw i32 %mul.i.i.1, %110
  %116 = sext i32 %115 to i64
  %117 = getelementptr inbounds float, float* %15, i64 %116
  %118 = bitcast float* %117 to <8 x float>*
  %wide.load13.1 = load <8 x float>, <8 x float>* %118, align 4, !tbaa !12, !llvm.access.group !16
  %119 = fsub <8 x float> %wide.load13.1, %wide.load12.1
  %120 = bitcast float* %117 to <8 x float>*
  store <8 x float> %119, <8 x float>* %120, align 4, !tbaa !12, !llvm.access.group !16
  %121 = getelementptr inbounds float, float* %11, i64 %112
  %122 = bitcast float* %121 to <8 x float>*
  %wide.load14.1 = load <8 x float>, <8 x float>* %122, align 4, !tbaa !12, !llvm.access.group !16
  %123 = fmul <8 x float> %broadcast.splat16, %wide.load14.1
  %124 = fdiv <8 x float> %119, %123, !fpmath !19
  %125 = bitcast float* %117 to <8 x float>*
  store <8 x float> %124, <8 x float>* %125, align 4, !tbaa !12, !llvm.access.group !16
  %126 = or i64 %mul.i.i.i, 16
  %127 = trunc i64 %126 to i32
  %128 = shl i64 %126, 32
  %129 = ashr exact i64 %128, 32
  %130 = getelementptr inbounds float, float* %7, i64 %129
  %131 = bitcast float* %130 to <8 x float>*
  %wide.load12.2 = load <8 x float>, <8 x float>* %131, align 4, !tbaa !12, !llvm.access.group !16
  %132 = add nsw i32 %mul.i.i.1, %127
  %133 = sext i32 %132 to i64
  %134 = getelementptr inbounds float, float* %15, i64 %133
  %135 = bitcast float* %134 to <8 x float>*
  %wide.load13.2 = load <8 x float>, <8 x float>* %135, align 4, !tbaa !12, !llvm.access.group !16
  %136 = fsub <8 x float> %wide.load13.2, %wide.load12.2
  %137 = bitcast float* %134 to <8 x float>*
  store <8 x float> %136, <8 x float>* %137, align 4, !tbaa !12, !llvm.access.group !16
  %138 = getelementptr inbounds float, float* %11, i64 %129
  %139 = bitcast float* %138 to <8 x float>*
  %wide.load14.2 = load <8 x float>, <8 x float>* %139, align 4, !tbaa !12, !llvm.access.group !16
  %140 = fmul <8 x float> %broadcast.splat16, %wide.load14.2
  %141 = fdiv <8 x float> %136, %140, !fpmath !19
  %142 = bitcast float* %134 to <8 x float>*
  store <8 x float> %141, <8 x float>* %142, align 4, !tbaa !12, !llvm.access.group !16
  %143 = or i64 %mul.i.i.i, 24
  %144 = trunc i64 %143 to i32
  %145 = shl i64 %143, 32
  %146 = ashr exact i64 %145, 32
  %147 = getelementptr inbounds float, float* %7, i64 %146
  %148 = bitcast float* %147 to <8 x float>*
  %wide.load12.3 = load <8 x float>, <8 x float>* %148, align 4, !tbaa !12, !llvm.access.group !16
  %149 = add nsw i32 %mul.i.i.1, %144
  %150 = sext i32 %149 to i64
  %151 = getelementptr inbounds float, float* %15, i64 %150
  %152 = bitcast float* %151 to <8 x float>*
  %wide.load13.3 = load <8 x float>, <8 x float>* %152, align 4, !tbaa !12, !llvm.access.group !16
  %153 = fsub <8 x float> %wide.load13.3, %wide.load12.3
  %154 = bitcast float* %151 to <8 x float>*
  store <8 x float> %153, <8 x float>* %154, align 4, !tbaa !12, !llvm.access.group !16
  %155 = getelementptr inbounds float, float* %11, i64 %146
  %156 = bitcast float* %155 to <8 x float>*
  %wide.load14.3 = load <8 x float>, <8 x float>* %156, align 4, !tbaa !12, !llvm.access.group !16
  %157 = fmul <8 x float> %broadcast.splat16, %wide.load14.3
  %158 = fdiv <8 x float> %153, %157, !fpmath !19
  %159 = bitcast float* %151 to <8 x float>*
  store <8 x float> %158, <8 x float>* %159, align 4, !tbaa !12, !llvm.access.group !16
  %160 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.2 = or i32 %160, 2
  %mul.i.i.2 = mul nsw i32 %23, %conv2.i.i.2
  %broadcast.splatinsert28 = insertelement <8 x float> undef, float %24, i32 0
  %broadcast.splat29 = shufflevector <8 x float> %broadcast.splatinsert28, <8 x float> undef, <8 x i32> zeroinitializer
  %161 = trunc i64 %mul.i.i.i to i32
  %162 = shl i64 %2, 37
  %163 = ashr exact i64 %162, 32
  %164 = getelementptr inbounds float, float* %7, i64 %163
  %165 = bitcast float* %164 to <8 x float>*
  %wide.load25 = load <8 x float>, <8 x float>* %165, align 4, !tbaa !12, !llvm.access.group !16
  %166 = add nsw i32 %mul.i.i.2, %161
  %167 = sext i32 %166 to i64
  %168 = getelementptr inbounds float, float* %15, i64 %167
  %169 = bitcast float* %168 to <8 x float>*
  %wide.load26 = load <8 x float>, <8 x float>* %169, align 4, !tbaa !12, !llvm.access.group !16
  %170 = fsub <8 x float> %wide.load26, %wide.load25
  %171 = bitcast float* %168 to <8 x float>*
  store <8 x float> %170, <8 x float>* %171, align 4, !tbaa !12, !llvm.access.group !16
  %172 = getelementptr inbounds float, float* %11, i64 %163
  %173 = bitcast float* %172 to <8 x float>*
  %wide.load27 = load <8 x float>, <8 x float>* %173, align 4, !tbaa !12, !llvm.access.group !16
  %174 = fmul <8 x float> %broadcast.splat29, %wide.load27
  %175 = fdiv <8 x float> %170, %174, !fpmath !19
  %176 = bitcast float* %168 to <8 x float>*
  store <8 x float> %175, <8 x float>* %176, align 4, !tbaa !12, !llvm.access.group !16
  %177 = or i64 %mul.i.i.i, 8
  %178 = trunc i64 %177 to i32
  %179 = shl i64 %177, 32
  %180 = ashr exact i64 %179, 32
  %181 = getelementptr inbounds float, float* %7, i64 %180
  %182 = bitcast float* %181 to <8 x float>*
  %wide.load25.1 = load <8 x float>, <8 x float>* %182, align 4, !tbaa !12, !llvm.access.group !16
  %183 = add nsw i32 %mul.i.i.2, %178
  %184 = sext i32 %183 to i64
  %185 = getelementptr inbounds float, float* %15, i64 %184
  %186 = bitcast float* %185 to <8 x float>*
  %wide.load26.1 = load <8 x float>, <8 x float>* %186, align 4, !tbaa !12, !llvm.access.group !16
  %187 = fsub <8 x float> %wide.load26.1, %wide.load25.1
  %188 = bitcast float* %185 to <8 x float>*
  store <8 x float> %187, <8 x float>* %188, align 4, !tbaa !12, !llvm.access.group !16
  %189 = getelementptr inbounds float, float* %11, i64 %180
  %190 = bitcast float* %189 to <8 x float>*
  %wide.load27.1 = load <8 x float>, <8 x float>* %190, align 4, !tbaa !12, !llvm.access.group !16
  %191 = fmul <8 x float> %broadcast.splat29, %wide.load27.1
  %192 = fdiv <8 x float> %187, %191, !fpmath !19
  %193 = bitcast float* %185 to <8 x float>*
  store <8 x float> %192, <8 x float>* %193, align 4, !tbaa !12, !llvm.access.group !16
  %194 = or i64 %mul.i.i.i, 16
  %195 = trunc i64 %194 to i32
  %196 = shl i64 %194, 32
  %197 = ashr exact i64 %196, 32
  %198 = getelementptr inbounds float, float* %7, i64 %197
  %199 = bitcast float* %198 to <8 x float>*
  %wide.load25.2 = load <8 x float>, <8 x float>* %199, align 4, !tbaa !12, !llvm.access.group !16
  %200 = add nsw i32 %mul.i.i.2, %195
  %201 = sext i32 %200 to i64
  %202 = getelementptr inbounds float, float* %15, i64 %201
  %203 = bitcast float* %202 to <8 x float>*
  %wide.load26.2 = load <8 x float>, <8 x float>* %203, align 4, !tbaa !12, !llvm.access.group !16
  %204 = fsub <8 x float> %wide.load26.2, %wide.load25.2
  %205 = bitcast float* %202 to <8 x float>*
  store <8 x float> %204, <8 x float>* %205, align 4, !tbaa !12, !llvm.access.group !16
  %206 = getelementptr inbounds float, float* %11, i64 %197
  %207 = bitcast float* %206 to <8 x float>*
  %wide.load27.2 = load <8 x float>, <8 x float>* %207, align 4, !tbaa !12, !llvm.access.group !16
  %208 = fmul <8 x float> %broadcast.splat29, %wide.load27.2
  %209 = fdiv <8 x float> %204, %208, !fpmath !19
  %210 = bitcast float* %202 to <8 x float>*
  store <8 x float> %209, <8 x float>* %210, align 4, !tbaa !12, !llvm.access.group !16
  %211 = or i64 %mul.i.i.i, 24
  %212 = trunc i64 %211 to i32
  %213 = shl i64 %211, 32
  %214 = ashr exact i64 %213, 32
  %215 = getelementptr inbounds float, float* %7, i64 %214
  %216 = bitcast float* %215 to <8 x float>*
  %wide.load25.3 = load <8 x float>, <8 x float>* %216, align 4, !tbaa !12, !llvm.access.group !16
  %217 = add nsw i32 %mul.i.i.2, %212
  %218 = sext i32 %217 to i64
  %219 = getelementptr inbounds float, float* %15, i64 %218
  %220 = bitcast float* %219 to <8 x float>*
  %wide.load26.3 = load <8 x float>, <8 x float>* %220, align 4, !tbaa !12, !llvm.access.group !16
  %221 = fsub <8 x float> %wide.load26.3, %wide.load25.3
  %222 = bitcast float* %219 to <8 x float>*
  store <8 x float> %221, <8 x float>* %222, align 4, !tbaa !12, !llvm.access.group !16
  %223 = getelementptr inbounds float, float* %11, i64 %214
  %224 = bitcast float* %223 to <8 x float>*
  %wide.load27.3 = load <8 x float>, <8 x float>* %224, align 4, !tbaa !12, !llvm.access.group !16
  %225 = fmul <8 x float> %broadcast.splat29, %wide.load27.3
  %226 = fdiv <8 x float> %221, %225, !fpmath !19
  %227 = bitcast float* %219 to <8 x float>*
  store <8 x float> %226, <8 x float>* %227, align 4, !tbaa !12, !llvm.access.group !16
  %228 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.3 = or i32 %228, 3
  %mul.i.i.3 = mul nsw i32 %23, %conv2.i.i.3
  %broadcast.splatinsert41 = insertelement <8 x float> undef, float %24, i32 0
  %broadcast.splat42 = shufflevector <8 x float> %broadcast.splatinsert41, <8 x float> undef, <8 x i32> zeroinitializer
  %229 = trunc i64 %mul.i.i.i to i32
  %230 = shl i64 %2, 37
  %231 = ashr exact i64 %230, 32
  %232 = getelementptr inbounds float, float* %7, i64 %231
  %233 = bitcast float* %232 to <8 x float>*
  %wide.load38 = load <8 x float>, <8 x float>* %233, align 4, !tbaa !12, !llvm.access.group !16
  %234 = add nsw i32 %mul.i.i.3, %229
  %235 = sext i32 %234 to i64
  %236 = getelementptr inbounds float, float* %15, i64 %235
  %237 = bitcast float* %236 to <8 x float>*
  %wide.load39 = load <8 x float>, <8 x float>* %237, align 4, !tbaa !12, !llvm.access.group !16
  %238 = fsub <8 x float> %wide.load39, %wide.load38
  %239 = bitcast float* %236 to <8 x float>*
  store <8 x float> %238, <8 x float>* %239, align 4, !tbaa !12, !llvm.access.group !16
  %240 = getelementptr inbounds float, float* %11, i64 %231
  %241 = bitcast float* %240 to <8 x float>*
  %wide.load40 = load <8 x float>, <8 x float>* %241, align 4, !tbaa !12, !llvm.access.group !16
  %242 = fmul <8 x float> %broadcast.splat42, %wide.load40
  %243 = fdiv <8 x float> %238, %242, !fpmath !19
  %244 = bitcast float* %236 to <8 x float>*
  store <8 x float> %243, <8 x float>* %244, align 4, !tbaa !12, !llvm.access.group !16
  %245 = or i64 %mul.i.i.i, 8
  %246 = trunc i64 %245 to i32
  %247 = shl i64 %245, 32
  %248 = ashr exact i64 %247, 32
  %249 = getelementptr inbounds float, float* %7, i64 %248
  %250 = bitcast float* %249 to <8 x float>*
  %wide.load38.1 = load <8 x float>, <8 x float>* %250, align 4, !tbaa !12, !llvm.access.group !16
  %251 = add nsw i32 %mul.i.i.3, %246
  %252 = sext i32 %251 to i64
  %253 = getelementptr inbounds float, float* %15, i64 %252
  %254 = bitcast float* %253 to <8 x float>*
  %wide.load39.1 = load <8 x float>, <8 x float>* %254, align 4, !tbaa !12, !llvm.access.group !16
  %255 = fsub <8 x float> %wide.load39.1, %wide.load38.1
  %256 = bitcast float* %253 to <8 x float>*
  store <8 x float> %255, <8 x float>* %256, align 4, !tbaa !12, !llvm.access.group !16
  %257 = getelementptr inbounds float, float* %11, i64 %248
  %258 = bitcast float* %257 to <8 x float>*
  %wide.load40.1 = load <8 x float>, <8 x float>* %258, align 4, !tbaa !12, !llvm.access.group !16
  %259 = fmul <8 x float> %broadcast.splat42, %wide.load40.1
  %260 = fdiv <8 x float> %255, %259, !fpmath !19
  %261 = bitcast float* %253 to <8 x float>*
  store <8 x float> %260, <8 x float>* %261, align 4, !tbaa !12, !llvm.access.group !16
  %262 = or i64 %mul.i.i.i, 16
  %263 = trunc i64 %262 to i32
  %264 = shl i64 %262, 32
  %265 = ashr exact i64 %264, 32
  %266 = getelementptr inbounds float, float* %7, i64 %265
  %267 = bitcast float* %266 to <8 x float>*
  %wide.load38.2 = load <8 x float>, <8 x float>* %267, align 4, !tbaa !12, !llvm.access.group !16
  %268 = add nsw i32 %mul.i.i.3, %263
  %269 = sext i32 %268 to i64
  %270 = getelementptr inbounds float, float* %15, i64 %269
  %271 = bitcast float* %270 to <8 x float>*
  %wide.load39.2 = load <8 x float>, <8 x float>* %271, align 4, !tbaa !12, !llvm.access.group !16
  %272 = fsub <8 x float> %wide.load39.2, %wide.load38.2
  %273 = bitcast float* %270 to <8 x float>*
  store <8 x float> %272, <8 x float>* %273, align 4, !tbaa !12, !llvm.access.group !16
  %274 = getelementptr inbounds float, float* %11, i64 %265
  %275 = bitcast float* %274 to <8 x float>*
  %wide.load40.2 = load <8 x float>, <8 x float>* %275, align 4, !tbaa !12, !llvm.access.group !16
  %276 = fmul <8 x float> %broadcast.splat42, %wide.load40.2
  %277 = fdiv <8 x float> %272, %276, !fpmath !19
  %278 = bitcast float* %270 to <8 x float>*
  store <8 x float> %277, <8 x float>* %278, align 4, !tbaa !12, !llvm.access.group !16
  %279 = or i64 %mul.i.i.i, 24
  %280 = trunc i64 %279 to i32
  %281 = shl i64 %279, 32
  %282 = ashr exact i64 %281, 32
  %283 = getelementptr inbounds float, float* %7, i64 %282
  %284 = bitcast float* %283 to <8 x float>*
  %wide.load38.3 = load <8 x float>, <8 x float>* %284, align 4, !tbaa !12, !llvm.access.group !16
  %285 = add nsw i32 %mul.i.i.3, %280
  %286 = sext i32 %285 to i64
  %287 = getelementptr inbounds float, float* %15, i64 %286
  %288 = bitcast float* %287 to <8 x float>*
  %wide.load39.3 = load <8 x float>, <8 x float>* %288, align 4, !tbaa !12, !llvm.access.group !16
  %289 = fsub <8 x float> %wide.load39.3, %wide.load38.3
  %290 = bitcast float* %287 to <8 x float>*
  store <8 x float> %289, <8 x float>* %290, align 4, !tbaa !12, !llvm.access.group !16
  %291 = getelementptr inbounds float, float* %11, i64 %282
  %292 = bitcast float* %291 to <8 x float>*
  %wide.load40.3 = load <8 x float>, <8 x float>* %292, align 4, !tbaa !12, !llvm.access.group !16
  %293 = fmul <8 x float> %broadcast.splat42, %wide.load40.3
  %294 = fdiv <8 x float> %289, %293, !fpmath !19
  %295 = bitcast float* %287 to <8 x float>*
  store <8 x float> %294, <8 x float>* %295, align 4, !tbaa !12, !llvm.access.group !16
  %296 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.4 = or i32 %296, 4
  %mul.i.i.4 = mul nsw i32 %23, %conv2.i.i.4
  %broadcast.splatinsert54 = insertelement <8 x float> undef, float %24, i32 0
  %broadcast.splat55 = shufflevector <8 x float> %broadcast.splatinsert54, <8 x float> undef, <8 x i32> zeroinitializer
  %297 = trunc i64 %mul.i.i.i to i32
  %298 = shl i64 %2, 37
  %299 = ashr exact i64 %298, 32
  %300 = getelementptr inbounds float, float* %7, i64 %299
  %301 = bitcast float* %300 to <8 x float>*
  %wide.load51 = load <8 x float>, <8 x float>* %301, align 4, !tbaa !12, !llvm.access.group !16
  %302 = add nsw i32 %mul.i.i.4, %297
  %303 = sext i32 %302 to i64
  %304 = getelementptr inbounds float, float* %15, i64 %303
  %305 = bitcast float* %304 to <8 x float>*
  %wide.load52 = load <8 x float>, <8 x float>* %305, align 4, !tbaa !12, !llvm.access.group !16
  %306 = fsub <8 x float> %wide.load52, %wide.load51
  %307 = bitcast float* %304 to <8 x float>*
  store <8 x float> %306, <8 x float>* %307, align 4, !tbaa !12, !llvm.access.group !16
  %308 = getelementptr inbounds float, float* %11, i64 %299
  %309 = bitcast float* %308 to <8 x float>*
  %wide.load53 = load <8 x float>, <8 x float>* %309, align 4, !tbaa !12, !llvm.access.group !16
  %310 = fmul <8 x float> %broadcast.splat55, %wide.load53
  %311 = fdiv <8 x float> %306, %310, !fpmath !19
  %312 = bitcast float* %304 to <8 x float>*
  store <8 x float> %311, <8 x float>* %312, align 4, !tbaa !12, !llvm.access.group !16
  %313 = or i64 %mul.i.i.i, 8
  %314 = trunc i64 %313 to i32
  %315 = shl i64 %313, 32
  %316 = ashr exact i64 %315, 32
  %317 = getelementptr inbounds float, float* %7, i64 %316
  %318 = bitcast float* %317 to <8 x float>*
  %wide.load51.1 = load <8 x float>, <8 x float>* %318, align 4, !tbaa !12, !llvm.access.group !16
  %319 = add nsw i32 %mul.i.i.4, %314
  %320 = sext i32 %319 to i64
  %321 = getelementptr inbounds float, float* %15, i64 %320
  %322 = bitcast float* %321 to <8 x float>*
  %wide.load52.1 = load <8 x float>, <8 x float>* %322, align 4, !tbaa !12, !llvm.access.group !16
  %323 = fsub <8 x float> %wide.load52.1, %wide.load51.1
  %324 = bitcast float* %321 to <8 x float>*
  store <8 x float> %323, <8 x float>* %324, align 4, !tbaa !12, !llvm.access.group !16
  %325 = getelementptr inbounds float, float* %11, i64 %316
  %326 = bitcast float* %325 to <8 x float>*
  %wide.load53.1 = load <8 x float>, <8 x float>* %326, align 4, !tbaa !12, !llvm.access.group !16
  %327 = fmul <8 x float> %broadcast.splat55, %wide.load53.1
  %328 = fdiv <8 x float> %323, %327, !fpmath !19
  %329 = bitcast float* %321 to <8 x float>*
  store <8 x float> %328, <8 x float>* %329, align 4, !tbaa !12, !llvm.access.group !16
  %330 = or i64 %mul.i.i.i, 16
  %331 = trunc i64 %330 to i32
  %332 = shl i64 %330, 32
  %333 = ashr exact i64 %332, 32
  %334 = getelementptr inbounds float, float* %7, i64 %333
  %335 = bitcast float* %334 to <8 x float>*
  %wide.load51.2 = load <8 x float>, <8 x float>* %335, align 4, !tbaa !12, !llvm.access.group !16
  %336 = add nsw i32 %mul.i.i.4, %331
  %337 = sext i32 %336 to i64
  %338 = getelementptr inbounds float, float* %15, i64 %337
  %339 = bitcast float* %338 to <8 x float>*
  %wide.load52.2 = load <8 x float>, <8 x float>* %339, align 4, !tbaa !12, !llvm.access.group !16
  %340 = fsub <8 x float> %wide.load52.2, %wide.load51.2
  %341 = bitcast float* %338 to <8 x float>*
  store <8 x float> %340, <8 x float>* %341, align 4, !tbaa !12, !llvm.access.group !16
  %342 = getelementptr inbounds float, float* %11, i64 %333
  %343 = bitcast float* %342 to <8 x float>*
  %wide.load53.2 = load <8 x float>, <8 x float>* %343, align 4, !tbaa !12, !llvm.access.group !16
  %344 = fmul <8 x float> %broadcast.splat55, %wide.load53.2
  %345 = fdiv <8 x float> %340, %344, !fpmath !19
  %346 = bitcast float* %338 to <8 x float>*
  store <8 x float> %345, <8 x float>* %346, align 4, !tbaa !12, !llvm.access.group !16
  %347 = or i64 %mul.i.i.i, 24
  %348 = trunc i64 %347 to i32
  %349 = shl i64 %347, 32
  %350 = ashr exact i64 %349, 32
  %351 = getelementptr inbounds float, float* %7, i64 %350
  %352 = bitcast float* %351 to <8 x float>*
  %wide.load51.3 = load <8 x float>, <8 x float>* %352, align 4, !tbaa !12, !llvm.access.group !16
  %353 = add nsw i32 %mul.i.i.4, %348
  %354 = sext i32 %353 to i64
  %355 = getelementptr inbounds float, float* %15, i64 %354
  %356 = bitcast float* %355 to <8 x float>*
  %wide.load52.3 = load <8 x float>, <8 x float>* %356, align 4, !tbaa !12, !llvm.access.group !16
  %357 = fsub <8 x float> %wide.load52.3, %wide.load51.3
  %358 = bitcast float* %355 to <8 x float>*
  store <8 x float> %357, <8 x float>* %358, align 4, !tbaa !12, !llvm.access.group !16
  %359 = getelementptr inbounds float, float* %11, i64 %350
  %360 = bitcast float* %359 to <8 x float>*
  %wide.load53.3 = load <8 x float>, <8 x float>* %360, align 4, !tbaa !12, !llvm.access.group !16
  %361 = fmul <8 x float> %broadcast.splat55, %wide.load53.3
  %362 = fdiv <8 x float> %357, %361, !fpmath !19
  %363 = bitcast float* %355 to <8 x float>*
  store <8 x float> %362, <8 x float>* %363, align 4, !tbaa !12, !llvm.access.group !16
  %364 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.5 = or i32 %364, 5
  %mul.i.i.5 = mul nsw i32 %23, %conv2.i.i.5
  %broadcast.splatinsert67 = insertelement <8 x float> undef, float %24, i32 0
  %broadcast.splat68 = shufflevector <8 x float> %broadcast.splatinsert67, <8 x float> undef, <8 x i32> zeroinitializer
  %365 = trunc i64 %mul.i.i.i to i32
  %366 = shl i64 %2, 37
  %367 = ashr exact i64 %366, 32
  %368 = getelementptr inbounds float, float* %7, i64 %367
  %369 = bitcast float* %368 to <8 x float>*
  %wide.load64 = load <8 x float>, <8 x float>* %369, align 4, !tbaa !12, !llvm.access.group !16
  %370 = add nsw i32 %mul.i.i.5, %365
  %371 = sext i32 %370 to i64
  %372 = getelementptr inbounds float, float* %15, i64 %371
  %373 = bitcast float* %372 to <8 x float>*
  %wide.load65 = load <8 x float>, <8 x float>* %373, align 4, !tbaa !12, !llvm.access.group !16
  %374 = fsub <8 x float> %wide.load65, %wide.load64
  %375 = bitcast float* %372 to <8 x float>*
  store <8 x float> %374, <8 x float>* %375, align 4, !tbaa !12, !llvm.access.group !16
  %376 = getelementptr inbounds float, float* %11, i64 %367
  %377 = bitcast float* %376 to <8 x float>*
  %wide.load66 = load <8 x float>, <8 x float>* %377, align 4, !tbaa !12, !llvm.access.group !16
  %378 = fmul <8 x float> %broadcast.splat68, %wide.load66
  %379 = fdiv <8 x float> %374, %378, !fpmath !19
  %380 = bitcast float* %372 to <8 x float>*
  store <8 x float> %379, <8 x float>* %380, align 4, !tbaa !12, !llvm.access.group !16
  %381 = or i64 %mul.i.i.i, 8
  %382 = trunc i64 %381 to i32
  %383 = shl i64 %381, 32
  %384 = ashr exact i64 %383, 32
  %385 = getelementptr inbounds float, float* %7, i64 %384
  %386 = bitcast float* %385 to <8 x float>*
  %wide.load64.1 = load <8 x float>, <8 x float>* %386, align 4, !tbaa !12, !llvm.access.group !16
  %387 = add nsw i32 %mul.i.i.5, %382
  %388 = sext i32 %387 to i64
  %389 = getelementptr inbounds float, float* %15, i64 %388
  %390 = bitcast float* %389 to <8 x float>*
  %wide.load65.1 = load <8 x float>, <8 x float>* %390, align 4, !tbaa !12, !llvm.access.group !16
  %391 = fsub <8 x float> %wide.load65.1, %wide.load64.1
  %392 = bitcast float* %389 to <8 x float>*
  store <8 x float> %391, <8 x float>* %392, align 4, !tbaa !12, !llvm.access.group !16
  %393 = getelementptr inbounds float, float* %11, i64 %384
  %394 = bitcast float* %393 to <8 x float>*
  %wide.load66.1 = load <8 x float>, <8 x float>* %394, align 4, !tbaa !12, !llvm.access.group !16
  %395 = fmul <8 x float> %broadcast.splat68, %wide.load66.1
  %396 = fdiv <8 x float> %391, %395, !fpmath !19
  %397 = bitcast float* %389 to <8 x float>*
  store <8 x float> %396, <8 x float>* %397, align 4, !tbaa !12, !llvm.access.group !16
  %398 = or i64 %mul.i.i.i, 16
  %399 = trunc i64 %398 to i32
  %400 = shl i64 %398, 32
  %401 = ashr exact i64 %400, 32
  %402 = getelementptr inbounds float, float* %7, i64 %401
  %403 = bitcast float* %402 to <8 x float>*
  %wide.load64.2 = load <8 x float>, <8 x float>* %403, align 4, !tbaa !12, !llvm.access.group !16
  %404 = add nsw i32 %mul.i.i.5, %399
  %405 = sext i32 %404 to i64
  %406 = getelementptr inbounds float, float* %15, i64 %405
  %407 = bitcast float* %406 to <8 x float>*
  %wide.load65.2 = load <8 x float>, <8 x float>* %407, align 4, !tbaa !12, !llvm.access.group !16
  %408 = fsub <8 x float> %wide.load65.2, %wide.load64.2
  %409 = bitcast float* %406 to <8 x float>*
  store <8 x float> %408, <8 x float>* %409, align 4, !tbaa !12, !llvm.access.group !16
  %410 = getelementptr inbounds float, float* %11, i64 %401
  %411 = bitcast float* %410 to <8 x float>*
  %wide.load66.2 = load <8 x float>, <8 x float>* %411, align 4, !tbaa !12, !llvm.access.group !16
  %412 = fmul <8 x float> %broadcast.splat68, %wide.load66.2
  %413 = fdiv <8 x float> %408, %412, !fpmath !19
  %414 = bitcast float* %406 to <8 x float>*
  store <8 x float> %413, <8 x float>* %414, align 4, !tbaa !12, !llvm.access.group !16
  %415 = or i64 %mul.i.i.i, 24
  %416 = trunc i64 %415 to i32
  %417 = shl i64 %415, 32
  %418 = ashr exact i64 %417, 32
  %419 = getelementptr inbounds float, float* %7, i64 %418
  %420 = bitcast float* %419 to <8 x float>*
  %wide.load64.3 = load <8 x float>, <8 x float>* %420, align 4, !tbaa !12, !llvm.access.group !16
  %421 = add nsw i32 %mul.i.i.5, %416
  %422 = sext i32 %421 to i64
  %423 = getelementptr inbounds float, float* %15, i64 %422
  %424 = bitcast float* %423 to <8 x float>*
  %wide.load65.3 = load <8 x float>, <8 x float>* %424, align 4, !tbaa !12, !llvm.access.group !16
  %425 = fsub <8 x float> %wide.load65.3, %wide.load64.3
  %426 = bitcast float* %423 to <8 x float>*
  store <8 x float> %425, <8 x float>* %426, align 4, !tbaa !12, !llvm.access.group !16
  %427 = getelementptr inbounds float, float* %11, i64 %418
  %428 = bitcast float* %427 to <8 x float>*
  %wide.load66.3 = load <8 x float>, <8 x float>* %428, align 4, !tbaa !12, !llvm.access.group !16
  %429 = fmul <8 x float> %broadcast.splat68, %wide.load66.3
  %430 = fdiv <8 x float> %425, %429, !fpmath !19
  %431 = bitcast float* %423 to <8 x float>*
  store <8 x float> %430, <8 x float>* %431, align 4, !tbaa !12, !llvm.access.group !16
  %432 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.6 = or i32 %432, 6
  %mul.i.i.6 = mul nsw i32 %23, %conv2.i.i.6
  %broadcast.splatinsert80 = insertelement <8 x float> undef, float %24, i32 0
  %broadcast.splat81 = shufflevector <8 x float> %broadcast.splatinsert80, <8 x float> undef, <8 x i32> zeroinitializer
  %433 = trunc i64 %mul.i.i.i to i32
  %434 = shl i64 %2, 37
  %435 = ashr exact i64 %434, 32
  %436 = getelementptr inbounds float, float* %7, i64 %435
  %437 = bitcast float* %436 to <8 x float>*
  %wide.load77 = load <8 x float>, <8 x float>* %437, align 4, !tbaa !12, !llvm.access.group !16
  %438 = add nsw i32 %mul.i.i.6, %433
  %439 = sext i32 %438 to i64
  %440 = getelementptr inbounds float, float* %15, i64 %439
  %441 = bitcast float* %440 to <8 x float>*
  %wide.load78 = load <8 x float>, <8 x float>* %441, align 4, !tbaa !12, !llvm.access.group !16
  %442 = fsub <8 x float> %wide.load78, %wide.load77
  %443 = bitcast float* %440 to <8 x float>*
  store <8 x float> %442, <8 x float>* %443, align 4, !tbaa !12, !llvm.access.group !16
  %444 = getelementptr inbounds float, float* %11, i64 %435
  %445 = bitcast float* %444 to <8 x float>*
  %wide.load79 = load <8 x float>, <8 x float>* %445, align 4, !tbaa !12, !llvm.access.group !16
  %446 = fmul <8 x float> %broadcast.splat81, %wide.load79
  %447 = fdiv <8 x float> %442, %446, !fpmath !19
  %448 = bitcast float* %440 to <8 x float>*
  store <8 x float> %447, <8 x float>* %448, align 4, !tbaa !12, !llvm.access.group !16
  %449 = or i64 %mul.i.i.i, 8
  %450 = trunc i64 %449 to i32
  %451 = shl i64 %449, 32
  %452 = ashr exact i64 %451, 32
  %453 = getelementptr inbounds float, float* %7, i64 %452
  %454 = bitcast float* %453 to <8 x float>*
  %wide.load77.1 = load <8 x float>, <8 x float>* %454, align 4, !tbaa !12, !llvm.access.group !16
  %455 = add nsw i32 %mul.i.i.6, %450
  %456 = sext i32 %455 to i64
  %457 = getelementptr inbounds float, float* %15, i64 %456
  %458 = bitcast float* %457 to <8 x float>*
  %wide.load78.1 = load <8 x float>, <8 x float>* %458, align 4, !tbaa !12, !llvm.access.group !16
  %459 = fsub <8 x float> %wide.load78.1, %wide.load77.1
  %460 = bitcast float* %457 to <8 x float>*
  store <8 x float> %459, <8 x float>* %460, align 4, !tbaa !12, !llvm.access.group !16
  %461 = getelementptr inbounds float, float* %11, i64 %452
  %462 = bitcast float* %461 to <8 x float>*
  %wide.load79.1 = load <8 x float>, <8 x float>* %462, align 4, !tbaa !12, !llvm.access.group !16
  %463 = fmul <8 x float> %broadcast.splat81, %wide.load79.1
  %464 = fdiv <8 x float> %459, %463, !fpmath !19
  %465 = bitcast float* %457 to <8 x float>*
  store <8 x float> %464, <8 x float>* %465, align 4, !tbaa !12, !llvm.access.group !16
  %466 = or i64 %mul.i.i.i, 16
  %467 = trunc i64 %466 to i32
  %468 = shl i64 %466, 32
  %469 = ashr exact i64 %468, 32
  %470 = getelementptr inbounds float, float* %7, i64 %469
  %471 = bitcast float* %470 to <8 x float>*
  %wide.load77.2 = load <8 x float>, <8 x float>* %471, align 4, !tbaa !12, !llvm.access.group !16
  %472 = add nsw i32 %mul.i.i.6, %467
  %473 = sext i32 %472 to i64
  %474 = getelementptr inbounds float, float* %15, i64 %473
  %475 = bitcast float* %474 to <8 x float>*
  %wide.load78.2 = load <8 x float>, <8 x float>* %475, align 4, !tbaa !12, !llvm.access.group !16
  %476 = fsub <8 x float> %wide.load78.2, %wide.load77.2
  %477 = bitcast float* %474 to <8 x float>*
  store <8 x float> %476, <8 x float>* %477, align 4, !tbaa !12, !llvm.access.group !16
  %478 = getelementptr inbounds float, float* %11, i64 %469
  %479 = bitcast float* %478 to <8 x float>*
  %wide.load79.2 = load <8 x float>, <8 x float>* %479, align 4, !tbaa !12, !llvm.access.group !16
  %480 = fmul <8 x float> %broadcast.splat81, %wide.load79.2
  %481 = fdiv <8 x float> %476, %480, !fpmath !19
  %482 = bitcast float* %474 to <8 x float>*
  store <8 x float> %481, <8 x float>* %482, align 4, !tbaa !12, !llvm.access.group !16
  %483 = or i64 %mul.i.i.i, 24
  %484 = trunc i64 %483 to i32
  %485 = shl i64 %483, 32
  %486 = ashr exact i64 %485, 32
  %487 = getelementptr inbounds float, float* %7, i64 %486
  %488 = bitcast float* %487 to <8 x float>*
  %wide.load77.3 = load <8 x float>, <8 x float>* %488, align 4, !tbaa !12, !llvm.access.group !16
  %489 = add nsw i32 %mul.i.i.6, %484
  %490 = sext i32 %489 to i64
  %491 = getelementptr inbounds float, float* %15, i64 %490
  %492 = bitcast float* %491 to <8 x float>*
  %wide.load78.3 = load <8 x float>, <8 x float>* %492, align 4, !tbaa !12, !llvm.access.group !16
  %493 = fsub <8 x float> %wide.load78.3, %wide.load77.3
  %494 = bitcast float* %491 to <8 x float>*
  store <8 x float> %493, <8 x float>* %494, align 4, !tbaa !12, !llvm.access.group !16
  %495 = getelementptr inbounds float, float* %11, i64 %486
  %496 = bitcast float* %495 to <8 x float>*
  %wide.load79.3 = load <8 x float>, <8 x float>* %496, align 4, !tbaa !12, !llvm.access.group !16
  %497 = fmul <8 x float> %broadcast.splat81, %wide.load79.3
  %498 = fdiv <8 x float> %493, %497, !fpmath !19
  %499 = bitcast float* %491 to <8 x float>*
  store <8 x float> %498, <8 x float>* %499, align 4, !tbaa !12, !llvm.access.group !16
  %500 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.7 = or i32 %500, 7
  %mul.i.i.7 = mul nsw i32 %23, %conv2.i.i.7
  %broadcast.splatinsert93 = insertelement <8 x float> undef, float %24, i32 0
  %broadcast.splat94 = shufflevector <8 x float> %broadcast.splatinsert93, <8 x float> undef, <8 x i32> zeroinitializer
  %501 = trunc i64 %mul.i.i.i to i32
  %502 = shl i64 %2, 37
  %503 = ashr exact i64 %502, 32
  %504 = getelementptr inbounds float, float* %7, i64 %503
  %505 = bitcast float* %504 to <8 x float>*
  %wide.load90 = load <8 x float>, <8 x float>* %505, align 4, !tbaa !12, !llvm.access.group !16
  %506 = add nsw i32 %mul.i.i.7, %501
  %507 = sext i32 %506 to i64
  %508 = getelementptr inbounds float, float* %15, i64 %507
  %509 = bitcast float* %508 to <8 x float>*
  %wide.load91 = load <8 x float>, <8 x float>* %509, align 4, !tbaa !12, !llvm.access.group !16
  %510 = fsub <8 x float> %wide.load91, %wide.load90
  %511 = bitcast float* %508 to <8 x float>*
  store <8 x float> %510, <8 x float>* %511, align 4, !tbaa !12, !llvm.access.group !16
  %512 = getelementptr inbounds float, float* %11, i64 %503
  %513 = bitcast float* %512 to <8 x float>*
  %wide.load92 = load <8 x float>, <8 x float>* %513, align 4, !tbaa !12, !llvm.access.group !16
  %514 = fmul <8 x float> %broadcast.splat94, %wide.load92
  %515 = fdiv <8 x float> %510, %514, !fpmath !19
  %516 = bitcast float* %508 to <8 x float>*
  store <8 x float> %515, <8 x float>* %516, align 4, !tbaa !12, !llvm.access.group !16
  %517 = or i64 %mul.i.i.i, 8
  %518 = trunc i64 %517 to i32
  %519 = shl i64 %517, 32
  %520 = ashr exact i64 %519, 32
  %521 = getelementptr inbounds float, float* %7, i64 %520
  %522 = bitcast float* %521 to <8 x float>*
  %wide.load90.1 = load <8 x float>, <8 x float>* %522, align 4, !tbaa !12, !llvm.access.group !16
  %523 = add nsw i32 %mul.i.i.7, %518
  %524 = sext i32 %523 to i64
  %525 = getelementptr inbounds float, float* %15, i64 %524
  %526 = bitcast float* %525 to <8 x float>*
  %wide.load91.1 = load <8 x float>, <8 x float>* %526, align 4, !tbaa !12, !llvm.access.group !16
  %527 = fsub <8 x float> %wide.load91.1, %wide.load90.1
  %528 = bitcast float* %525 to <8 x float>*
  store <8 x float> %527, <8 x float>* %528, align 4, !tbaa !12, !llvm.access.group !16
  %529 = getelementptr inbounds float, float* %11, i64 %520
  %530 = bitcast float* %529 to <8 x float>*
  %wide.load92.1 = load <8 x float>, <8 x float>* %530, align 4, !tbaa !12, !llvm.access.group !16
  %531 = fmul <8 x float> %broadcast.splat94, %wide.load92.1
  %532 = fdiv <8 x float> %527, %531, !fpmath !19
  %533 = bitcast float* %525 to <8 x float>*
  store <8 x float> %532, <8 x float>* %533, align 4, !tbaa !12, !llvm.access.group !16
  %534 = or i64 %mul.i.i.i, 16
  %535 = trunc i64 %534 to i32
  %536 = shl i64 %534, 32
  %537 = ashr exact i64 %536, 32
  %538 = getelementptr inbounds float, float* %7, i64 %537
  %539 = bitcast float* %538 to <8 x float>*
  %wide.load90.2 = load <8 x float>, <8 x float>* %539, align 4, !tbaa !12, !llvm.access.group !16
  %540 = add nsw i32 %mul.i.i.7, %535
  %541 = sext i32 %540 to i64
  %542 = getelementptr inbounds float, float* %15, i64 %541
  %543 = bitcast float* %542 to <8 x float>*
  %wide.load91.2 = load <8 x float>, <8 x float>* %543, align 4, !tbaa !12, !llvm.access.group !16
  %544 = fsub <8 x float> %wide.load91.2, %wide.load90.2
  %545 = bitcast float* %542 to <8 x float>*
  store <8 x float> %544, <8 x float>* %545, align 4, !tbaa !12, !llvm.access.group !16
  %546 = getelementptr inbounds float, float* %11, i64 %537
  %547 = bitcast float* %546 to <8 x float>*
  %wide.load92.2 = load <8 x float>, <8 x float>* %547, align 4, !tbaa !12, !llvm.access.group !16
  %548 = fmul <8 x float> %broadcast.splat94, %wide.load92.2
  %549 = fdiv <8 x float> %544, %548, !fpmath !19
  %550 = bitcast float* %542 to <8 x float>*
  store <8 x float> %549, <8 x float>* %550, align 4, !tbaa !12, !llvm.access.group !16
  %551 = or i64 %mul.i.i.i, 24
  %552 = trunc i64 %551 to i32
  %553 = shl i64 %551, 32
  %554 = ashr exact i64 %553, 32
  %555 = getelementptr inbounds float, float* %7, i64 %554
  %556 = bitcast float* %555 to <8 x float>*
  %wide.load90.3 = load <8 x float>, <8 x float>* %556, align 4, !tbaa !12, !llvm.access.group !16
  %557 = add nsw i32 %mul.i.i.7, %552
  %558 = sext i32 %557 to i64
  %559 = getelementptr inbounds float, float* %15, i64 %558
  %560 = bitcast float* %559 to <8 x float>*
  %wide.load91.3 = load <8 x float>, <8 x float>* %560, align 4, !tbaa !12, !llvm.access.group !16
  %561 = fsub <8 x float> %wide.load91.3, %wide.load90.3
  %562 = bitcast float* %559 to <8 x float>*
  store <8 x float> %561, <8 x float>* %562, align 4, !tbaa !12, !llvm.access.group !16
  %563 = getelementptr inbounds float, float* %11, i64 %554
  %564 = bitcast float* %563 to <8 x float>*
  %wide.load92.3 = load <8 x float>, <8 x float>* %564, align 4, !tbaa !12, !llvm.access.group !16
  %565 = fmul <8 x float> %broadcast.splat94, %wide.load92.3
  %566 = fdiv <8 x float> %561, %565, !fpmath !19
  %567 = bitcast float* %559 to <8 x float>*
  store <8 x float> %566, <8 x float>* %567, align 4, !tbaa !12, !llvm.access.group !16
  ret void
}

; Function Attrs: nofree nounwind
define void @_pocl_kernel_reduce_kernel_workgroup_fast(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #2 {
pregion_for_entry.pregion_for_init.i.i:
  %5 = bitcast i8** %0 to float**
  %6 = load float*, float** %5, align 8
  %7 = getelementptr i8*, i8** %0, i64 1
  %8 = bitcast i8** %7 to float**
  %9 = load float*, float** %8, align 8
  %10 = getelementptr i8*, i8** %0, i64 2
  %11 = bitcast i8** %10 to float**
  %12 = load float*, float** %11, align 8
  %13 = getelementptr i8*, i8** %0, i64 3
  %14 = bitcast i8** %13 to float**
  %15 = load float*, float** %14, align 8
  %16 = load float, float* %15, align 4
  %17 = getelementptr i8*, i8** %0, i64 4
  %18 = bitcast i8** %17 to i32**
  %19 = load i32*, i32** %18, align 8
  %20 = load i32, i32* %19, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %21 = tail call float @llvm.sqrt.f32(float %16) #3
  %conv2.i.i = trunc i64 %mul3.i.i.i to i32
  %mul.i.i = mul nsw i32 %20, %conv2.i.i
  %broadcast.splatinsert = insertelement <8 x float> undef, float %21, i32 0
  %broadcast.splat = shufflevector <8 x float> %broadcast.splatinsert, <8 x float> undef, <8 x i32> zeroinitializer
  %22 = trunc i64 %mul.i.i.i to i32
  %23 = shl i64 %2, 37
  %24 = ashr exact i64 %23, 32
  %25 = getelementptr inbounds float, float* %6, i64 %24
  %26 = bitcast float* %25 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %26, align 4, !tbaa !12, !llvm.access.group !16
  %27 = add nsw i32 %mul.i.i, %22
  %28 = sext i32 %27 to i64
  %29 = getelementptr inbounds float, float* %12, i64 %28
  %30 = bitcast float* %29 to <8 x float>*
  %wide.load2 = load <8 x float>, <8 x float>* %30, align 4, !tbaa !12, !llvm.access.group !16
  %31 = fsub <8 x float> %wide.load2, %wide.load
  %32 = bitcast float* %29 to <8 x float>*
  store <8 x float> %31, <8 x float>* %32, align 4, !tbaa !12, !llvm.access.group !16
  %33 = getelementptr inbounds float, float* %9, i64 %24
  %34 = bitcast float* %33 to <8 x float>*
  %wide.load3 = load <8 x float>, <8 x float>* %34, align 4, !tbaa !12, !llvm.access.group !16
  %35 = fmul <8 x float> %broadcast.splat, %wide.load3
  %36 = fdiv <8 x float> %31, %35, !fpmath !19
  %37 = bitcast float* %29 to <8 x float>*
  store <8 x float> %36, <8 x float>* %37, align 4, !tbaa !12, !llvm.access.group !16
  %38 = or i64 %mul.i.i.i, 8
  %39 = trunc i64 %38 to i32
  %40 = shl i64 %38, 32
  %41 = ashr exact i64 %40, 32
  %42 = getelementptr inbounds float, float* %6, i64 %41
  %43 = bitcast float* %42 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %43, align 4, !tbaa !12, !llvm.access.group !16
  %44 = add nsw i32 %mul.i.i, %39
  %45 = sext i32 %44 to i64
  %46 = getelementptr inbounds float, float* %12, i64 %45
  %47 = bitcast float* %46 to <8 x float>*
  %wide.load2.1 = load <8 x float>, <8 x float>* %47, align 4, !tbaa !12, !llvm.access.group !16
  %48 = fsub <8 x float> %wide.load2.1, %wide.load.1
  %49 = bitcast float* %46 to <8 x float>*
  store <8 x float> %48, <8 x float>* %49, align 4, !tbaa !12, !llvm.access.group !16
  %50 = getelementptr inbounds float, float* %9, i64 %41
  %51 = bitcast float* %50 to <8 x float>*
  %wide.load3.1 = load <8 x float>, <8 x float>* %51, align 4, !tbaa !12, !llvm.access.group !16
  %52 = fmul <8 x float> %broadcast.splat, %wide.load3.1
  %53 = fdiv <8 x float> %48, %52, !fpmath !19
  %54 = bitcast float* %46 to <8 x float>*
  store <8 x float> %53, <8 x float>* %54, align 4, !tbaa !12, !llvm.access.group !16
  %55 = or i64 %mul.i.i.i, 16
  %56 = trunc i64 %55 to i32
  %57 = shl i64 %55, 32
  %58 = ashr exact i64 %57, 32
  %59 = getelementptr inbounds float, float* %6, i64 %58
  %60 = bitcast float* %59 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %60, align 4, !tbaa !12, !llvm.access.group !16
  %61 = add nsw i32 %mul.i.i, %56
  %62 = sext i32 %61 to i64
  %63 = getelementptr inbounds float, float* %12, i64 %62
  %64 = bitcast float* %63 to <8 x float>*
  %wide.load2.2 = load <8 x float>, <8 x float>* %64, align 4, !tbaa !12, !llvm.access.group !16
  %65 = fsub <8 x float> %wide.load2.2, %wide.load.2
  %66 = bitcast float* %63 to <8 x float>*
  store <8 x float> %65, <8 x float>* %66, align 4, !tbaa !12, !llvm.access.group !16
  %67 = getelementptr inbounds float, float* %9, i64 %58
  %68 = bitcast float* %67 to <8 x float>*
  %wide.load3.2 = load <8 x float>, <8 x float>* %68, align 4, !tbaa !12, !llvm.access.group !16
  %69 = fmul <8 x float> %broadcast.splat, %wide.load3.2
  %70 = fdiv <8 x float> %65, %69, !fpmath !19
  %71 = bitcast float* %63 to <8 x float>*
  store <8 x float> %70, <8 x float>* %71, align 4, !tbaa !12, !llvm.access.group !16
  %72 = or i64 %mul.i.i.i, 24
  %73 = trunc i64 %72 to i32
  %74 = shl i64 %72, 32
  %75 = ashr exact i64 %74, 32
  %76 = getelementptr inbounds float, float* %6, i64 %75
  %77 = bitcast float* %76 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %77, align 4, !tbaa !12, !llvm.access.group !16
  %78 = add nsw i32 %mul.i.i, %73
  %79 = sext i32 %78 to i64
  %80 = getelementptr inbounds float, float* %12, i64 %79
  %81 = bitcast float* %80 to <8 x float>*
  %wide.load2.3 = load <8 x float>, <8 x float>* %81, align 4, !tbaa !12, !llvm.access.group !16
  %82 = fsub <8 x float> %wide.load2.3, %wide.load.3
  %83 = bitcast float* %80 to <8 x float>*
  store <8 x float> %82, <8 x float>* %83, align 4, !tbaa !12, !llvm.access.group !16
  %84 = getelementptr inbounds float, float* %9, i64 %75
  %85 = bitcast float* %84 to <8 x float>*
  %wide.load3.3 = load <8 x float>, <8 x float>* %85, align 4, !tbaa !12, !llvm.access.group !16
  %86 = fmul <8 x float> %broadcast.splat, %wide.load3.3
  %87 = fdiv <8 x float> %82, %86, !fpmath !19
  %88 = bitcast float* %80 to <8 x float>*
  store <8 x float> %87, <8 x float>* %88, align 4, !tbaa !12, !llvm.access.group !16
  %89 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.1 = or i32 %89, 1
  %mul.i.i.1 = mul nsw i32 %20, %conv2.i.i.1
  %broadcast.splatinsert15 = insertelement <8 x float> undef, float %21, i32 0
  %broadcast.splat16 = shufflevector <8 x float> %broadcast.splatinsert15, <8 x float> undef, <8 x i32> zeroinitializer
  %90 = trunc i64 %mul.i.i.i to i32
  %91 = shl i64 %2, 37
  %92 = ashr exact i64 %91, 32
  %93 = getelementptr inbounds float, float* %6, i64 %92
  %94 = bitcast float* %93 to <8 x float>*
  %wide.load12 = load <8 x float>, <8 x float>* %94, align 4, !tbaa !12, !llvm.access.group !16
  %95 = add nsw i32 %mul.i.i.1, %90
  %96 = sext i32 %95 to i64
  %97 = getelementptr inbounds float, float* %12, i64 %96
  %98 = bitcast float* %97 to <8 x float>*
  %wide.load13 = load <8 x float>, <8 x float>* %98, align 4, !tbaa !12, !llvm.access.group !16
  %99 = fsub <8 x float> %wide.load13, %wide.load12
  %100 = bitcast float* %97 to <8 x float>*
  store <8 x float> %99, <8 x float>* %100, align 4, !tbaa !12, !llvm.access.group !16
  %101 = getelementptr inbounds float, float* %9, i64 %92
  %102 = bitcast float* %101 to <8 x float>*
  %wide.load14 = load <8 x float>, <8 x float>* %102, align 4, !tbaa !12, !llvm.access.group !16
  %103 = fmul <8 x float> %broadcast.splat16, %wide.load14
  %104 = fdiv <8 x float> %99, %103, !fpmath !19
  %105 = bitcast float* %97 to <8 x float>*
  store <8 x float> %104, <8 x float>* %105, align 4, !tbaa !12, !llvm.access.group !16
  %106 = or i64 %mul.i.i.i, 8
  %107 = trunc i64 %106 to i32
  %108 = shl i64 %106, 32
  %109 = ashr exact i64 %108, 32
  %110 = getelementptr inbounds float, float* %6, i64 %109
  %111 = bitcast float* %110 to <8 x float>*
  %wide.load12.1 = load <8 x float>, <8 x float>* %111, align 4, !tbaa !12, !llvm.access.group !16
  %112 = add nsw i32 %mul.i.i.1, %107
  %113 = sext i32 %112 to i64
  %114 = getelementptr inbounds float, float* %12, i64 %113
  %115 = bitcast float* %114 to <8 x float>*
  %wide.load13.1 = load <8 x float>, <8 x float>* %115, align 4, !tbaa !12, !llvm.access.group !16
  %116 = fsub <8 x float> %wide.load13.1, %wide.load12.1
  %117 = bitcast float* %114 to <8 x float>*
  store <8 x float> %116, <8 x float>* %117, align 4, !tbaa !12, !llvm.access.group !16
  %118 = getelementptr inbounds float, float* %9, i64 %109
  %119 = bitcast float* %118 to <8 x float>*
  %wide.load14.1 = load <8 x float>, <8 x float>* %119, align 4, !tbaa !12, !llvm.access.group !16
  %120 = fmul <8 x float> %broadcast.splat16, %wide.load14.1
  %121 = fdiv <8 x float> %116, %120, !fpmath !19
  %122 = bitcast float* %114 to <8 x float>*
  store <8 x float> %121, <8 x float>* %122, align 4, !tbaa !12, !llvm.access.group !16
  %123 = or i64 %mul.i.i.i, 16
  %124 = trunc i64 %123 to i32
  %125 = shl i64 %123, 32
  %126 = ashr exact i64 %125, 32
  %127 = getelementptr inbounds float, float* %6, i64 %126
  %128 = bitcast float* %127 to <8 x float>*
  %wide.load12.2 = load <8 x float>, <8 x float>* %128, align 4, !tbaa !12, !llvm.access.group !16
  %129 = add nsw i32 %mul.i.i.1, %124
  %130 = sext i32 %129 to i64
  %131 = getelementptr inbounds float, float* %12, i64 %130
  %132 = bitcast float* %131 to <8 x float>*
  %wide.load13.2 = load <8 x float>, <8 x float>* %132, align 4, !tbaa !12, !llvm.access.group !16
  %133 = fsub <8 x float> %wide.load13.2, %wide.load12.2
  %134 = bitcast float* %131 to <8 x float>*
  store <8 x float> %133, <8 x float>* %134, align 4, !tbaa !12, !llvm.access.group !16
  %135 = getelementptr inbounds float, float* %9, i64 %126
  %136 = bitcast float* %135 to <8 x float>*
  %wide.load14.2 = load <8 x float>, <8 x float>* %136, align 4, !tbaa !12, !llvm.access.group !16
  %137 = fmul <8 x float> %broadcast.splat16, %wide.load14.2
  %138 = fdiv <8 x float> %133, %137, !fpmath !19
  %139 = bitcast float* %131 to <8 x float>*
  store <8 x float> %138, <8 x float>* %139, align 4, !tbaa !12, !llvm.access.group !16
  %140 = or i64 %mul.i.i.i, 24
  %141 = trunc i64 %140 to i32
  %142 = shl i64 %140, 32
  %143 = ashr exact i64 %142, 32
  %144 = getelementptr inbounds float, float* %6, i64 %143
  %145 = bitcast float* %144 to <8 x float>*
  %wide.load12.3 = load <8 x float>, <8 x float>* %145, align 4, !tbaa !12, !llvm.access.group !16
  %146 = add nsw i32 %mul.i.i.1, %141
  %147 = sext i32 %146 to i64
  %148 = getelementptr inbounds float, float* %12, i64 %147
  %149 = bitcast float* %148 to <8 x float>*
  %wide.load13.3 = load <8 x float>, <8 x float>* %149, align 4, !tbaa !12, !llvm.access.group !16
  %150 = fsub <8 x float> %wide.load13.3, %wide.load12.3
  %151 = bitcast float* %148 to <8 x float>*
  store <8 x float> %150, <8 x float>* %151, align 4, !tbaa !12, !llvm.access.group !16
  %152 = getelementptr inbounds float, float* %9, i64 %143
  %153 = bitcast float* %152 to <8 x float>*
  %wide.load14.3 = load <8 x float>, <8 x float>* %153, align 4, !tbaa !12, !llvm.access.group !16
  %154 = fmul <8 x float> %broadcast.splat16, %wide.load14.3
  %155 = fdiv <8 x float> %150, %154, !fpmath !19
  %156 = bitcast float* %148 to <8 x float>*
  store <8 x float> %155, <8 x float>* %156, align 4, !tbaa !12, !llvm.access.group !16
  %157 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.2 = or i32 %157, 2
  %mul.i.i.2 = mul nsw i32 %20, %conv2.i.i.2
  %broadcast.splatinsert28 = insertelement <8 x float> undef, float %21, i32 0
  %broadcast.splat29 = shufflevector <8 x float> %broadcast.splatinsert28, <8 x float> undef, <8 x i32> zeroinitializer
  %158 = trunc i64 %mul.i.i.i to i32
  %159 = shl i64 %2, 37
  %160 = ashr exact i64 %159, 32
  %161 = getelementptr inbounds float, float* %6, i64 %160
  %162 = bitcast float* %161 to <8 x float>*
  %wide.load25 = load <8 x float>, <8 x float>* %162, align 4, !tbaa !12, !llvm.access.group !16
  %163 = add nsw i32 %mul.i.i.2, %158
  %164 = sext i32 %163 to i64
  %165 = getelementptr inbounds float, float* %12, i64 %164
  %166 = bitcast float* %165 to <8 x float>*
  %wide.load26 = load <8 x float>, <8 x float>* %166, align 4, !tbaa !12, !llvm.access.group !16
  %167 = fsub <8 x float> %wide.load26, %wide.load25
  %168 = bitcast float* %165 to <8 x float>*
  store <8 x float> %167, <8 x float>* %168, align 4, !tbaa !12, !llvm.access.group !16
  %169 = getelementptr inbounds float, float* %9, i64 %160
  %170 = bitcast float* %169 to <8 x float>*
  %wide.load27 = load <8 x float>, <8 x float>* %170, align 4, !tbaa !12, !llvm.access.group !16
  %171 = fmul <8 x float> %broadcast.splat29, %wide.load27
  %172 = fdiv <8 x float> %167, %171, !fpmath !19
  %173 = bitcast float* %165 to <8 x float>*
  store <8 x float> %172, <8 x float>* %173, align 4, !tbaa !12, !llvm.access.group !16
  %174 = or i64 %mul.i.i.i, 8
  %175 = trunc i64 %174 to i32
  %176 = shl i64 %174, 32
  %177 = ashr exact i64 %176, 32
  %178 = getelementptr inbounds float, float* %6, i64 %177
  %179 = bitcast float* %178 to <8 x float>*
  %wide.load25.1 = load <8 x float>, <8 x float>* %179, align 4, !tbaa !12, !llvm.access.group !16
  %180 = add nsw i32 %mul.i.i.2, %175
  %181 = sext i32 %180 to i64
  %182 = getelementptr inbounds float, float* %12, i64 %181
  %183 = bitcast float* %182 to <8 x float>*
  %wide.load26.1 = load <8 x float>, <8 x float>* %183, align 4, !tbaa !12, !llvm.access.group !16
  %184 = fsub <8 x float> %wide.load26.1, %wide.load25.1
  %185 = bitcast float* %182 to <8 x float>*
  store <8 x float> %184, <8 x float>* %185, align 4, !tbaa !12, !llvm.access.group !16
  %186 = getelementptr inbounds float, float* %9, i64 %177
  %187 = bitcast float* %186 to <8 x float>*
  %wide.load27.1 = load <8 x float>, <8 x float>* %187, align 4, !tbaa !12, !llvm.access.group !16
  %188 = fmul <8 x float> %broadcast.splat29, %wide.load27.1
  %189 = fdiv <8 x float> %184, %188, !fpmath !19
  %190 = bitcast float* %182 to <8 x float>*
  store <8 x float> %189, <8 x float>* %190, align 4, !tbaa !12, !llvm.access.group !16
  %191 = or i64 %mul.i.i.i, 16
  %192 = trunc i64 %191 to i32
  %193 = shl i64 %191, 32
  %194 = ashr exact i64 %193, 32
  %195 = getelementptr inbounds float, float* %6, i64 %194
  %196 = bitcast float* %195 to <8 x float>*
  %wide.load25.2 = load <8 x float>, <8 x float>* %196, align 4, !tbaa !12, !llvm.access.group !16
  %197 = add nsw i32 %mul.i.i.2, %192
  %198 = sext i32 %197 to i64
  %199 = getelementptr inbounds float, float* %12, i64 %198
  %200 = bitcast float* %199 to <8 x float>*
  %wide.load26.2 = load <8 x float>, <8 x float>* %200, align 4, !tbaa !12, !llvm.access.group !16
  %201 = fsub <8 x float> %wide.load26.2, %wide.load25.2
  %202 = bitcast float* %199 to <8 x float>*
  store <8 x float> %201, <8 x float>* %202, align 4, !tbaa !12, !llvm.access.group !16
  %203 = getelementptr inbounds float, float* %9, i64 %194
  %204 = bitcast float* %203 to <8 x float>*
  %wide.load27.2 = load <8 x float>, <8 x float>* %204, align 4, !tbaa !12, !llvm.access.group !16
  %205 = fmul <8 x float> %broadcast.splat29, %wide.load27.2
  %206 = fdiv <8 x float> %201, %205, !fpmath !19
  %207 = bitcast float* %199 to <8 x float>*
  store <8 x float> %206, <8 x float>* %207, align 4, !tbaa !12, !llvm.access.group !16
  %208 = or i64 %mul.i.i.i, 24
  %209 = trunc i64 %208 to i32
  %210 = shl i64 %208, 32
  %211 = ashr exact i64 %210, 32
  %212 = getelementptr inbounds float, float* %6, i64 %211
  %213 = bitcast float* %212 to <8 x float>*
  %wide.load25.3 = load <8 x float>, <8 x float>* %213, align 4, !tbaa !12, !llvm.access.group !16
  %214 = add nsw i32 %mul.i.i.2, %209
  %215 = sext i32 %214 to i64
  %216 = getelementptr inbounds float, float* %12, i64 %215
  %217 = bitcast float* %216 to <8 x float>*
  %wide.load26.3 = load <8 x float>, <8 x float>* %217, align 4, !tbaa !12, !llvm.access.group !16
  %218 = fsub <8 x float> %wide.load26.3, %wide.load25.3
  %219 = bitcast float* %216 to <8 x float>*
  store <8 x float> %218, <8 x float>* %219, align 4, !tbaa !12, !llvm.access.group !16
  %220 = getelementptr inbounds float, float* %9, i64 %211
  %221 = bitcast float* %220 to <8 x float>*
  %wide.load27.3 = load <8 x float>, <8 x float>* %221, align 4, !tbaa !12, !llvm.access.group !16
  %222 = fmul <8 x float> %broadcast.splat29, %wide.load27.3
  %223 = fdiv <8 x float> %218, %222, !fpmath !19
  %224 = bitcast float* %216 to <8 x float>*
  store <8 x float> %223, <8 x float>* %224, align 4, !tbaa !12, !llvm.access.group !16
  %225 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.3 = or i32 %225, 3
  %mul.i.i.3 = mul nsw i32 %20, %conv2.i.i.3
  %broadcast.splatinsert41 = insertelement <8 x float> undef, float %21, i32 0
  %broadcast.splat42 = shufflevector <8 x float> %broadcast.splatinsert41, <8 x float> undef, <8 x i32> zeroinitializer
  %226 = trunc i64 %mul.i.i.i to i32
  %227 = shl i64 %2, 37
  %228 = ashr exact i64 %227, 32
  %229 = getelementptr inbounds float, float* %6, i64 %228
  %230 = bitcast float* %229 to <8 x float>*
  %wide.load38 = load <8 x float>, <8 x float>* %230, align 4, !tbaa !12, !llvm.access.group !16
  %231 = add nsw i32 %mul.i.i.3, %226
  %232 = sext i32 %231 to i64
  %233 = getelementptr inbounds float, float* %12, i64 %232
  %234 = bitcast float* %233 to <8 x float>*
  %wide.load39 = load <8 x float>, <8 x float>* %234, align 4, !tbaa !12, !llvm.access.group !16
  %235 = fsub <8 x float> %wide.load39, %wide.load38
  %236 = bitcast float* %233 to <8 x float>*
  store <8 x float> %235, <8 x float>* %236, align 4, !tbaa !12, !llvm.access.group !16
  %237 = getelementptr inbounds float, float* %9, i64 %228
  %238 = bitcast float* %237 to <8 x float>*
  %wide.load40 = load <8 x float>, <8 x float>* %238, align 4, !tbaa !12, !llvm.access.group !16
  %239 = fmul <8 x float> %broadcast.splat42, %wide.load40
  %240 = fdiv <8 x float> %235, %239, !fpmath !19
  %241 = bitcast float* %233 to <8 x float>*
  store <8 x float> %240, <8 x float>* %241, align 4, !tbaa !12, !llvm.access.group !16
  %242 = or i64 %mul.i.i.i, 8
  %243 = trunc i64 %242 to i32
  %244 = shl i64 %242, 32
  %245 = ashr exact i64 %244, 32
  %246 = getelementptr inbounds float, float* %6, i64 %245
  %247 = bitcast float* %246 to <8 x float>*
  %wide.load38.1 = load <8 x float>, <8 x float>* %247, align 4, !tbaa !12, !llvm.access.group !16
  %248 = add nsw i32 %mul.i.i.3, %243
  %249 = sext i32 %248 to i64
  %250 = getelementptr inbounds float, float* %12, i64 %249
  %251 = bitcast float* %250 to <8 x float>*
  %wide.load39.1 = load <8 x float>, <8 x float>* %251, align 4, !tbaa !12, !llvm.access.group !16
  %252 = fsub <8 x float> %wide.load39.1, %wide.load38.1
  %253 = bitcast float* %250 to <8 x float>*
  store <8 x float> %252, <8 x float>* %253, align 4, !tbaa !12, !llvm.access.group !16
  %254 = getelementptr inbounds float, float* %9, i64 %245
  %255 = bitcast float* %254 to <8 x float>*
  %wide.load40.1 = load <8 x float>, <8 x float>* %255, align 4, !tbaa !12, !llvm.access.group !16
  %256 = fmul <8 x float> %broadcast.splat42, %wide.load40.1
  %257 = fdiv <8 x float> %252, %256, !fpmath !19
  %258 = bitcast float* %250 to <8 x float>*
  store <8 x float> %257, <8 x float>* %258, align 4, !tbaa !12, !llvm.access.group !16
  %259 = or i64 %mul.i.i.i, 16
  %260 = trunc i64 %259 to i32
  %261 = shl i64 %259, 32
  %262 = ashr exact i64 %261, 32
  %263 = getelementptr inbounds float, float* %6, i64 %262
  %264 = bitcast float* %263 to <8 x float>*
  %wide.load38.2 = load <8 x float>, <8 x float>* %264, align 4, !tbaa !12, !llvm.access.group !16
  %265 = add nsw i32 %mul.i.i.3, %260
  %266 = sext i32 %265 to i64
  %267 = getelementptr inbounds float, float* %12, i64 %266
  %268 = bitcast float* %267 to <8 x float>*
  %wide.load39.2 = load <8 x float>, <8 x float>* %268, align 4, !tbaa !12, !llvm.access.group !16
  %269 = fsub <8 x float> %wide.load39.2, %wide.load38.2
  %270 = bitcast float* %267 to <8 x float>*
  store <8 x float> %269, <8 x float>* %270, align 4, !tbaa !12, !llvm.access.group !16
  %271 = getelementptr inbounds float, float* %9, i64 %262
  %272 = bitcast float* %271 to <8 x float>*
  %wide.load40.2 = load <8 x float>, <8 x float>* %272, align 4, !tbaa !12, !llvm.access.group !16
  %273 = fmul <8 x float> %broadcast.splat42, %wide.load40.2
  %274 = fdiv <8 x float> %269, %273, !fpmath !19
  %275 = bitcast float* %267 to <8 x float>*
  store <8 x float> %274, <8 x float>* %275, align 4, !tbaa !12, !llvm.access.group !16
  %276 = or i64 %mul.i.i.i, 24
  %277 = trunc i64 %276 to i32
  %278 = shl i64 %276, 32
  %279 = ashr exact i64 %278, 32
  %280 = getelementptr inbounds float, float* %6, i64 %279
  %281 = bitcast float* %280 to <8 x float>*
  %wide.load38.3 = load <8 x float>, <8 x float>* %281, align 4, !tbaa !12, !llvm.access.group !16
  %282 = add nsw i32 %mul.i.i.3, %277
  %283 = sext i32 %282 to i64
  %284 = getelementptr inbounds float, float* %12, i64 %283
  %285 = bitcast float* %284 to <8 x float>*
  %wide.load39.3 = load <8 x float>, <8 x float>* %285, align 4, !tbaa !12, !llvm.access.group !16
  %286 = fsub <8 x float> %wide.load39.3, %wide.load38.3
  %287 = bitcast float* %284 to <8 x float>*
  store <8 x float> %286, <8 x float>* %287, align 4, !tbaa !12, !llvm.access.group !16
  %288 = getelementptr inbounds float, float* %9, i64 %279
  %289 = bitcast float* %288 to <8 x float>*
  %wide.load40.3 = load <8 x float>, <8 x float>* %289, align 4, !tbaa !12, !llvm.access.group !16
  %290 = fmul <8 x float> %broadcast.splat42, %wide.load40.3
  %291 = fdiv <8 x float> %286, %290, !fpmath !19
  %292 = bitcast float* %284 to <8 x float>*
  store <8 x float> %291, <8 x float>* %292, align 4, !tbaa !12, !llvm.access.group !16
  %293 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.4 = or i32 %293, 4
  %mul.i.i.4 = mul nsw i32 %20, %conv2.i.i.4
  %broadcast.splatinsert54 = insertelement <8 x float> undef, float %21, i32 0
  %broadcast.splat55 = shufflevector <8 x float> %broadcast.splatinsert54, <8 x float> undef, <8 x i32> zeroinitializer
  %294 = trunc i64 %mul.i.i.i to i32
  %295 = shl i64 %2, 37
  %296 = ashr exact i64 %295, 32
  %297 = getelementptr inbounds float, float* %6, i64 %296
  %298 = bitcast float* %297 to <8 x float>*
  %wide.load51 = load <8 x float>, <8 x float>* %298, align 4, !tbaa !12, !llvm.access.group !16
  %299 = add nsw i32 %mul.i.i.4, %294
  %300 = sext i32 %299 to i64
  %301 = getelementptr inbounds float, float* %12, i64 %300
  %302 = bitcast float* %301 to <8 x float>*
  %wide.load52 = load <8 x float>, <8 x float>* %302, align 4, !tbaa !12, !llvm.access.group !16
  %303 = fsub <8 x float> %wide.load52, %wide.load51
  %304 = bitcast float* %301 to <8 x float>*
  store <8 x float> %303, <8 x float>* %304, align 4, !tbaa !12, !llvm.access.group !16
  %305 = getelementptr inbounds float, float* %9, i64 %296
  %306 = bitcast float* %305 to <8 x float>*
  %wide.load53 = load <8 x float>, <8 x float>* %306, align 4, !tbaa !12, !llvm.access.group !16
  %307 = fmul <8 x float> %broadcast.splat55, %wide.load53
  %308 = fdiv <8 x float> %303, %307, !fpmath !19
  %309 = bitcast float* %301 to <8 x float>*
  store <8 x float> %308, <8 x float>* %309, align 4, !tbaa !12, !llvm.access.group !16
  %310 = or i64 %mul.i.i.i, 8
  %311 = trunc i64 %310 to i32
  %312 = shl i64 %310, 32
  %313 = ashr exact i64 %312, 32
  %314 = getelementptr inbounds float, float* %6, i64 %313
  %315 = bitcast float* %314 to <8 x float>*
  %wide.load51.1 = load <8 x float>, <8 x float>* %315, align 4, !tbaa !12, !llvm.access.group !16
  %316 = add nsw i32 %mul.i.i.4, %311
  %317 = sext i32 %316 to i64
  %318 = getelementptr inbounds float, float* %12, i64 %317
  %319 = bitcast float* %318 to <8 x float>*
  %wide.load52.1 = load <8 x float>, <8 x float>* %319, align 4, !tbaa !12, !llvm.access.group !16
  %320 = fsub <8 x float> %wide.load52.1, %wide.load51.1
  %321 = bitcast float* %318 to <8 x float>*
  store <8 x float> %320, <8 x float>* %321, align 4, !tbaa !12, !llvm.access.group !16
  %322 = getelementptr inbounds float, float* %9, i64 %313
  %323 = bitcast float* %322 to <8 x float>*
  %wide.load53.1 = load <8 x float>, <8 x float>* %323, align 4, !tbaa !12, !llvm.access.group !16
  %324 = fmul <8 x float> %broadcast.splat55, %wide.load53.1
  %325 = fdiv <8 x float> %320, %324, !fpmath !19
  %326 = bitcast float* %318 to <8 x float>*
  store <8 x float> %325, <8 x float>* %326, align 4, !tbaa !12, !llvm.access.group !16
  %327 = or i64 %mul.i.i.i, 16
  %328 = trunc i64 %327 to i32
  %329 = shl i64 %327, 32
  %330 = ashr exact i64 %329, 32
  %331 = getelementptr inbounds float, float* %6, i64 %330
  %332 = bitcast float* %331 to <8 x float>*
  %wide.load51.2 = load <8 x float>, <8 x float>* %332, align 4, !tbaa !12, !llvm.access.group !16
  %333 = add nsw i32 %mul.i.i.4, %328
  %334 = sext i32 %333 to i64
  %335 = getelementptr inbounds float, float* %12, i64 %334
  %336 = bitcast float* %335 to <8 x float>*
  %wide.load52.2 = load <8 x float>, <8 x float>* %336, align 4, !tbaa !12, !llvm.access.group !16
  %337 = fsub <8 x float> %wide.load52.2, %wide.load51.2
  %338 = bitcast float* %335 to <8 x float>*
  store <8 x float> %337, <8 x float>* %338, align 4, !tbaa !12, !llvm.access.group !16
  %339 = getelementptr inbounds float, float* %9, i64 %330
  %340 = bitcast float* %339 to <8 x float>*
  %wide.load53.2 = load <8 x float>, <8 x float>* %340, align 4, !tbaa !12, !llvm.access.group !16
  %341 = fmul <8 x float> %broadcast.splat55, %wide.load53.2
  %342 = fdiv <8 x float> %337, %341, !fpmath !19
  %343 = bitcast float* %335 to <8 x float>*
  store <8 x float> %342, <8 x float>* %343, align 4, !tbaa !12, !llvm.access.group !16
  %344 = or i64 %mul.i.i.i, 24
  %345 = trunc i64 %344 to i32
  %346 = shl i64 %344, 32
  %347 = ashr exact i64 %346, 32
  %348 = getelementptr inbounds float, float* %6, i64 %347
  %349 = bitcast float* %348 to <8 x float>*
  %wide.load51.3 = load <8 x float>, <8 x float>* %349, align 4, !tbaa !12, !llvm.access.group !16
  %350 = add nsw i32 %mul.i.i.4, %345
  %351 = sext i32 %350 to i64
  %352 = getelementptr inbounds float, float* %12, i64 %351
  %353 = bitcast float* %352 to <8 x float>*
  %wide.load52.3 = load <8 x float>, <8 x float>* %353, align 4, !tbaa !12, !llvm.access.group !16
  %354 = fsub <8 x float> %wide.load52.3, %wide.load51.3
  %355 = bitcast float* %352 to <8 x float>*
  store <8 x float> %354, <8 x float>* %355, align 4, !tbaa !12, !llvm.access.group !16
  %356 = getelementptr inbounds float, float* %9, i64 %347
  %357 = bitcast float* %356 to <8 x float>*
  %wide.load53.3 = load <8 x float>, <8 x float>* %357, align 4, !tbaa !12, !llvm.access.group !16
  %358 = fmul <8 x float> %broadcast.splat55, %wide.load53.3
  %359 = fdiv <8 x float> %354, %358, !fpmath !19
  %360 = bitcast float* %352 to <8 x float>*
  store <8 x float> %359, <8 x float>* %360, align 4, !tbaa !12, !llvm.access.group !16
  %361 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.5 = or i32 %361, 5
  %mul.i.i.5 = mul nsw i32 %20, %conv2.i.i.5
  %broadcast.splatinsert67 = insertelement <8 x float> undef, float %21, i32 0
  %broadcast.splat68 = shufflevector <8 x float> %broadcast.splatinsert67, <8 x float> undef, <8 x i32> zeroinitializer
  %362 = trunc i64 %mul.i.i.i to i32
  %363 = shl i64 %2, 37
  %364 = ashr exact i64 %363, 32
  %365 = getelementptr inbounds float, float* %6, i64 %364
  %366 = bitcast float* %365 to <8 x float>*
  %wide.load64 = load <8 x float>, <8 x float>* %366, align 4, !tbaa !12, !llvm.access.group !16
  %367 = add nsw i32 %mul.i.i.5, %362
  %368 = sext i32 %367 to i64
  %369 = getelementptr inbounds float, float* %12, i64 %368
  %370 = bitcast float* %369 to <8 x float>*
  %wide.load65 = load <8 x float>, <8 x float>* %370, align 4, !tbaa !12, !llvm.access.group !16
  %371 = fsub <8 x float> %wide.load65, %wide.load64
  %372 = bitcast float* %369 to <8 x float>*
  store <8 x float> %371, <8 x float>* %372, align 4, !tbaa !12, !llvm.access.group !16
  %373 = getelementptr inbounds float, float* %9, i64 %364
  %374 = bitcast float* %373 to <8 x float>*
  %wide.load66 = load <8 x float>, <8 x float>* %374, align 4, !tbaa !12, !llvm.access.group !16
  %375 = fmul <8 x float> %broadcast.splat68, %wide.load66
  %376 = fdiv <8 x float> %371, %375, !fpmath !19
  %377 = bitcast float* %369 to <8 x float>*
  store <8 x float> %376, <8 x float>* %377, align 4, !tbaa !12, !llvm.access.group !16
  %378 = or i64 %mul.i.i.i, 8
  %379 = trunc i64 %378 to i32
  %380 = shl i64 %378, 32
  %381 = ashr exact i64 %380, 32
  %382 = getelementptr inbounds float, float* %6, i64 %381
  %383 = bitcast float* %382 to <8 x float>*
  %wide.load64.1 = load <8 x float>, <8 x float>* %383, align 4, !tbaa !12, !llvm.access.group !16
  %384 = add nsw i32 %mul.i.i.5, %379
  %385 = sext i32 %384 to i64
  %386 = getelementptr inbounds float, float* %12, i64 %385
  %387 = bitcast float* %386 to <8 x float>*
  %wide.load65.1 = load <8 x float>, <8 x float>* %387, align 4, !tbaa !12, !llvm.access.group !16
  %388 = fsub <8 x float> %wide.load65.1, %wide.load64.1
  %389 = bitcast float* %386 to <8 x float>*
  store <8 x float> %388, <8 x float>* %389, align 4, !tbaa !12, !llvm.access.group !16
  %390 = getelementptr inbounds float, float* %9, i64 %381
  %391 = bitcast float* %390 to <8 x float>*
  %wide.load66.1 = load <8 x float>, <8 x float>* %391, align 4, !tbaa !12, !llvm.access.group !16
  %392 = fmul <8 x float> %broadcast.splat68, %wide.load66.1
  %393 = fdiv <8 x float> %388, %392, !fpmath !19
  %394 = bitcast float* %386 to <8 x float>*
  store <8 x float> %393, <8 x float>* %394, align 4, !tbaa !12, !llvm.access.group !16
  %395 = or i64 %mul.i.i.i, 16
  %396 = trunc i64 %395 to i32
  %397 = shl i64 %395, 32
  %398 = ashr exact i64 %397, 32
  %399 = getelementptr inbounds float, float* %6, i64 %398
  %400 = bitcast float* %399 to <8 x float>*
  %wide.load64.2 = load <8 x float>, <8 x float>* %400, align 4, !tbaa !12, !llvm.access.group !16
  %401 = add nsw i32 %mul.i.i.5, %396
  %402 = sext i32 %401 to i64
  %403 = getelementptr inbounds float, float* %12, i64 %402
  %404 = bitcast float* %403 to <8 x float>*
  %wide.load65.2 = load <8 x float>, <8 x float>* %404, align 4, !tbaa !12, !llvm.access.group !16
  %405 = fsub <8 x float> %wide.load65.2, %wide.load64.2
  %406 = bitcast float* %403 to <8 x float>*
  store <8 x float> %405, <8 x float>* %406, align 4, !tbaa !12, !llvm.access.group !16
  %407 = getelementptr inbounds float, float* %9, i64 %398
  %408 = bitcast float* %407 to <8 x float>*
  %wide.load66.2 = load <8 x float>, <8 x float>* %408, align 4, !tbaa !12, !llvm.access.group !16
  %409 = fmul <8 x float> %broadcast.splat68, %wide.load66.2
  %410 = fdiv <8 x float> %405, %409, !fpmath !19
  %411 = bitcast float* %403 to <8 x float>*
  store <8 x float> %410, <8 x float>* %411, align 4, !tbaa !12, !llvm.access.group !16
  %412 = or i64 %mul.i.i.i, 24
  %413 = trunc i64 %412 to i32
  %414 = shl i64 %412, 32
  %415 = ashr exact i64 %414, 32
  %416 = getelementptr inbounds float, float* %6, i64 %415
  %417 = bitcast float* %416 to <8 x float>*
  %wide.load64.3 = load <8 x float>, <8 x float>* %417, align 4, !tbaa !12, !llvm.access.group !16
  %418 = add nsw i32 %mul.i.i.5, %413
  %419 = sext i32 %418 to i64
  %420 = getelementptr inbounds float, float* %12, i64 %419
  %421 = bitcast float* %420 to <8 x float>*
  %wide.load65.3 = load <8 x float>, <8 x float>* %421, align 4, !tbaa !12, !llvm.access.group !16
  %422 = fsub <8 x float> %wide.load65.3, %wide.load64.3
  %423 = bitcast float* %420 to <8 x float>*
  store <8 x float> %422, <8 x float>* %423, align 4, !tbaa !12, !llvm.access.group !16
  %424 = getelementptr inbounds float, float* %9, i64 %415
  %425 = bitcast float* %424 to <8 x float>*
  %wide.load66.3 = load <8 x float>, <8 x float>* %425, align 4, !tbaa !12, !llvm.access.group !16
  %426 = fmul <8 x float> %broadcast.splat68, %wide.load66.3
  %427 = fdiv <8 x float> %422, %426, !fpmath !19
  %428 = bitcast float* %420 to <8 x float>*
  store <8 x float> %427, <8 x float>* %428, align 4, !tbaa !12, !llvm.access.group !16
  %429 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.6 = or i32 %429, 6
  %mul.i.i.6 = mul nsw i32 %20, %conv2.i.i.6
  %broadcast.splatinsert80 = insertelement <8 x float> undef, float %21, i32 0
  %broadcast.splat81 = shufflevector <8 x float> %broadcast.splatinsert80, <8 x float> undef, <8 x i32> zeroinitializer
  %430 = trunc i64 %mul.i.i.i to i32
  %431 = shl i64 %2, 37
  %432 = ashr exact i64 %431, 32
  %433 = getelementptr inbounds float, float* %6, i64 %432
  %434 = bitcast float* %433 to <8 x float>*
  %wide.load77 = load <8 x float>, <8 x float>* %434, align 4, !tbaa !12, !llvm.access.group !16
  %435 = add nsw i32 %mul.i.i.6, %430
  %436 = sext i32 %435 to i64
  %437 = getelementptr inbounds float, float* %12, i64 %436
  %438 = bitcast float* %437 to <8 x float>*
  %wide.load78 = load <8 x float>, <8 x float>* %438, align 4, !tbaa !12, !llvm.access.group !16
  %439 = fsub <8 x float> %wide.load78, %wide.load77
  %440 = bitcast float* %437 to <8 x float>*
  store <8 x float> %439, <8 x float>* %440, align 4, !tbaa !12, !llvm.access.group !16
  %441 = getelementptr inbounds float, float* %9, i64 %432
  %442 = bitcast float* %441 to <8 x float>*
  %wide.load79 = load <8 x float>, <8 x float>* %442, align 4, !tbaa !12, !llvm.access.group !16
  %443 = fmul <8 x float> %broadcast.splat81, %wide.load79
  %444 = fdiv <8 x float> %439, %443, !fpmath !19
  %445 = bitcast float* %437 to <8 x float>*
  store <8 x float> %444, <8 x float>* %445, align 4, !tbaa !12, !llvm.access.group !16
  %446 = or i64 %mul.i.i.i, 8
  %447 = trunc i64 %446 to i32
  %448 = shl i64 %446, 32
  %449 = ashr exact i64 %448, 32
  %450 = getelementptr inbounds float, float* %6, i64 %449
  %451 = bitcast float* %450 to <8 x float>*
  %wide.load77.1 = load <8 x float>, <8 x float>* %451, align 4, !tbaa !12, !llvm.access.group !16
  %452 = add nsw i32 %mul.i.i.6, %447
  %453 = sext i32 %452 to i64
  %454 = getelementptr inbounds float, float* %12, i64 %453
  %455 = bitcast float* %454 to <8 x float>*
  %wide.load78.1 = load <8 x float>, <8 x float>* %455, align 4, !tbaa !12, !llvm.access.group !16
  %456 = fsub <8 x float> %wide.load78.1, %wide.load77.1
  %457 = bitcast float* %454 to <8 x float>*
  store <8 x float> %456, <8 x float>* %457, align 4, !tbaa !12, !llvm.access.group !16
  %458 = getelementptr inbounds float, float* %9, i64 %449
  %459 = bitcast float* %458 to <8 x float>*
  %wide.load79.1 = load <8 x float>, <8 x float>* %459, align 4, !tbaa !12, !llvm.access.group !16
  %460 = fmul <8 x float> %broadcast.splat81, %wide.load79.1
  %461 = fdiv <8 x float> %456, %460, !fpmath !19
  %462 = bitcast float* %454 to <8 x float>*
  store <8 x float> %461, <8 x float>* %462, align 4, !tbaa !12, !llvm.access.group !16
  %463 = or i64 %mul.i.i.i, 16
  %464 = trunc i64 %463 to i32
  %465 = shl i64 %463, 32
  %466 = ashr exact i64 %465, 32
  %467 = getelementptr inbounds float, float* %6, i64 %466
  %468 = bitcast float* %467 to <8 x float>*
  %wide.load77.2 = load <8 x float>, <8 x float>* %468, align 4, !tbaa !12, !llvm.access.group !16
  %469 = add nsw i32 %mul.i.i.6, %464
  %470 = sext i32 %469 to i64
  %471 = getelementptr inbounds float, float* %12, i64 %470
  %472 = bitcast float* %471 to <8 x float>*
  %wide.load78.2 = load <8 x float>, <8 x float>* %472, align 4, !tbaa !12, !llvm.access.group !16
  %473 = fsub <8 x float> %wide.load78.2, %wide.load77.2
  %474 = bitcast float* %471 to <8 x float>*
  store <8 x float> %473, <8 x float>* %474, align 4, !tbaa !12, !llvm.access.group !16
  %475 = getelementptr inbounds float, float* %9, i64 %466
  %476 = bitcast float* %475 to <8 x float>*
  %wide.load79.2 = load <8 x float>, <8 x float>* %476, align 4, !tbaa !12, !llvm.access.group !16
  %477 = fmul <8 x float> %broadcast.splat81, %wide.load79.2
  %478 = fdiv <8 x float> %473, %477, !fpmath !19
  %479 = bitcast float* %471 to <8 x float>*
  store <8 x float> %478, <8 x float>* %479, align 4, !tbaa !12, !llvm.access.group !16
  %480 = or i64 %mul.i.i.i, 24
  %481 = trunc i64 %480 to i32
  %482 = shl i64 %480, 32
  %483 = ashr exact i64 %482, 32
  %484 = getelementptr inbounds float, float* %6, i64 %483
  %485 = bitcast float* %484 to <8 x float>*
  %wide.load77.3 = load <8 x float>, <8 x float>* %485, align 4, !tbaa !12, !llvm.access.group !16
  %486 = add nsw i32 %mul.i.i.6, %481
  %487 = sext i32 %486 to i64
  %488 = getelementptr inbounds float, float* %12, i64 %487
  %489 = bitcast float* %488 to <8 x float>*
  %wide.load78.3 = load <8 x float>, <8 x float>* %489, align 4, !tbaa !12, !llvm.access.group !16
  %490 = fsub <8 x float> %wide.load78.3, %wide.load77.3
  %491 = bitcast float* %488 to <8 x float>*
  store <8 x float> %490, <8 x float>* %491, align 4, !tbaa !12, !llvm.access.group !16
  %492 = getelementptr inbounds float, float* %9, i64 %483
  %493 = bitcast float* %492 to <8 x float>*
  %wide.load79.3 = load <8 x float>, <8 x float>* %493, align 4, !tbaa !12, !llvm.access.group !16
  %494 = fmul <8 x float> %broadcast.splat81, %wide.load79.3
  %495 = fdiv <8 x float> %490, %494, !fpmath !19
  %496 = bitcast float* %488 to <8 x float>*
  store <8 x float> %495, <8 x float>* %496, align 4, !tbaa !12, !llvm.access.group !16
  %497 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.7 = or i32 %497, 7
  %mul.i.i.7 = mul nsw i32 %20, %conv2.i.i.7
  %broadcast.splatinsert93 = insertelement <8 x float> undef, float %21, i32 0
  %broadcast.splat94 = shufflevector <8 x float> %broadcast.splatinsert93, <8 x float> undef, <8 x i32> zeroinitializer
  %498 = trunc i64 %mul.i.i.i to i32
  %499 = shl i64 %2, 37
  %500 = ashr exact i64 %499, 32
  %501 = getelementptr inbounds float, float* %6, i64 %500
  %502 = bitcast float* %501 to <8 x float>*
  %wide.load90 = load <8 x float>, <8 x float>* %502, align 4, !tbaa !12, !llvm.access.group !16
  %503 = add nsw i32 %mul.i.i.7, %498
  %504 = sext i32 %503 to i64
  %505 = getelementptr inbounds float, float* %12, i64 %504
  %506 = bitcast float* %505 to <8 x float>*
  %wide.load91 = load <8 x float>, <8 x float>* %506, align 4, !tbaa !12, !llvm.access.group !16
  %507 = fsub <8 x float> %wide.load91, %wide.load90
  %508 = bitcast float* %505 to <8 x float>*
  store <8 x float> %507, <8 x float>* %508, align 4, !tbaa !12, !llvm.access.group !16
  %509 = getelementptr inbounds float, float* %9, i64 %500
  %510 = bitcast float* %509 to <8 x float>*
  %wide.load92 = load <8 x float>, <8 x float>* %510, align 4, !tbaa !12, !llvm.access.group !16
  %511 = fmul <8 x float> %broadcast.splat94, %wide.load92
  %512 = fdiv <8 x float> %507, %511, !fpmath !19
  %513 = bitcast float* %505 to <8 x float>*
  store <8 x float> %512, <8 x float>* %513, align 4, !tbaa !12, !llvm.access.group !16
  %514 = or i64 %mul.i.i.i, 8
  %515 = trunc i64 %514 to i32
  %516 = shl i64 %514, 32
  %517 = ashr exact i64 %516, 32
  %518 = getelementptr inbounds float, float* %6, i64 %517
  %519 = bitcast float* %518 to <8 x float>*
  %wide.load90.1 = load <8 x float>, <8 x float>* %519, align 4, !tbaa !12, !llvm.access.group !16
  %520 = add nsw i32 %mul.i.i.7, %515
  %521 = sext i32 %520 to i64
  %522 = getelementptr inbounds float, float* %12, i64 %521
  %523 = bitcast float* %522 to <8 x float>*
  %wide.load91.1 = load <8 x float>, <8 x float>* %523, align 4, !tbaa !12, !llvm.access.group !16
  %524 = fsub <8 x float> %wide.load91.1, %wide.load90.1
  %525 = bitcast float* %522 to <8 x float>*
  store <8 x float> %524, <8 x float>* %525, align 4, !tbaa !12, !llvm.access.group !16
  %526 = getelementptr inbounds float, float* %9, i64 %517
  %527 = bitcast float* %526 to <8 x float>*
  %wide.load92.1 = load <8 x float>, <8 x float>* %527, align 4, !tbaa !12, !llvm.access.group !16
  %528 = fmul <8 x float> %broadcast.splat94, %wide.load92.1
  %529 = fdiv <8 x float> %524, %528, !fpmath !19
  %530 = bitcast float* %522 to <8 x float>*
  store <8 x float> %529, <8 x float>* %530, align 4, !tbaa !12, !llvm.access.group !16
  %531 = or i64 %mul.i.i.i, 16
  %532 = trunc i64 %531 to i32
  %533 = shl i64 %531, 32
  %534 = ashr exact i64 %533, 32
  %535 = getelementptr inbounds float, float* %6, i64 %534
  %536 = bitcast float* %535 to <8 x float>*
  %wide.load90.2 = load <8 x float>, <8 x float>* %536, align 4, !tbaa !12, !llvm.access.group !16
  %537 = add nsw i32 %mul.i.i.7, %532
  %538 = sext i32 %537 to i64
  %539 = getelementptr inbounds float, float* %12, i64 %538
  %540 = bitcast float* %539 to <8 x float>*
  %wide.load91.2 = load <8 x float>, <8 x float>* %540, align 4, !tbaa !12, !llvm.access.group !16
  %541 = fsub <8 x float> %wide.load91.2, %wide.load90.2
  %542 = bitcast float* %539 to <8 x float>*
  store <8 x float> %541, <8 x float>* %542, align 4, !tbaa !12, !llvm.access.group !16
  %543 = getelementptr inbounds float, float* %9, i64 %534
  %544 = bitcast float* %543 to <8 x float>*
  %wide.load92.2 = load <8 x float>, <8 x float>* %544, align 4, !tbaa !12, !llvm.access.group !16
  %545 = fmul <8 x float> %broadcast.splat94, %wide.load92.2
  %546 = fdiv <8 x float> %541, %545, !fpmath !19
  %547 = bitcast float* %539 to <8 x float>*
  store <8 x float> %546, <8 x float>* %547, align 4, !tbaa !12, !llvm.access.group !16
  %548 = or i64 %mul.i.i.i, 24
  %549 = trunc i64 %548 to i32
  %550 = shl i64 %548, 32
  %551 = ashr exact i64 %550, 32
  %552 = getelementptr inbounds float, float* %6, i64 %551
  %553 = bitcast float* %552 to <8 x float>*
  %wide.load90.3 = load <8 x float>, <8 x float>* %553, align 4, !tbaa !12, !llvm.access.group !16
  %554 = add nsw i32 %mul.i.i.7, %549
  %555 = sext i32 %554 to i64
  %556 = getelementptr inbounds float, float* %12, i64 %555
  %557 = bitcast float* %556 to <8 x float>*
  %wide.load91.3 = load <8 x float>, <8 x float>* %557, align 4, !tbaa !12, !llvm.access.group !16
  %558 = fsub <8 x float> %wide.load91.3, %wide.load90.3
  %559 = bitcast float* %556 to <8 x float>*
  store <8 x float> %558, <8 x float>* %559, align 4, !tbaa !12, !llvm.access.group !16
  %560 = getelementptr inbounds float, float* %9, i64 %551
  %561 = bitcast float* %560 to <8 x float>*
  %wide.load92.3 = load <8 x float>, <8 x float>* %561, align 4, !tbaa !12, !llvm.access.group !16
  %562 = fmul <8 x float> %broadcast.splat94, %wide.load92.3
  %563 = fdiv <8 x float> %558, %562, !fpmath !19
  %564 = bitcast float* %556 to <8 x float>*
  store <8 x float> %563, <8 x float>* %564, align 4, !tbaa !12, !llvm.access.group !16
  ret void
}

attributes #0 = { nounwind readnone speculatable willreturn }
attributes #1 = { alwaysinline nofree norecurse nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "stackrealign" "target-cpu"="skylake" "target-features"="+adx,+aes,+avx,+avx2,+bmi,+bmi2,+clflushopt,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+prfchw,+rdrnd,+rdseed,+sahf,+sgx,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "uniform-work-group-size"="true" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { nofree nounwind }
attributes #3 = { nounwind }

!llvm.module.flags = !{!0, !1, !2}
!opencl.ocl.version = !{!3}
!llvm.ident = !{!4}
!opencl.spir.version = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 1, i32 2}
!4 = !{!"clang version 11.0.0 (git@github.com:llvm/llvm-project.git 91e89f9a5115b0f83b8f026e1ad0e6d1f885fa9b)"}
!5 = !{i32 1, i32 1, i32 1, i32 0, i32 0, i32 0}
!6 = !{!"none", !"none", !"none", !"none", !"none", !"none"}
!7 = !{!"DATA_TYPE*", !"DATA_TYPE*", !"DATA_TYPE*", !"DATA_TYPE", !"int", !"int"}
!8 = !{!"float*", !"float*", !"float*", !"float", !"int", !"int"}
!9 = !{!"", !"", !"", !"", !"", !""}
!10 = !{!"mean", !"std", !"data", !"float_n", !"m", !"n"}
!11 = !{i32 1}
!12 = !{!13, !13, i64 0}
!13 = !{!"float", !14, i64 0}
!14 = !{!"omnipotent char", !15, i64 0}
!15 = !{!"Simple C/C++ TBAA"}
!16 = !{!17, !18}
!17 = distinct !{}
!18 = distinct !{}
!19 = !{float 2.500000e+00}
