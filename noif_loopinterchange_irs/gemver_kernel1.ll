; ModuleID = './DH/KACFBKFCFLBKAEPLDFEDCFHLMJKEDPIJDPDND/gemver_kernel1/32-8-1-goffs0-smallgrid/parallel.bc'
source_filename = "parallel_bc"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: alwaysinline nofree norecurse nounwind
define void @_pocl_kernel_gemver_kernel1(float* nocapture %0, float* nocapture readonly %1, float* nocapture readonly %2, float* nocapture readonly %3, float* nocapture readonly %4, i32 %5, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %6, i64 %7, i64 %8, i64 %9) local_unnamed_addr #0 !kernel_arg_addr_space !5 !kernel_arg_access_qual !6 !kernel_arg_type !7 !kernel_arg_base_type !8 !kernel_arg_type_qual !9 !kernel_arg_name !10 !pocl_generated !11 {
pregion_for_entry.pregion_for_init.i:
  %mul.i.i = shl i64 %7, 5
  %mul3.i.i = shl i64 %8, 3
  %conv2.i = trunc i64 %mul3.i.i to i32
  %sext.i = shl i64 %8, 35
  %idxprom.i = ashr exact i64 %sext.i, 32
  %arrayidx.i = getelementptr inbounds float, float* %3, i64 %idxprom.i
  %arrayidx6.i = getelementptr inbounds float, float* %4, i64 %idxprom.i
  %mul.i = mul nsw i32 %conv2.i, %5
  %10 = trunc i64 %mul.i.i to i32
  %11 = load float, float* %arrayidx.i, align 4, !tbaa !12, !llvm.access.group !16
  %12 = insertelement <8 x float> undef, float %11, i32 0
  %13 = shufflevector <8 x float> %12, <8 x float> undef, <8 x i32> zeroinitializer
  %14 = shl i64 %7, 37
  %15 = ashr exact i64 %14, 32
  %16 = getelementptr inbounds float, float* %1, i64 %15
  %17 = bitcast float* %16 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %17, align 4, !tbaa !12, !llvm.access.group !16
  %18 = load float, float* %arrayidx6.i, align 4, !tbaa !12, !llvm.access.group !16
  %19 = insertelement <8 x float> undef, float %18, i32 0
  %20 = shufflevector <8 x float> %19, <8 x float> undef, <8 x i32> zeroinitializer
  %21 = getelementptr inbounds float, float* %2, i64 %15
  %22 = bitcast float* %21 to <8 x float>*
  %wide.load2 = load <8 x float>, <8 x float>* %22, align 4, !tbaa !12, !llvm.access.group !16
  %23 = fmul <8 x float> %20, %wide.load2
  %24 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %13, <8 x float> %wide.load, <8 x float> %23)
  %25 = add nsw i32 %mul.i, %10
  %26 = sext i32 %25 to i64
  %27 = getelementptr inbounds float, float* %0, i64 %26
  %28 = bitcast float* %27 to <8 x float>*
  %wide.load3 = load <8 x float>, <8 x float>* %28, align 4, !tbaa !12, !llvm.access.group !16
  %29 = fadd <8 x float> %wide.load3, %24
  %30 = bitcast float* %27 to <8 x float>*
  store <8 x float> %29, <8 x float>* %30, align 4, !tbaa !12, !llvm.access.group !16
  %31 = or i64 %mul.i.i, 8
  %32 = trunc i64 %31 to i32
  %33 = load float, float* %arrayidx.i, align 4, !tbaa !12, !llvm.access.group !16
  %34 = insertelement <8 x float> undef, float %33, i32 0
  %35 = shufflevector <8 x float> %34, <8 x float> undef, <8 x i32> zeroinitializer
  %36 = shl i64 %31, 32
  %37 = ashr exact i64 %36, 32
  %38 = getelementptr inbounds float, float* %1, i64 %37
  %39 = bitcast float* %38 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %39, align 4, !tbaa !12, !llvm.access.group !16
  %40 = load float, float* %arrayidx6.i, align 4, !tbaa !12, !llvm.access.group !16
  %41 = insertelement <8 x float> undef, float %40, i32 0
  %42 = shufflevector <8 x float> %41, <8 x float> undef, <8 x i32> zeroinitializer
  %43 = getelementptr inbounds float, float* %2, i64 %37
  %44 = bitcast float* %43 to <8 x float>*
  %wide.load2.1 = load <8 x float>, <8 x float>* %44, align 4, !tbaa !12, !llvm.access.group !16
  %45 = fmul <8 x float> %42, %wide.load2.1
  %46 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %35, <8 x float> %wide.load.1, <8 x float> %45)
  %47 = add nsw i32 %mul.i, %32
  %48 = sext i32 %47 to i64
  %49 = getelementptr inbounds float, float* %0, i64 %48
  %50 = bitcast float* %49 to <8 x float>*
  %wide.load3.1 = load <8 x float>, <8 x float>* %50, align 4, !tbaa !12, !llvm.access.group !16
  %51 = fadd <8 x float> %wide.load3.1, %46
  %52 = bitcast float* %49 to <8 x float>*
  store <8 x float> %51, <8 x float>* %52, align 4, !tbaa !12, !llvm.access.group !16
  %53 = or i64 %mul.i.i, 16
  %54 = trunc i64 %53 to i32
  %55 = load float, float* %arrayidx.i, align 4, !tbaa !12, !llvm.access.group !16
  %56 = insertelement <8 x float> undef, float %55, i32 0
  %57 = shufflevector <8 x float> %56, <8 x float> undef, <8 x i32> zeroinitializer
  %58 = shl i64 %53, 32
  %59 = ashr exact i64 %58, 32
  %60 = getelementptr inbounds float, float* %1, i64 %59
  %61 = bitcast float* %60 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %61, align 4, !tbaa !12, !llvm.access.group !16
  %62 = load float, float* %arrayidx6.i, align 4, !tbaa !12, !llvm.access.group !16
  %63 = insertelement <8 x float> undef, float %62, i32 0
  %64 = shufflevector <8 x float> %63, <8 x float> undef, <8 x i32> zeroinitializer
  %65 = getelementptr inbounds float, float* %2, i64 %59
  %66 = bitcast float* %65 to <8 x float>*
  %wide.load2.2 = load <8 x float>, <8 x float>* %66, align 4, !tbaa !12, !llvm.access.group !16
  %67 = fmul <8 x float> %64, %wide.load2.2
  %68 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %57, <8 x float> %wide.load.2, <8 x float> %67)
  %69 = add nsw i32 %mul.i, %54
  %70 = sext i32 %69 to i64
  %71 = getelementptr inbounds float, float* %0, i64 %70
  %72 = bitcast float* %71 to <8 x float>*
  %wide.load3.2 = load <8 x float>, <8 x float>* %72, align 4, !tbaa !12, !llvm.access.group !16
  %73 = fadd <8 x float> %wide.load3.2, %68
  %74 = bitcast float* %71 to <8 x float>*
  store <8 x float> %73, <8 x float>* %74, align 4, !tbaa !12, !llvm.access.group !16
  %75 = or i64 %mul.i.i, 24
  %76 = trunc i64 %75 to i32
  %77 = load float, float* %arrayidx.i, align 4, !tbaa !12, !llvm.access.group !16
  %78 = insertelement <8 x float> undef, float %77, i32 0
  %79 = shufflevector <8 x float> %78, <8 x float> undef, <8 x i32> zeroinitializer
  %80 = shl i64 %75, 32
  %81 = ashr exact i64 %80, 32
  %82 = getelementptr inbounds float, float* %1, i64 %81
  %83 = bitcast float* %82 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %83, align 4, !tbaa !12, !llvm.access.group !16
  %84 = load float, float* %arrayidx6.i, align 4, !tbaa !12, !llvm.access.group !16
  %85 = insertelement <8 x float> undef, float %84, i32 0
  %86 = shufflevector <8 x float> %85, <8 x float> undef, <8 x i32> zeroinitializer
  %87 = getelementptr inbounds float, float* %2, i64 %81
  %88 = bitcast float* %87 to <8 x float>*
  %wide.load2.3 = load <8 x float>, <8 x float>* %88, align 4, !tbaa !12, !llvm.access.group !16
  %89 = fmul <8 x float> %86, %wide.load2.3
  %90 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %79, <8 x float> %wide.load.3, <8 x float> %89)
  %91 = add nsw i32 %mul.i, %76
  %92 = sext i32 %91 to i64
  %93 = getelementptr inbounds float, float* %0, i64 %92
  %94 = bitcast float* %93 to <8 x float>*
  %wide.load3.3 = load <8 x float>, <8 x float>* %94, align 4, !tbaa !12, !llvm.access.group !16
  %95 = fadd <8 x float> %wide.load3.3, %90
  %96 = bitcast float* %93 to <8 x float>*
  store <8 x float> %95, <8 x float>* %96, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.1 = or i64 %mul3.i.i, 1
  %conv2.i.1 = trunc i64 %add6.i.i.1 to i32
  %sext.i.1 = shl i64 %add6.i.i.1, 32
  %idxprom.i.1 = ashr exact i64 %sext.i.1, 32
  %arrayidx.i.1 = getelementptr inbounds float, float* %3, i64 %idxprom.i.1
  %arrayidx6.i.1 = getelementptr inbounds float, float* %4, i64 %idxprom.i.1
  %mul.i.1 = mul nsw i32 %conv2.i.1, %5
  %97 = trunc i64 %mul.i.i to i32
  %98 = load float, float* %arrayidx.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %99 = insertelement <8 x float> undef, float %98, i32 0
  %100 = shufflevector <8 x float> %99, <8 x float> undef, <8 x i32> zeroinitializer
  %101 = shl i64 %7, 37
  %102 = ashr exact i64 %101, 32
  %103 = getelementptr inbounds float, float* %1, i64 %102
  %104 = bitcast float* %103 to <8 x float>*
  %wide.load12 = load <8 x float>, <8 x float>* %104, align 4, !tbaa !12, !llvm.access.group !16
  %105 = load float, float* %arrayidx6.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %106 = insertelement <8 x float> undef, float %105, i32 0
  %107 = shufflevector <8 x float> %106, <8 x float> undef, <8 x i32> zeroinitializer
  %108 = getelementptr inbounds float, float* %2, i64 %102
  %109 = bitcast float* %108 to <8 x float>*
  %wide.load13 = load <8 x float>, <8 x float>* %109, align 4, !tbaa !12, !llvm.access.group !16
  %110 = fmul <8 x float> %107, %wide.load13
  %111 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %100, <8 x float> %wide.load12, <8 x float> %110)
  %112 = add nsw i32 %mul.i.1, %97
  %113 = sext i32 %112 to i64
  %114 = getelementptr inbounds float, float* %0, i64 %113
  %115 = bitcast float* %114 to <8 x float>*
  %wide.load14 = load <8 x float>, <8 x float>* %115, align 4, !tbaa !12, !llvm.access.group !16
  %116 = fadd <8 x float> %wide.load14, %111
  %117 = bitcast float* %114 to <8 x float>*
  store <8 x float> %116, <8 x float>* %117, align 4, !tbaa !12, !llvm.access.group !16
  %118 = or i64 %mul.i.i, 8
  %119 = trunc i64 %118 to i32
  %120 = load float, float* %arrayidx.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %121 = insertelement <8 x float> undef, float %120, i32 0
  %122 = shufflevector <8 x float> %121, <8 x float> undef, <8 x i32> zeroinitializer
  %123 = shl i64 %118, 32
  %124 = ashr exact i64 %123, 32
  %125 = getelementptr inbounds float, float* %1, i64 %124
  %126 = bitcast float* %125 to <8 x float>*
  %wide.load12.1 = load <8 x float>, <8 x float>* %126, align 4, !tbaa !12, !llvm.access.group !16
  %127 = load float, float* %arrayidx6.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %128 = insertelement <8 x float> undef, float %127, i32 0
  %129 = shufflevector <8 x float> %128, <8 x float> undef, <8 x i32> zeroinitializer
  %130 = getelementptr inbounds float, float* %2, i64 %124
  %131 = bitcast float* %130 to <8 x float>*
  %wide.load13.1 = load <8 x float>, <8 x float>* %131, align 4, !tbaa !12, !llvm.access.group !16
  %132 = fmul <8 x float> %129, %wide.load13.1
  %133 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %122, <8 x float> %wide.load12.1, <8 x float> %132)
  %134 = add nsw i32 %mul.i.1, %119
  %135 = sext i32 %134 to i64
  %136 = getelementptr inbounds float, float* %0, i64 %135
  %137 = bitcast float* %136 to <8 x float>*
  %wide.load14.1 = load <8 x float>, <8 x float>* %137, align 4, !tbaa !12, !llvm.access.group !16
  %138 = fadd <8 x float> %wide.load14.1, %133
  %139 = bitcast float* %136 to <8 x float>*
  store <8 x float> %138, <8 x float>* %139, align 4, !tbaa !12, !llvm.access.group !16
  %140 = or i64 %mul.i.i, 16
  %141 = trunc i64 %140 to i32
  %142 = load float, float* %arrayidx.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %143 = insertelement <8 x float> undef, float %142, i32 0
  %144 = shufflevector <8 x float> %143, <8 x float> undef, <8 x i32> zeroinitializer
  %145 = shl i64 %140, 32
  %146 = ashr exact i64 %145, 32
  %147 = getelementptr inbounds float, float* %1, i64 %146
  %148 = bitcast float* %147 to <8 x float>*
  %wide.load12.2 = load <8 x float>, <8 x float>* %148, align 4, !tbaa !12, !llvm.access.group !16
  %149 = load float, float* %arrayidx6.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %150 = insertelement <8 x float> undef, float %149, i32 0
  %151 = shufflevector <8 x float> %150, <8 x float> undef, <8 x i32> zeroinitializer
  %152 = getelementptr inbounds float, float* %2, i64 %146
  %153 = bitcast float* %152 to <8 x float>*
  %wide.load13.2 = load <8 x float>, <8 x float>* %153, align 4, !tbaa !12, !llvm.access.group !16
  %154 = fmul <8 x float> %151, %wide.load13.2
  %155 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %144, <8 x float> %wide.load12.2, <8 x float> %154)
  %156 = add nsw i32 %mul.i.1, %141
  %157 = sext i32 %156 to i64
  %158 = getelementptr inbounds float, float* %0, i64 %157
  %159 = bitcast float* %158 to <8 x float>*
  %wide.load14.2 = load <8 x float>, <8 x float>* %159, align 4, !tbaa !12, !llvm.access.group !16
  %160 = fadd <8 x float> %wide.load14.2, %155
  %161 = bitcast float* %158 to <8 x float>*
  store <8 x float> %160, <8 x float>* %161, align 4, !tbaa !12, !llvm.access.group !16
  %162 = or i64 %mul.i.i, 24
  %163 = trunc i64 %162 to i32
  %164 = load float, float* %arrayidx.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %165 = insertelement <8 x float> undef, float %164, i32 0
  %166 = shufflevector <8 x float> %165, <8 x float> undef, <8 x i32> zeroinitializer
  %167 = shl i64 %162, 32
  %168 = ashr exact i64 %167, 32
  %169 = getelementptr inbounds float, float* %1, i64 %168
  %170 = bitcast float* %169 to <8 x float>*
  %wide.load12.3 = load <8 x float>, <8 x float>* %170, align 4, !tbaa !12, !llvm.access.group !16
  %171 = load float, float* %arrayidx6.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %172 = insertelement <8 x float> undef, float %171, i32 0
  %173 = shufflevector <8 x float> %172, <8 x float> undef, <8 x i32> zeroinitializer
  %174 = getelementptr inbounds float, float* %2, i64 %168
  %175 = bitcast float* %174 to <8 x float>*
  %wide.load13.3 = load <8 x float>, <8 x float>* %175, align 4, !tbaa !12, !llvm.access.group !16
  %176 = fmul <8 x float> %173, %wide.load13.3
  %177 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %166, <8 x float> %wide.load12.3, <8 x float> %176)
  %178 = add nsw i32 %mul.i.1, %163
  %179 = sext i32 %178 to i64
  %180 = getelementptr inbounds float, float* %0, i64 %179
  %181 = bitcast float* %180 to <8 x float>*
  %wide.load14.3 = load <8 x float>, <8 x float>* %181, align 4, !tbaa !12, !llvm.access.group !16
  %182 = fadd <8 x float> %wide.load14.3, %177
  %183 = bitcast float* %180 to <8 x float>*
  store <8 x float> %182, <8 x float>* %183, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.2 = or i64 %mul3.i.i, 2
  %conv2.i.2 = trunc i64 %add6.i.i.2 to i32
  %sext.i.2 = shl i64 %add6.i.i.2, 32
  %idxprom.i.2 = ashr exact i64 %sext.i.2, 32
  %arrayidx.i.2 = getelementptr inbounds float, float* %3, i64 %idxprom.i.2
  %arrayidx6.i.2 = getelementptr inbounds float, float* %4, i64 %idxprom.i.2
  %mul.i.2 = mul nsw i32 %conv2.i.2, %5
  %184 = trunc i64 %mul.i.i to i32
  %185 = load float, float* %arrayidx.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %186 = insertelement <8 x float> undef, float %185, i32 0
  %187 = shufflevector <8 x float> %186, <8 x float> undef, <8 x i32> zeroinitializer
  %188 = shl i64 %7, 37
  %189 = ashr exact i64 %188, 32
  %190 = getelementptr inbounds float, float* %1, i64 %189
  %191 = bitcast float* %190 to <8 x float>*
  %wide.load23 = load <8 x float>, <8 x float>* %191, align 4, !tbaa !12, !llvm.access.group !16
  %192 = load float, float* %arrayidx6.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %193 = insertelement <8 x float> undef, float %192, i32 0
  %194 = shufflevector <8 x float> %193, <8 x float> undef, <8 x i32> zeroinitializer
  %195 = getelementptr inbounds float, float* %2, i64 %189
  %196 = bitcast float* %195 to <8 x float>*
  %wide.load24 = load <8 x float>, <8 x float>* %196, align 4, !tbaa !12, !llvm.access.group !16
  %197 = fmul <8 x float> %194, %wide.load24
  %198 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %187, <8 x float> %wide.load23, <8 x float> %197)
  %199 = add nsw i32 %mul.i.2, %184
  %200 = sext i32 %199 to i64
  %201 = getelementptr inbounds float, float* %0, i64 %200
  %202 = bitcast float* %201 to <8 x float>*
  %wide.load25 = load <8 x float>, <8 x float>* %202, align 4, !tbaa !12, !llvm.access.group !16
  %203 = fadd <8 x float> %wide.load25, %198
  %204 = bitcast float* %201 to <8 x float>*
  store <8 x float> %203, <8 x float>* %204, align 4, !tbaa !12, !llvm.access.group !16
  %205 = or i64 %mul.i.i, 8
  %206 = trunc i64 %205 to i32
  %207 = load float, float* %arrayidx.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %208 = insertelement <8 x float> undef, float %207, i32 0
  %209 = shufflevector <8 x float> %208, <8 x float> undef, <8 x i32> zeroinitializer
  %210 = shl i64 %205, 32
  %211 = ashr exact i64 %210, 32
  %212 = getelementptr inbounds float, float* %1, i64 %211
  %213 = bitcast float* %212 to <8 x float>*
  %wide.load23.1 = load <8 x float>, <8 x float>* %213, align 4, !tbaa !12, !llvm.access.group !16
  %214 = load float, float* %arrayidx6.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %215 = insertelement <8 x float> undef, float %214, i32 0
  %216 = shufflevector <8 x float> %215, <8 x float> undef, <8 x i32> zeroinitializer
  %217 = getelementptr inbounds float, float* %2, i64 %211
  %218 = bitcast float* %217 to <8 x float>*
  %wide.load24.1 = load <8 x float>, <8 x float>* %218, align 4, !tbaa !12, !llvm.access.group !16
  %219 = fmul <8 x float> %216, %wide.load24.1
  %220 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %209, <8 x float> %wide.load23.1, <8 x float> %219)
  %221 = add nsw i32 %mul.i.2, %206
  %222 = sext i32 %221 to i64
  %223 = getelementptr inbounds float, float* %0, i64 %222
  %224 = bitcast float* %223 to <8 x float>*
  %wide.load25.1 = load <8 x float>, <8 x float>* %224, align 4, !tbaa !12, !llvm.access.group !16
  %225 = fadd <8 x float> %wide.load25.1, %220
  %226 = bitcast float* %223 to <8 x float>*
  store <8 x float> %225, <8 x float>* %226, align 4, !tbaa !12, !llvm.access.group !16
  %227 = or i64 %mul.i.i, 16
  %228 = trunc i64 %227 to i32
  %229 = load float, float* %arrayidx.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %230 = insertelement <8 x float> undef, float %229, i32 0
  %231 = shufflevector <8 x float> %230, <8 x float> undef, <8 x i32> zeroinitializer
  %232 = shl i64 %227, 32
  %233 = ashr exact i64 %232, 32
  %234 = getelementptr inbounds float, float* %1, i64 %233
  %235 = bitcast float* %234 to <8 x float>*
  %wide.load23.2 = load <8 x float>, <8 x float>* %235, align 4, !tbaa !12, !llvm.access.group !16
  %236 = load float, float* %arrayidx6.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %237 = insertelement <8 x float> undef, float %236, i32 0
  %238 = shufflevector <8 x float> %237, <8 x float> undef, <8 x i32> zeroinitializer
  %239 = getelementptr inbounds float, float* %2, i64 %233
  %240 = bitcast float* %239 to <8 x float>*
  %wide.load24.2 = load <8 x float>, <8 x float>* %240, align 4, !tbaa !12, !llvm.access.group !16
  %241 = fmul <8 x float> %238, %wide.load24.2
  %242 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %231, <8 x float> %wide.load23.2, <8 x float> %241)
  %243 = add nsw i32 %mul.i.2, %228
  %244 = sext i32 %243 to i64
  %245 = getelementptr inbounds float, float* %0, i64 %244
  %246 = bitcast float* %245 to <8 x float>*
  %wide.load25.2 = load <8 x float>, <8 x float>* %246, align 4, !tbaa !12, !llvm.access.group !16
  %247 = fadd <8 x float> %wide.load25.2, %242
  %248 = bitcast float* %245 to <8 x float>*
  store <8 x float> %247, <8 x float>* %248, align 4, !tbaa !12, !llvm.access.group !16
  %249 = or i64 %mul.i.i, 24
  %250 = trunc i64 %249 to i32
  %251 = load float, float* %arrayidx.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %252 = insertelement <8 x float> undef, float %251, i32 0
  %253 = shufflevector <8 x float> %252, <8 x float> undef, <8 x i32> zeroinitializer
  %254 = shl i64 %249, 32
  %255 = ashr exact i64 %254, 32
  %256 = getelementptr inbounds float, float* %1, i64 %255
  %257 = bitcast float* %256 to <8 x float>*
  %wide.load23.3 = load <8 x float>, <8 x float>* %257, align 4, !tbaa !12, !llvm.access.group !16
  %258 = load float, float* %arrayidx6.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %259 = insertelement <8 x float> undef, float %258, i32 0
  %260 = shufflevector <8 x float> %259, <8 x float> undef, <8 x i32> zeroinitializer
  %261 = getelementptr inbounds float, float* %2, i64 %255
  %262 = bitcast float* %261 to <8 x float>*
  %wide.load24.3 = load <8 x float>, <8 x float>* %262, align 4, !tbaa !12, !llvm.access.group !16
  %263 = fmul <8 x float> %260, %wide.load24.3
  %264 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %253, <8 x float> %wide.load23.3, <8 x float> %263)
  %265 = add nsw i32 %mul.i.2, %250
  %266 = sext i32 %265 to i64
  %267 = getelementptr inbounds float, float* %0, i64 %266
  %268 = bitcast float* %267 to <8 x float>*
  %wide.load25.3 = load <8 x float>, <8 x float>* %268, align 4, !tbaa !12, !llvm.access.group !16
  %269 = fadd <8 x float> %wide.load25.3, %264
  %270 = bitcast float* %267 to <8 x float>*
  store <8 x float> %269, <8 x float>* %270, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.3 = or i64 %mul3.i.i, 3
  %conv2.i.3 = trunc i64 %add6.i.i.3 to i32
  %sext.i.3 = shl i64 %add6.i.i.3, 32
  %idxprom.i.3 = ashr exact i64 %sext.i.3, 32
  %arrayidx.i.3 = getelementptr inbounds float, float* %3, i64 %idxprom.i.3
  %arrayidx6.i.3 = getelementptr inbounds float, float* %4, i64 %idxprom.i.3
  %mul.i.3 = mul nsw i32 %conv2.i.3, %5
  %271 = trunc i64 %mul.i.i to i32
  %272 = load float, float* %arrayidx.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %273 = insertelement <8 x float> undef, float %272, i32 0
  %274 = shufflevector <8 x float> %273, <8 x float> undef, <8 x i32> zeroinitializer
  %275 = shl i64 %7, 37
  %276 = ashr exact i64 %275, 32
  %277 = getelementptr inbounds float, float* %1, i64 %276
  %278 = bitcast float* %277 to <8 x float>*
  %wide.load34 = load <8 x float>, <8 x float>* %278, align 4, !tbaa !12, !llvm.access.group !16
  %279 = load float, float* %arrayidx6.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %280 = insertelement <8 x float> undef, float %279, i32 0
  %281 = shufflevector <8 x float> %280, <8 x float> undef, <8 x i32> zeroinitializer
  %282 = getelementptr inbounds float, float* %2, i64 %276
  %283 = bitcast float* %282 to <8 x float>*
  %wide.load35 = load <8 x float>, <8 x float>* %283, align 4, !tbaa !12, !llvm.access.group !16
  %284 = fmul <8 x float> %281, %wide.load35
  %285 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %274, <8 x float> %wide.load34, <8 x float> %284)
  %286 = add nsw i32 %mul.i.3, %271
  %287 = sext i32 %286 to i64
  %288 = getelementptr inbounds float, float* %0, i64 %287
  %289 = bitcast float* %288 to <8 x float>*
  %wide.load36 = load <8 x float>, <8 x float>* %289, align 4, !tbaa !12, !llvm.access.group !16
  %290 = fadd <8 x float> %wide.load36, %285
  %291 = bitcast float* %288 to <8 x float>*
  store <8 x float> %290, <8 x float>* %291, align 4, !tbaa !12, !llvm.access.group !16
  %292 = or i64 %mul.i.i, 8
  %293 = trunc i64 %292 to i32
  %294 = load float, float* %arrayidx.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %295 = insertelement <8 x float> undef, float %294, i32 0
  %296 = shufflevector <8 x float> %295, <8 x float> undef, <8 x i32> zeroinitializer
  %297 = shl i64 %292, 32
  %298 = ashr exact i64 %297, 32
  %299 = getelementptr inbounds float, float* %1, i64 %298
  %300 = bitcast float* %299 to <8 x float>*
  %wide.load34.1 = load <8 x float>, <8 x float>* %300, align 4, !tbaa !12, !llvm.access.group !16
  %301 = load float, float* %arrayidx6.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %302 = insertelement <8 x float> undef, float %301, i32 0
  %303 = shufflevector <8 x float> %302, <8 x float> undef, <8 x i32> zeroinitializer
  %304 = getelementptr inbounds float, float* %2, i64 %298
  %305 = bitcast float* %304 to <8 x float>*
  %wide.load35.1 = load <8 x float>, <8 x float>* %305, align 4, !tbaa !12, !llvm.access.group !16
  %306 = fmul <8 x float> %303, %wide.load35.1
  %307 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %296, <8 x float> %wide.load34.1, <8 x float> %306)
  %308 = add nsw i32 %mul.i.3, %293
  %309 = sext i32 %308 to i64
  %310 = getelementptr inbounds float, float* %0, i64 %309
  %311 = bitcast float* %310 to <8 x float>*
  %wide.load36.1 = load <8 x float>, <8 x float>* %311, align 4, !tbaa !12, !llvm.access.group !16
  %312 = fadd <8 x float> %wide.load36.1, %307
  %313 = bitcast float* %310 to <8 x float>*
  store <8 x float> %312, <8 x float>* %313, align 4, !tbaa !12, !llvm.access.group !16
  %314 = or i64 %mul.i.i, 16
  %315 = trunc i64 %314 to i32
  %316 = load float, float* %arrayidx.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %317 = insertelement <8 x float> undef, float %316, i32 0
  %318 = shufflevector <8 x float> %317, <8 x float> undef, <8 x i32> zeroinitializer
  %319 = shl i64 %314, 32
  %320 = ashr exact i64 %319, 32
  %321 = getelementptr inbounds float, float* %1, i64 %320
  %322 = bitcast float* %321 to <8 x float>*
  %wide.load34.2 = load <8 x float>, <8 x float>* %322, align 4, !tbaa !12, !llvm.access.group !16
  %323 = load float, float* %arrayidx6.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %324 = insertelement <8 x float> undef, float %323, i32 0
  %325 = shufflevector <8 x float> %324, <8 x float> undef, <8 x i32> zeroinitializer
  %326 = getelementptr inbounds float, float* %2, i64 %320
  %327 = bitcast float* %326 to <8 x float>*
  %wide.load35.2 = load <8 x float>, <8 x float>* %327, align 4, !tbaa !12, !llvm.access.group !16
  %328 = fmul <8 x float> %325, %wide.load35.2
  %329 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %318, <8 x float> %wide.load34.2, <8 x float> %328)
  %330 = add nsw i32 %mul.i.3, %315
  %331 = sext i32 %330 to i64
  %332 = getelementptr inbounds float, float* %0, i64 %331
  %333 = bitcast float* %332 to <8 x float>*
  %wide.load36.2 = load <8 x float>, <8 x float>* %333, align 4, !tbaa !12, !llvm.access.group !16
  %334 = fadd <8 x float> %wide.load36.2, %329
  %335 = bitcast float* %332 to <8 x float>*
  store <8 x float> %334, <8 x float>* %335, align 4, !tbaa !12, !llvm.access.group !16
  %336 = or i64 %mul.i.i, 24
  %337 = trunc i64 %336 to i32
  %338 = load float, float* %arrayidx.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %339 = insertelement <8 x float> undef, float %338, i32 0
  %340 = shufflevector <8 x float> %339, <8 x float> undef, <8 x i32> zeroinitializer
  %341 = shl i64 %336, 32
  %342 = ashr exact i64 %341, 32
  %343 = getelementptr inbounds float, float* %1, i64 %342
  %344 = bitcast float* %343 to <8 x float>*
  %wide.load34.3 = load <8 x float>, <8 x float>* %344, align 4, !tbaa !12, !llvm.access.group !16
  %345 = load float, float* %arrayidx6.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %346 = insertelement <8 x float> undef, float %345, i32 0
  %347 = shufflevector <8 x float> %346, <8 x float> undef, <8 x i32> zeroinitializer
  %348 = getelementptr inbounds float, float* %2, i64 %342
  %349 = bitcast float* %348 to <8 x float>*
  %wide.load35.3 = load <8 x float>, <8 x float>* %349, align 4, !tbaa !12, !llvm.access.group !16
  %350 = fmul <8 x float> %347, %wide.load35.3
  %351 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %340, <8 x float> %wide.load34.3, <8 x float> %350)
  %352 = add nsw i32 %mul.i.3, %337
  %353 = sext i32 %352 to i64
  %354 = getelementptr inbounds float, float* %0, i64 %353
  %355 = bitcast float* %354 to <8 x float>*
  %wide.load36.3 = load <8 x float>, <8 x float>* %355, align 4, !tbaa !12, !llvm.access.group !16
  %356 = fadd <8 x float> %wide.load36.3, %351
  %357 = bitcast float* %354 to <8 x float>*
  store <8 x float> %356, <8 x float>* %357, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.4 = or i64 %mul3.i.i, 4
  %conv2.i.4 = trunc i64 %add6.i.i.4 to i32
  %sext.i.4 = shl i64 %add6.i.i.4, 32
  %idxprom.i.4 = ashr exact i64 %sext.i.4, 32
  %arrayidx.i.4 = getelementptr inbounds float, float* %3, i64 %idxprom.i.4
  %arrayidx6.i.4 = getelementptr inbounds float, float* %4, i64 %idxprom.i.4
  %mul.i.4 = mul nsw i32 %conv2.i.4, %5
  %358 = trunc i64 %mul.i.i to i32
  %359 = load float, float* %arrayidx.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %360 = insertelement <8 x float> undef, float %359, i32 0
  %361 = shufflevector <8 x float> %360, <8 x float> undef, <8 x i32> zeroinitializer
  %362 = shl i64 %7, 37
  %363 = ashr exact i64 %362, 32
  %364 = getelementptr inbounds float, float* %1, i64 %363
  %365 = bitcast float* %364 to <8 x float>*
  %wide.load45 = load <8 x float>, <8 x float>* %365, align 4, !tbaa !12, !llvm.access.group !16
  %366 = load float, float* %arrayidx6.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %367 = insertelement <8 x float> undef, float %366, i32 0
  %368 = shufflevector <8 x float> %367, <8 x float> undef, <8 x i32> zeroinitializer
  %369 = getelementptr inbounds float, float* %2, i64 %363
  %370 = bitcast float* %369 to <8 x float>*
  %wide.load46 = load <8 x float>, <8 x float>* %370, align 4, !tbaa !12, !llvm.access.group !16
  %371 = fmul <8 x float> %368, %wide.load46
  %372 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %361, <8 x float> %wide.load45, <8 x float> %371)
  %373 = add nsw i32 %mul.i.4, %358
  %374 = sext i32 %373 to i64
  %375 = getelementptr inbounds float, float* %0, i64 %374
  %376 = bitcast float* %375 to <8 x float>*
  %wide.load47 = load <8 x float>, <8 x float>* %376, align 4, !tbaa !12, !llvm.access.group !16
  %377 = fadd <8 x float> %wide.load47, %372
  %378 = bitcast float* %375 to <8 x float>*
  store <8 x float> %377, <8 x float>* %378, align 4, !tbaa !12, !llvm.access.group !16
  %379 = or i64 %mul.i.i, 8
  %380 = trunc i64 %379 to i32
  %381 = load float, float* %arrayidx.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %382 = insertelement <8 x float> undef, float %381, i32 0
  %383 = shufflevector <8 x float> %382, <8 x float> undef, <8 x i32> zeroinitializer
  %384 = shl i64 %379, 32
  %385 = ashr exact i64 %384, 32
  %386 = getelementptr inbounds float, float* %1, i64 %385
  %387 = bitcast float* %386 to <8 x float>*
  %wide.load45.1 = load <8 x float>, <8 x float>* %387, align 4, !tbaa !12, !llvm.access.group !16
  %388 = load float, float* %arrayidx6.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %389 = insertelement <8 x float> undef, float %388, i32 0
  %390 = shufflevector <8 x float> %389, <8 x float> undef, <8 x i32> zeroinitializer
  %391 = getelementptr inbounds float, float* %2, i64 %385
  %392 = bitcast float* %391 to <8 x float>*
  %wide.load46.1 = load <8 x float>, <8 x float>* %392, align 4, !tbaa !12, !llvm.access.group !16
  %393 = fmul <8 x float> %390, %wide.load46.1
  %394 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %383, <8 x float> %wide.load45.1, <8 x float> %393)
  %395 = add nsw i32 %mul.i.4, %380
  %396 = sext i32 %395 to i64
  %397 = getelementptr inbounds float, float* %0, i64 %396
  %398 = bitcast float* %397 to <8 x float>*
  %wide.load47.1 = load <8 x float>, <8 x float>* %398, align 4, !tbaa !12, !llvm.access.group !16
  %399 = fadd <8 x float> %wide.load47.1, %394
  %400 = bitcast float* %397 to <8 x float>*
  store <8 x float> %399, <8 x float>* %400, align 4, !tbaa !12, !llvm.access.group !16
  %401 = or i64 %mul.i.i, 16
  %402 = trunc i64 %401 to i32
  %403 = load float, float* %arrayidx.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %404 = insertelement <8 x float> undef, float %403, i32 0
  %405 = shufflevector <8 x float> %404, <8 x float> undef, <8 x i32> zeroinitializer
  %406 = shl i64 %401, 32
  %407 = ashr exact i64 %406, 32
  %408 = getelementptr inbounds float, float* %1, i64 %407
  %409 = bitcast float* %408 to <8 x float>*
  %wide.load45.2 = load <8 x float>, <8 x float>* %409, align 4, !tbaa !12, !llvm.access.group !16
  %410 = load float, float* %arrayidx6.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %411 = insertelement <8 x float> undef, float %410, i32 0
  %412 = shufflevector <8 x float> %411, <8 x float> undef, <8 x i32> zeroinitializer
  %413 = getelementptr inbounds float, float* %2, i64 %407
  %414 = bitcast float* %413 to <8 x float>*
  %wide.load46.2 = load <8 x float>, <8 x float>* %414, align 4, !tbaa !12, !llvm.access.group !16
  %415 = fmul <8 x float> %412, %wide.load46.2
  %416 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %405, <8 x float> %wide.load45.2, <8 x float> %415)
  %417 = add nsw i32 %mul.i.4, %402
  %418 = sext i32 %417 to i64
  %419 = getelementptr inbounds float, float* %0, i64 %418
  %420 = bitcast float* %419 to <8 x float>*
  %wide.load47.2 = load <8 x float>, <8 x float>* %420, align 4, !tbaa !12, !llvm.access.group !16
  %421 = fadd <8 x float> %wide.load47.2, %416
  %422 = bitcast float* %419 to <8 x float>*
  store <8 x float> %421, <8 x float>* %422, align 4, !tbaa !12, !llvm.access.group !16
  %423 = or i64 %mul.i.i, 24
  %424 = trunc i64 %423 to i32
  %425 = load float, float* %arrayidx.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %426 = insertelement <8 x float> undef, float %425, i32 0
  %427 = shufflevector <8 x float> %426, <8 x float> undef, <8 x i32> zeroinitializer
  %428 = shl i64 %423, 32
  %429 = ashr exact i64 %428, 32
  %430 = getelementptr inbounds float, float* %1, i64 %429
  %431 = bitcast float* %430 to <8 x float>*
  %wide.load45.3 = load <8 x float>, <8 x float>* %431, align 4, !tbaa !12, !llvm.access.group !16
  %432 = load float, float* %arrayidx6.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %433 = insertelement <8 x float> undef, float %432, i32 0
  %434 = shufflevector <8 x float> %433, <8 x float> undef, <8 x i32> zeroinitializer
  %435 = getelementptr inbounds float, float* %2, i64 %429
  %436 = bitcast float* %435 to <8 x float>*
  %wide.load46.3 = load <8 x float>, <8 x float>* %436, align 4, !tbaa !12, !llvm.access.group !16
  %437 = fmul <8 x float> %434, %wide.load46.3
  %438 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %427, <8 x float> %wide.load45.3, <8 x float> %437)
  %439 = add nsw i32 %mul.i.4, %424
  %440 = sext i32 %439 to i64
  %441 = getelementptr inbounds float, float* %0, i64 %440
  %442 = bitcast float* %441 to <8 x float>*
  %wide.load47.3 = load <8 x float>, <8 x float>* %442, align 4, !tbaa !12, !llvm.access.group !16
  %443 = fadd <8 x float> %wide.load47.3, %438
  %444 = bitcast float* %441 to <8 x float>*
  store <8 x float> %443, <8 x float>* %444, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.5 = or i64 %mul3.i.i, 5
  %conv2.i.5 = trunc i64 %add6.i.i.5 to i32
  %sext.i.5 = shl i64 %add6.i.i.5, 32
  %idxprom.i.5 = ashr exact i64 %sext.i.5, 32
  %arrayidx.i.5 = getelementptr inbounds float, float* %3, i64 %idxprom.i.5
  %arrayidx6.i.5 = getelementptr inbounds float, float* %4, i64 %idxprom.i.5
  %mul.i.5 = mul nsw i32 %conv2.i.5, %5
  %445 = trunc i64 %mul.i.i to i32
  %446 = load float, float* %arrayidx.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %447 = insertelement <8 x float> undef, float %446, i32 0
  %448 = shufflevector <8 x float> %447, <8 x float> undef, <8 x i32> zeroinitializer
  %449 = shl i64 %7, 37
  %450 = ashr exact i64 %449, 32
  %451 = getelementptr inbounds float, float* %1, i64 %450
  %452 = bitcast float* %451 to <8 x float>*
  %wide.load56 = load <8 x float>, <8 x float>* %452, align 4, !tbaa !12, !llvm.access.group !16
  %453 = load float, float* %arrayidx6.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %454 = insertelement <8 x float> undef, float %453, i32 0
  %455 = shufflevector <8 x float> %454, <8 x float> undef, <8 x i32> zeroinitializer
  %456 = getelementptr inbounds float, float* %2, i64 %450
  %457 = bitcast float* %456 to <8 x float>*
  %wide.load57 = load <8 x float>, <8 x float>* %457, align 4, !tbaa !12, !llvm.access.group !16
  %458 = fmul <8 x float> %455, %wide.load57
  %459 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %448, <8 x float> %wide.load56, <8 x float> %458)
  %460 = add nsw i32 %mul.i.5, %445
  %461 = sext i32 %460 to i64
  %462 = getelementptr inbounds float, float* %0, i64 %461
  %463 = bitcast float* %462 to <8 x float>*
  %wide.load58 = load <8 x float>, <8 x float>* %463, align 4, !tbaa !12, !llvm.access.group !16
  %464 = fadd <8 x float> %wide.load58, %459
  %465 = bitcast float* %462 to <8 x float>*
  store <8 x float> %464, <8 x float>* %465, align 4, !tbaa !12, !llvm.access.group !16
  %466 = or i64 %mul.i.i, 8
  %467 = trunc i64 %466 to i32
  %468 = load float, float* %arrayidx.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %469 = insertelement <8 x float> undef, float %468, i32 0
  %470 = shufflevector <8 x float> %469, <8 x float> undef, <8 x i32> zeroinitializer
  %471 = shl i64 %466, 32
  %472 = ashr exact i64 %471, 32
  %473 = getelementptr inbounds float, float* %1, i64 %472
  %474 = bitcast float* %473 to <8 x float>*
  %wide.load56.1 = load <8 x float>, <8 x float>* %474, align 4, !tbaa !12, !llvm.access.group !16
  %475 = load float, float* %arrayidx6.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %476 = insertelement <8 x float> undef, float %475, i32 0
  %477 = shufflevector <8 x float> %476, <8 x float> undef, <8 x i32> zeroinitializer
  %478 = getelementptr inbounds float, float* %2, i64 %472
  %479 = bitcast float* %478 to <8 x float>*
  %wide.load57.1 = load <8 x float>, <8 x float>* %479, align 4, !tbaa !12, !llvm.access.group !16
  %480 = fmul <8 x float> %477, %wide.load57.1
  %481 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %470, <8 x float> %wide.load56.1, <8 x float> %480)
  %482 = add nsw i32 %mul.i.5, %467
  %483 = sext i32 %482 to i64
  %484 = getelementptr inbounds float, float* %0, i64 %483
  %485 = bitcast float* %484 to <8 x float>*
  %wide.load58.1 = load <8 x float>, <8 x float>* %485, align 4, !tbaa !12, !llvm.access.group !16
  %486 = fadd <8 x float> %wide.load58.1, %481
  %487 = bitcast float* %484 to <8 x float>*
  store <8 x float> %486, <8 x float>* %487, align 4, !tbaa !12, !llvm.access.group !16
  %488 = or i64 %mul.i.i, 16
  %489 = trunc i64 %488 to i32
  %490 = load float, float* %arrayidx.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %491 = insertelement <8 x float> undef, float %490, i32 0
  %492 = shufflevector <8 x float> %491, <8 x float> undef, <8 x i32> zeroinitializer
  %493 = shl i64 %488, 32
  %494 = ashr exact i64 %493, 32
  %495 = getelementptr inbounds float, float* %1, i64 %494
  %496 = bitcast float* %495 to <8 x float>*
  %wide.load56.2 = load <8 x float>, <8 x float>* %496, align 4, !tbaa !12, !llvm.access.group !16
  %497 = load float, float* %arrayidx6.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %498 = insertelement <8 x float> undef, float %497, i32 0
  %499 = shufflevector <8 x float> %498, <8 x float> undef, <8 x i32> zeroinitializer
  %500 = getelementptr inbounds float, float* %2, i64 %494
  %501 = bitcast float* %500 to <8 x float>*
  %wide.load57.2 = load <8 x float>, <8 x float>* %501, align 4, !tbaa !12, !llvm.access.group !16
  %502 = fmul <8 x float> %499, %wide.load57.2
  %503 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %492, <8 x float> %wide.load56.2, <8 x float> %502)
  %504 = add nsw i32 %mul.i.5, %489
  %505 = sext i32 %504 to i64
  %506 = getelementptr inbounds float, float* %0, i64 %505
  %507 = bitcast float* %506 to <8 x float>*
  %wide.load58.2 = load <8 x float>, <8 x float>* %507, align 4, !tbaa !12, !llvm.access.group !16
  %508 = fadd <8 x float> %wide.load58.2, %503
  %509 = bitcast float* %506 to <8 x float>*
  store <8 x float> %508, <8 x float>* %509, align 4, !tbaa !12, !llvm.access.group !16
  %510 = or i64 %mul.i.i, 24
  %511 = trunc i64 %510 to i32
  %512 = load float, float* %arrayidx.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %513 = insertelement <8 x float> undef, float %512, i32 0
  %514 = shufflevector <8 x float> %513, <8 x float> undef, <8 x i32> zeroinitializer
  %515 = shl i64 %510, 32
  %516 = ashr exact i64 %515, 32
  %517 = getelementptr inbounds float, float* %1, i64 %516
  %518 = bitcast float* %517 to <8 x float>*
  %wide.load56.3 = load <8 x float>, <8 x float>* %518, align 4, !tbaa !12, !llvm.access.group !16
  %519 = load float, float* %arrayidx6.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %520 = insertelement <8 x float> undef, float %519, i32 0
  %521 = shufflevector <8 x float> %520, <8 x float> undef, <8 x i32> zeroinitializer
  %522 = getelementptr inbounds float, float* %2, i64 %516
  %523 = bitcast float* %522 to <8 x float>*
  %wide.load57.3 = load <8 x float>, <8 x float>* %523, align 4, !tbaa !12, !llvm.access.group !16
  %524 = fmul <8 x float> %521, %wide.load57.3
  %525 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %514, <8 x float> %wide.load56.3, <8 x float> %524)
  %526 = add nsw i32 %mul.i.5, %511
  %527 = sext i32 %526 to i64
  %528 = getelementptr inbounds float, float* %0, i64 %527
  %529 = bitcast float* %528 to <8 x float>*
  %wide.load58.3 = load <8 x float>, <8 x float>* %529, align 4, !tbaa !12, !llvm.access.group !16
  %530 = fadd <8 x float> %wide.load58.3, %525
  %531 = bitcast float* %528 to <8 x float>*
  store <8 x float> %530, <8 x float>* %531, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.6 = or i64 %mul3.i.i, 6
  %conv2.i.6 = trunc i64 %add6.i.i.6 to i32
  %sext.i.6 = shl i64 %add6.i.i.6, 32
  %idxprom.i.6 = ashr exact i64 %sext.i.6, 32
  %arrayidx.i.6 = getelementptr inbounds float, float* %3, i64 %idxprom.i.6
  %arrayidx6.i.6 = getelementptr inbounds float, float* %4, i64 %idxprom.i.6
  %mul.i.6 = mul nsw i32 %conv2.i.6, %5
  %532 = trunc i64 %mul.i.i to i32
  %533 = load float, float* %arrayidx.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %534 = insertelement <8 x float> undef, float %533, i32 0
  %535 = shufflevector <8 x float> %534, <8 x float> undef, <8 x i32> zeroinitializer
  %536 = shl i64 %7, 37
  %537 = ashr exact i64 %536, 32
  %538 = getelementptr inbounds float, float* %1, i64 %537
  %539 = bitcast float* %538 to <8 x float>*
  %wide.load67 = load <8 x float>, <8 x float>* %539, align 4, !tbaa !12, !llvm.access.group !16
  %540 = load float, float* %arrayidx6.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %541 = insertelement <8 x float> undef, float %540, i32 0
  %542 = shufflevector <8 x float> %541, <8 x float> undef, <8 x i32> zeroinitializer
  %543 = getelementptr inbounds float, float* %2, i64 %537
  %544 = bitcast float* %543 to <8 x float>*
  %wide.load68 = load <8 x float>, <8 x float>* %544, align 4, !tbaa !12, !llvm.access.group !16
  %545 = fmul <8 x float> %542, %wide.load68
  %546 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %535, <8 x float> %wide.load67, <8 x float> %545)
  %547 = add nsw i32 %mul.i.6, %532
  %548 = sext i32 %547 to i64
  %549 = getelementptr inbounds float, float* %0, i64 %548
  %550 = bitcast float* %549 to <8 x float>*
  %wide.load69 = load <8 x float>, <8 x float>* %550, align 4, !tbaa !12, !llvm.access.group !16
  %551 = fadd <8 x float> %wide.load69, %546
  %552 = bitcast float* %549 to <8 x float>*
  store <8 x float> %551, <8 x float>* %552, align 4, !tbaa !12, !llvm.access.group !16
  %553 = or i64 %mul.i.i, 8
  %554 = trunc i64 %553 to i32
  %555 = load float, float* %arrayidx.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %556 = insertelement <8 x float> undef, float %555, i32 0
  %557 = shufflevector <8 x float> %556, <8 x float> undef, <8 x i32> zeroinitializer
  %558 = shl i64 %553, 32
  %559 = ashr exact i64 %558, 32
  %560 = getelementptr inbounds float, float* %1, i64 %559
  %561 = bitcast float* %560 to <8 x float>*
  %wide.load67.1 = load <8 x float>, <8 x float>* %561, align 4, !tbaa !12, !llvm.access.group !16
  %562 = load float, float* %arrayidx6.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %563 = insertelement <8 x float> undef, float %562, i32 0
  %564 = shufflevector <8 x float> %563, <8 x float> undef, <8 x i32> zeroinitializer
  %565 = getelementptr inbounds float, float* %2, i64 %559
  %566 = bitcast float* %565 to <8 x float>*
  %wide.load68.1 = load <8 x float>, <8 x float>* %566, align 4, !tbaa !12, !llvm.access.group !16
  %567 = fmul <8 x float> %564, %wide.load68.1
  %568 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %557, <8 x float> %wide.load67.1, <8 x float> %567)
  %569 = add nsw i32 %mul.i.6, %554
  %570 = sext i32 %569 to i64
  %571 = getelementptr inbounds float, float* %0, i64 %570
  %572 = bitcast float* %571 to <8 x float>*
  %wide.load69.1 = load <8 x float>, <8 x float>* %572, align 4, !tbaa !12, !llvm.access.group !16
  %573 = fadd <8 x float> %wide.load69.1, %568
  %574 = bitcast float* %571 to <8 x float>*
  store <8 x float> %573, <8 x float>* %574, align 4, !tbaa !12, !llvm.access.group !16
  %575 = or i64 %mul.i.i, 16
  %576 = trunc i64 %575 to i32
  %577 = load float, float* %arrayidx.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %578 = insertelement <8 x float> undef, float %577, i32 0
  %579 = shufflevector <8 x float> %578, <8 x float> undef, <8 x i32> zeroinitializer
  %580 = shl i64 %575, 32
  %581 = ashr exact i64 %580, 32
  %582 = getelementptr inbounds float, float* %1, i64 %581
  %583 = bitcast float* %582 to <8 x float>*
  %wide.load67.2 = load <8 x float>, <8 x float>* %583, align 4, !tbaa !12, !llvm.access.group !16
  %584 = load float, float* %arrayidx6.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %585 = insertelement <8 x float> undef, float %584, i32 0
  %586 = shufflevector <8 x float> %585, <8 x float> undef, <8 x i32> zeroinitializer
  %587 = getelementptr inbounds float, float* %2, i64 %581
  %588 = bitcast float* %587 to <8 x float>*
  %wide.load68.2 = load <8 x float>, <8 x float>* %588, align 4, !tbaa !12, !llvm.access.group !16
  %589 = fmul <8 x float> %586, %wide.load68.2
  %590 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %579, <8 x float> %wide.load67.2, <8 x float> %589)
  %591 = add nsw i32 %mul.i.6, %576
  %592 = sext i32 %591 to i64
  %593 = getelementptr inbounds float, float* %0, i64 %592
  %594 = bitcast float* %593 to <8 x float>*
  %wide.load69.2 = load <8 x float>, <8 x float>* %594, align 4, !tbaa !12, !llvm.access.group !16
  %595 = fadd <8 x float> %wide.load69.2, %590
  %596 = bitcast float* %593 to <8 x float>*
  store <8 x float> %595, <8 x float>* %596, align 4, !tbaa !12, !llvm.access.group !16
  %597 = or i64 %mul.i.i, 24
  %598 = trunc i64 %597 to i32
  %599 = load float, float* %arrayidx.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %600 = insertelement <8 x float> undef, float %599, i32 0
  %601 = shufflevector <8 x float> %600, <8 x float> undef, <8 x i32> zeroinitializer
  %602 = shl i64 %597, 32
  %603 = ashr exact i64 %602, 32
  %604 = getelementptr inbounds float, float* %1, i64 %603
  %605 = bitcast float* %604 to <8 x float>*
  %wide.load67.3 = load <8 x float>, <8 x float>* %605, align 4, !tbaa !12, !llvm.access.group !16
  %606 = load float, float* %arrayidx6.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %607 = insertelement <8 x float> undef, float %606, i32 0
  %608 = shufflevector <8 x float> %607, <8 x float> undef, <8 x i32> zeroinitializer
  %609 = getelementptr inbounds float, float* %2, i64 %603
  %610 = bitcast float* %609 to <8 x float>*
  %wide.load68.3 = load <8 x float>, <8 x float>* %610, align 4, !tbaa !12, !llvm.access.group !16
  %611 = fmul <8 x float> %608, %wide.load68.3
  %612 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %601, <8 x float> %wide.load67.3, <8 x float> %611)
  %613 = add nsw i32 %mul.i.6, %598
  %614 = sext i32 %613 to i64
  %615 = getelementptr inbounds float, float* %0, i64 %614
  %616 = bitcast float* %615 to <8 x float>*
  %wide.load69.3 = load <8 x float>, <8 x float>* %616, align 4, !tbaa !12, !llvm.access.group !16
  %617 = fadd <8 x float> %wide.load69.3, %612
  %618 = bitcast float* %615 to <8 x float>*
  store <8 x float> %617, <8 x float>* %618, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.7 = or i64 %mul3.i.i, 7
  %conv2.i.7 = trunc i64 %add6.i.i.7 to i32
  %sext.i.7 = shl i64 %add6.i.i.7, 32
  %idxprom.i.7 = ashr exact i64 %sext.i.7, 32
  %arrayidx.i.7 = getelementptr inbounds float, float* %3, i64 %idxprom.i.7
  %arrayidx6.i.7 = getelementptr inbounds float, float* %4, i64 %idxprom.i.7
  %mul.i.7 = mul nsw i32 %conv2.i.7, %5
  %619 = trunc i64 %mul.i.i to i32
  %620 = load float, float* %arrayidx.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %621 = insertelement <8 x float> undef, float %620, i32 0
  %622 = shufflevector <8 x float> %621, <8 x float> undef, <8 x i32> zeroinitializer
  %623 = shl i64 %7, 37
  %624 = ashr exact i64 %623, 32
  %625 = getelementptr inbounds float, float* %1, i64 %624
  %626 = bitcast float* %625 to <8 x float>*
  %wide.load78 = load <8 x float>, <8 x float>* %626, align 4, !tbaa !12, !llvm.access.group !16
  %627 = load float, float* %arrayidx6.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %628 = insertelement <8 x float> undef, float %627, i32 0
  %629 = shufflevector <8 x float> %628, <8 x float> undef, <8 x i32> zeroinitializer
  %630 = getelementptr inbounds float, float* %2, i64 %624
  %631 = bitcast float* %630 to <8 x float>*
  %wide.load79 = load <8 x float>, <8 x float>* %631, align 4, !tbaa !12, !llvm.access.group !16
  %632 = fmul <8 x float> %629, %wide.load79
  %633 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %622, <8 x float> %wide.load78, <8 x float> %632)
  %634 = add nsw i32 %mul.i.7, %619
  %635 = sext i32 %634 to i64
  %636 = getelementptr inbounds float, float* %0, i64 %635
  %637 = bitcast float* %636 to <8 x float>*
  %wide.load80 = load <8 x float>, <8 x float>* %637, align 4, !tbaa !12, !llvm.access.group !16
  %638 = fadd <8 x float> %wide.load80, %633
  %639 = bitcast float* %636 to <8 x float>*
  store <8 x float> %638, <8 x float>* %639, align 4, !tbaa !12, !llvm.access.group !16
  %640 = or i64 %mul.i.i, 8
  %641 = trunc i64 %640 to i32
  %642 = load float, float* %arrayidx.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %643 = insertelement <8 x float> undef, float %642, i32 0
  %644 = shufflevector <8 x float> %643, <8 x float> undef, <8 x i32> zeroinitializer
  %645 = shl i64 %640, 32
  %646 = ashr exact i64 %645, 32
  %647 = getelementptr inbounds float, float* %1, i64 %646
  %648 = bitcast float* %647 to <8 x float>*
  %wide.load78.1 = load <8 x float>, <8 x float>* %648, align 4, !tbaa !12, !llvm.access.group !16
  %649 = load float, float* %arrayidx6.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %650 = insertelement <8 x float> undef, float %649, i32 0
  %651 = shufflevector <8 x float> %650, <8 x float> undef, <8 x i32> zeroinitializer
  %652 = getelementptr inbounds float, float* %2, i64 %646
  %653 = bitcast float* %652 to <8 x float>*
  %wide.load79.1 = load <8 x float>, <8 x float>* %653, align 4, !tbaa !12, !llvm.access.group !16
  %654 = fmul <8 x float> %651, %wide.load79.1
  %655 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %644, <8 x float> %wide.load78.1, <8 x float> %654)
  %656 = add nsw i32 %mul.i.7, %641
  %657 = sext i32 %656 to i64
  %658 = getelementptr inbounds float, float* %0, i64 %657
  %659 = bitcast float* %658 to <8 x float>*
  %wide.load80.1 = load <8 x float>, <8 x float>* %659, align 4, !tbaa !12, !llvm.access.group !16
  %660 = fadd <8 x float> %wide.load80.1, %655
  %661 = bitcast float* %658 to <8 x float>*
  store <8 x float> %660, <8 x float>* %661, align 4, !tbaa !12, !llvm.access.group !16
  %662 = or i64 %mul.i.i, 16
  %663 = trunc i64 %662 to i32
  %664 = load float, float* %arrayidx.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %665 = insertelement <8 x float> undef, float %664, i32 0
  %666 = shufflevector <8 x float> %665, <8 x float> undef, <8 x i32> zeroinitializer
  %667 = shl i64 %662, 32
  %668 = ashr exact i64 %667, 32
  %669 = getelementptr inbounds float, float* %1, i64 %668
  %670 = bitcast float* %669 to <8 x float>*
  %wide.load78.2 = load <8 x float>, <8 x float>* %670, align 4, !tbaa !12, !llvm.access.group !16
  %671 = load float, float* %arrayidx6.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %672 = insertelement <8 x float> undef, float %671, i32 0
  %673 = shufflevector <8 x float> %672, <8 x float> undef, <8 x i32> zeroinitializer
  %674 = getelementptr inbounds float, float* %2, i64 %668
  %675 = bitcast float* %674 to <8 x float>*
  %wide.load79.2 = load <8 x float>, <8 x float>* %675, align 4, !tbaa !12, !llvm.access.group !16
  %676 = fmul <8 x float> %673, %wide.load79.2
  %677 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %666, <8 x float> %wide.load78.2, <8 x float> %676)
  %678 = add nsw i32 %mul.i.7, %663
  %679 = sext i32 %678 to i64
  %680 = getelementptr inbounds float, float* %0, i64 %679
  %681 = bitcast float* %680 to <8 x float>*
  %wide.load80.2 = load <8 x float>, <8 x float>* %681, align 4, !tbaa !12, !llvm.access.group !16
  %682 = fadd <8 x float> %wide.load80.2, %677
  %683 = bitcast float* %680 to <8 x float>*
  store <8 x float> %682, <8 x float>* %683, align 4, !tbaa !12, !llvm.access.group !16
  %684 = or i64 %mul.i.i, 24
  %685 = trunc i64 %684 to i32
  %686 = load float, float* %arrayidx.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %687 = insertelement <8 x float> undef, float %686, i32 0
  %688 = shufflevector <8 x float> %687, <8 x float> undef, <8 x i32> zeroinitializer
  %689 = shl i64 %684, 32
  %690 = ashr exact i64 %689, 32
  %691 = getelementptr inbounds float, float* %1, i64 %690
  %692 = bitcast float* %691 to <8 x float>*
  %wide.load78.3 = load <8 x float>, <8 x float>* %692, align 4, !tbaa !12, !llvm.access.group !16
  %693 = load float, float* %arrayidx6.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %694 = insertelement <8 x float> undef, float %693, i32 0
  %695 = shufflevector <8 x float> %694, <8 x float> undef, <8 x i32> zeroinitializer
  %696 = getelementptr inbounds float, float* %2, i64 %690
  %697 = bitcast float* %696 to <8 x float>*
  %wide.load79.3 = load <8 x float>, <8 x float>* %697, align 4, !tbaa !12, !llvm.access.group !16
  %698 = fmul <8 x float> %695, %wide.load79.3
  %699 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %688, <8 x float> %wide.load78.3, <8 x float> %698)
  %700 = add nsw i32 %mul.i.7, %685
  %701 = sext i32 %700 to i64
  %702 = getelementptr inbounds float, float* %0, i64 %701
  %703 = bitcast float* %702 to <8 x float>*
  %wide.load80.3 = load <8 x float>, <8 x float>* %703, align 4, !tbaa !12, !llvm.access.group !16
  %704 = fadd <8 x float> %wide.load80.3, %699
  %705 = bitcast float* %702 to <8 x float>*
  store <8 x float> %704, <8 x float>* %705, align 4, !tbaa !12, !llvm.access.group !16
  ret void
}

; Function Attrs: nofree nounwind
define void @_pocl_kernel_gemver_kernel1_workgroup(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #1 {
pregion_for_entry.pregion_for_init.i.i:
  %5 = bitcast i8** %0 to float***
  %6 = load float**, float*** %5, align 8
  %7 = load float*, float** %6, align 8
  %8 = getelementptr i8*, i8** %0, i64 1
  %9 = bitcast i8** %8 to float***
  %10 = load float**, float*** %9, align 8
  %11 = load float*, float** %10, align 8
  %12 = getelementptr i8*, i8** %0, i64 2
  %13 = bitcast i8** %12 to float***
  %14 = load float**, float*** %13, align 8
  %15 = load float*, float** %14, align 8
  %16 = getelementptr i8*, i8** %0, i64 3
  %17 = bitcast i8** %16 to float***
  %18 = load float**, float*** %17, align 8
  %19 = load float*, float** %18, align 8
  %20 = getelementptr i8*, i8** %0, i64 4
  %21 = bitcast i8** %20 to float***
  %22 = load float**, float*** %21, align 8
  %23 = load float*, float** %22, align 8
  %24 = getelementptr i8*, i8** %0, i64 5
  %25 = bitcast i8** %24 to i32**
  %26 = load i32*, i32** %25, align 8
  %27 = load i32, i32* %26, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %conv2.i.i = trunc i64 %mul3.i.i.i to i32
  %sext.i.i = shl i64 %3, 35
  %idxprom.i.i = ashr exact i64 %sext.i.i, 32
  %arrayidx.i.i = getelementptr inbounds float, float* %19, i64 %idxprom.i.i
  %arrayidx6.i.i = getelementptr inbounds float, float* %23, i64 %idxprom.i.i
  %mul.i.i = mul nsw i32 %27, %conv2.i.i
  %28 = trunc i64 %mul.i.i.i to i32
  %29 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %30 = insertelement <8 x float> undef, float %29, i32 0
  %31 = shufflevector <8 x float> %30, <8 x float> undef, <8 x i32> zeroinitializer
  %32 = shl i64 %2, 37
  %33 = ashr exact i64 %32, 32
  %34 = getelementptr inbounds float, float* %11, i64 %33
  %35 = bitcast float* %34 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %35, align 4, !tbaa !12, !llvm.access.group !16
  %36 = load float, float* %arrayidx6.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %37 = insertelement <8 x float> undef, float %36, i32 0
  %38 = shufflevector <8 x float> %37, <8 x float> undef, <8 x i32> zeroinitializer
  %39 = getelementptr inbounds float, float* %15, i64 %33
  %40 = bitcast float* %39 to <8 x float>*
  %wide.load2 = load <8 x float>, <8 x float>* %40, align 4, !tbaa !12, !llvm.access.group !16
  %41 = fmul <8 x float> %38, %wide.load2
  %42 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %31, <8 x float> %wide.load, <8 x float> %41)
  %43 = add nsw i32 %mul.i.i, %28
  %44 = sext i32 %43 to i64
  %45 = getelementptr inbounds float, float* %7, i64 %44
  %46 = bitcast float* %45 to <8 x float>*
  %wide.load3 = load <8 x float>, <8 x float>* %46, align 4, !tbaa !12, !llvm.access.group !16
  %47 = fadd <8 x float> %wide.load3, %42
  %48 = bitcast float* %45 to <8 x float>*
  store <8 x float> %47, <8 x float>* %48, align 4, !tbaa !12, !llvm.access.group !16
  %49 = or i64 %mul.i.i.i, 8
  %50 = trunc i64 %49 to i32
  %51 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %52 = insertelement <8 x float> undef, float %51, i32 0
  %53 = shufflevector <8 x float> %52, <8 x float> undef, <8 x i32> zeroinitializer
  %54 = shl i64 %49, 32
  %55 = ashr exact i64 %54, 32
  %56 = getelementptr inbounds float, float* %11, i64 %55
  %57 = bitcast float* %56 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %57, align 4, !tbaa !12, !llvm.access.group !16
  %58 = load float, float* %arrayidx6.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %59 = insertelement <8 x float> undef, float %58, i32 0
  %60 = shufflevector <8 x float> %59, <8 x float> undef, <8 x i32> zeroinitializer
  %61 = getelementptr inbounds float, float* %15, i64 %55
  %62 = bitcast float* %61 to <8 x float>*
  %wide.load2.1 = load <8 x float>, <8 x float>* %62, align 4, !tbaa !12, !llvm.access.group !16
  %63 = fmul <8 x float> %60, %wide.load2.1
  %64 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %53, <8 x float> %wide.load.1, <8 x float> %63)
  %65 = add nsw i32 %mul.i.i, %50
  %66 = sext i32 %65 to i64
  %67 = getelementptr inbounds float, float* %7, i64 %66
  %68 = bitcast float* %67 to <8 x float>*
  %wide.load3.1 = load <8 x float>, <8 x float>* %68, align 4, !tbaa !12, !llvm.access.group !16
  %69 = fadd <8 x float> %wide.load3.1, %64
  %70 = bitcast float* %67 to <8 x float>*
  store <8 x float> %69, <8 x float>* %70, align 4, !tbaa !12, !llvm.access.group !16
  %71 = or i64 %mul.i.i.i, 16
  %72 = trunc i64 %71 to i32
  %73 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %74 = insertelement <8 x float> undef, float %73, i32 0
  %75 = shufflevector <8 x float> %74, <8 x float> undef, <8 x i32> zeroinitializer
  %76 = shl i64 %71, 32
  %77 = ashr exact i64 %76, 32
  %78 = getelementptr inbounds float, float* %11, i64 %77
  %79 = bitcast float* %78 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %79, align 4, !tbaa !12, !llvm.access.group !16
  %80 = load float, float* %arrayidx6.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %81 = insertelement <8 x float> undef, float %80, i32 0
  %82 = shufflevector <8 x float> %81, <8 x float> undef, <8 x i32> zeroinitializer
  %83 = getelementptr inbounds float, float* %15, i64 %77
  %84 = bitcast float* %83 to <8 x float>*
  %wide.load2.2 = load <8 x float>, <8 x float>* %84, align 4, !tbaa !12, !llvm.access.group !16
  %85 = fmul <8 x float> %82, %wide.load2.2
  %86 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %75, <8 x float> %wide.load.2, <8 x float> %85)
  %87 = add nsw i32 %mul.i.i, %72
  %88 = sext i32 %87 to i64
  %89 = getelementptr inbounds float, float* %7, i64 %88
  %90 = bitcast float* %89 to <8 x float>*
  %wide.load3.2 = load <8 x float>, <8 x float>* %90, align 4, !tbaa !12, !llvm.access.group !16
  %91 = fadd <8 x float> %wide.load3.2, %86
  %92 = bitcast float* %89 to <8 x float>*
  store <8 x float> %91, <8 x float>* %92, align 4, !tbaa !12, !llvm.access.group !16
  %93 = or i64 %mul.i.i.i, 24
  %94 = trunc i64 %93 to i32
  %95 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %96 = insertelement <8 x float> undef, float %95, i32 0
  %97 = shufflevector <8 x float> %96, <8 x float> undef, <8 x i32> zeroinitializer
  %98 = shl i64 %93, 32
  %99 = ashr exact i64 %98, 32
  %100 = getelementptr inbounds float, float* %11, i64 %99
  %101 = bitcast float* %100 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %101, align 4, !tbaa !12, !llvm.access.group !16
  %102 = load float, float* %arrayidx6.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %103 = insertelement <8 x float> undef, float %102, i32 0
  %104 = shufflevector <8 x float> %103, <8 x float> undef, <8 x i32> zeroinitializer
  %105 = getelementptr inbounds float, float* %15, i64 %99
  %106 = bitcast float* %105 to <8 x float>*
  %wide.load2.3 = load <8 x float>, <8 x float>* %106, align 4, !tbaa !12, !llvm.access.group !16
  %107 = fmul <8 x float> %104, %wide.load2.3
  %108 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %97, <8 x float> %wide.load.3, <8 x float> %107)
  %109 = add nsw i32 %mul.i.i, %94
  %110 = sext i32 %109 to i64
  %111 = getelementptr inbounds float, float* %7, i64 %110
  %112 = bitcast float* %111 to <8 x float>*
  %wide.load3.3 = load <8 x float>, <8 x float>* %112, align 4, !tbaa !12, !llvm.access.group !16
  %113 = fadd <8 x float> %wide.load3.3, %108
  %114 = bitcast float* %111 to <8 x float>*
  store <8 x float> %113, <8 x float>* %114, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.1 = or i64 %mul3.i.i.i, 1
  %conv2.i.i.1 = trunc i64 %add6.i.i.i.1 to i32
  %sext.i.i.1 = shl i64 %add6.i.i.i.1, 32
  %idxprom.i.i.1 = ashr exact i64 %sext.i.i.1, 32
  %arrayidx.i.i.1 = getelementptr inbounds float, float* %19, i64 %idxprom.i.i.1
  %arrayidx6.i.i.1 = getelementptr inbounds float, float* %23, i64 %idxprom.i.i.1
  %mul.i.i.1 = mul nsw i32 %27, %conv2.i.i.1
  %115 = trunc i64 %mul.i.i.i to i32
  %116 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %117 = insertelement <8 x float> undef, float %116, i32 0
  %118 = shufflevector <8 x float> %117, <8 x float> undef, <8 x i32> zeroinitializer
  %119 = shl i64 %2, 37
  %120 = ashr exact i64 %119, 32
  %121 = getelementptr inbounds float, float* %11, i64 %120
  %122 = bitcast float* %121 to <8 x float>*
  %wide.load12 = load <8 x float>, <8 x float>* %122, align 4, !tbaa !12, !llvm.access.group !16
  %123 = load float, float* %arrayidx6.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %124 = insertelement <8 x float> undef, float %123, i32 0
  %125 = shufflevector <8 x float> %124, <8 x float> undef, <8 x i32> zeroinitializer
  %126 = getelementptr inbounds float, float* %15, i64 %120
  %127 = bitcast float* %126 to <8 x float>*
  %wide.load13 = load <8 x float>, <8 x float>* %127, align 4, !tbaa !12, !llvm.access.group !16
  %128 = fmul <8 x float> %125, %wide.load13
  %129 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %118, <8 x float> %wide.load12, <8 x float> %128)
  %130 = add nsw i32 %mul.i.i.1, %115
  %131 = sext i32 %130 to i64
  %132 = getelementptr inbounds float, float* %7, i64 %131
  %133 = bitcast float* %132 to <8 x float>*
  %wide.load14 = load <8 x float>, <8 x float>* %133, align 4, !tbaa !12, !llvm.access.group !16
  %134 = fadd <8 x float> %wide.load14, %129
  %135 = bitcast float* %132 to <8 x float>*
  store <8 x float> %134, <8 x float>* %135, align 4, !tbaa !12, !llvm.access.group !16
  %136 = or i64 %mul.i.i.i, 8
  %137 = trunc i64 %136 to i32
  %138 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %139 = insertelement <8 x float> undef, float %138, i32 0
  %140 = shufflevector <8 x float> %139, <8 x float> undef, <8 x i32> zeroinitializer
  %141 = shl i64 %136, 32
  %142 = ashr exact i64 %141, 32
  %143 = getelementptr inbounds float, float* %11, i64 %142
  %144 = bitcast float* %143 to <8 x float>*
  %wide.load12.1 = load <8 x float>, <8 x float>* %144, align 4, !tbaa !12, !llvm.access.group !16
  %145 = load float, float* %arrayidx6.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %146 = insertelement <8 x float> undef, float %145, i32 0
  %147 = shufflevector <8 x float> %146, <8 x float> undef, <8 x i32> zeroinitializer
  %148 = getelementptr inbounds float, float* %15, i64 %142
  %149 = bitcast float* %148 to <8 x float>*
  %wide.load13.1 = load <8 x float>, <8 x float>* %149, align 4, !tbaa !12, !llvm.access.group !16
  %150 = fmul <8 x float> %147, %wide.load13.1
  %151 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %140, <8 x float> %wide.load12.1, <8 x float> %150)
  %152 = add nsw i32 %mul.i.i.1, %137
  %153 = sext i32 %152 to i64
  %154 = getelementptr inbounds float, float* %7, i64 %153
  %155 = bitcast float* %154 to <8 x float>*
  %wide.load14.1 = load <8 x float>, <8 x float>* %155, align 4, !tbaa !12, !llvm.access.group !16
  %156 = fadd <8 x float> %wide.load14.1, %151
  %157 = bitcast float* %154 to <8 x float>*
  store <8 x float> %156, <8 x float>* %157, align 4, !tbaa !12, !llvm.access.group !16
  %158 = or i64 %mul.i.i.i, 16
  %159 = trunc i64 %158 to i32
  %160 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %161 = insertelement <8 x float> undef, float %160, i32 0
  %162 = shufflevector <8 x float> %161, <8 x float> undef, <8 x i32> zeroinitializer
  %163 = shl i64 %158, 32
  %164 = ashr exact i64 %163, 32
  %165 = getelementptr inbounds float, float* %11, i64 %164
  %166 = bitcast float* %165 to <8 x float>*
  %wide.load12.2 = load <8 x float>, <8 x float>* %166, align 4, !tbaa !12, !llvm.access.group !16
  %167 = load float, float* %arrayidx6.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %168 = insertelement <8 x float> undef, float %167, i32 0
  %169 = shufflevector <8 x float> %168, <8 x float> undef, <8 x i32> zeroinitializer
  %170 = getelementptr inbounds float, float* %15, i64 %164
  %171 = bitcast float* %170 to <8 x float>*
  %wide.load13.2 = load <8 x float>, <8 x float>* %171, align 4, !tbaa !12, !llvm.access.group !16
  %172 = fmul <8 x float> %169, %wide.load13.2
  %173 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %162, <8 x float> %wide.load12.2, <8 x float> %172)
  %174 = add nsw i32 %mul.i.i.1, %159
  %175 = sext i32 %174 to i64
  %176 = getelementptr inbounds float, float* %7, i64 %175
  %177 = bitcast float* %176 to <8 x float>*
  %wide.load14.2 = load <8 x float>, <8 x float>* %177, align 4, !tbaa !12, !llvm.access.group !16
  %178 = fadd <8 x float> %wide.load14.2, %173
  %179 = bitcast float* %176 to <8 x float>*
  store <8 x float> %178, <8 x float>* %179, align 4, !tbaa !12, !llvm.access.group !16
  %180 = or i64 %mul.i.i.i, 24
  %181 = trunc i64 %180 to i32
  %182 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %183 = insertelement <8 x float> undef, float %182, i32 0
  %184 = shufflevector <8 x float> %183, <8 x float> undef, <8 x i32> zeroinitializer
  %185 = shl i64 %180, 32
  %186 = ashr exact i64 %185, 32
  %187 = getelementptr inbounds float, float* %11, i64 %186
  %188 = bitcast float* %187 to <8 x float>*
  %wide.load12.3 = load <8 x float>, <8 x float>* %188, align 4, !tbaa !12, !llvm.access.group !16
  %189 = load float, float* %arrayidx6.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %190 = insertelement <8 x float> undef, float %189, i32 0
  %191 = shufflevector <8 x float> %190, <8 x float> undef, <8 x i32> zeroinitializer
  %192 = getelementptr inbounds float, float* %15, i64 %186
  %193 = bitcast float* %192 to <8 x float>*
  %wide.load13.3 = load <8 x float>, <8 x float>* %193, align 4, !tbaa !12, !llvm.access.group !16
  %194 = fmul <8 x float> %191, %wide.load13.3
  %195 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %184, <8 x float> %wide.load12.3, <8 x float> %194)
  %196 = add nsw i32 %mul.i.i.1, %181
  %197 = sext i32 %196 to i64
  %198 = getelementptr inbounds float, float* %7, i64 %197
  %199 = bitcast float* %198 to <8 x float>*
  %wide.load14.3 = load <8 x float>, <8 x float>* %199, align 4, !tbaa !12, !llvm.access.group !16
  %200 = fadd <8 x float> %wide.load14.3, %195
  %201 = bitcast float* %198 to <8 x float>*
  store <8 x float> %200, <8 x float>* %201, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.2 = or i64 %mul3.i.i.i, 2
  %conv2.i.i.2 = trunc i64 %add6.i.i.i.2 to i32
  %sext.i.i.2 = shl i64 %add6.i.i.i.2, 32
  %idxprom.i.i.2 = ashr exact i64 %sext.i.i.2, 32
  %arrayidx.i.i.2 = getelementptr inbounds float, float* %19, i64 %idxprom.i.i.2
  %arrayidx6.i.i.2 = getelementptr inbounds float, float* %23, i64 %idxprom.i.i.2
  %mul.i.i.2 = mul nsw i32 %27, %conv2.i.i.2
  %202 = trunc i64 %mul.i.i.i to i32
  %203 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %204 = insertelement <8 x float> undef, float %203, i32 0
  %205 = shufflevector <8 x float> %204, <8 x float> undef, <8 x i32> zeroinitializer
  %206 = shl i64 %2, 37
  %207 = ashr exact i64 %206, 32
  %208 = getelementptr inbounds float, float* %11, i64 %207
  %209 = bitcast float* %208 to <8 x float>*
  %wide.load23 = load <8 x float>, <8 x float>* %209, align 4, !tbaa !12, !llvm.access.group !16
  %210 = load float, float* %arrayidx6.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %211 = insertelement <8 x float> undef, float %210, i32 0
  %212 = shufflevector <8 x float> %211, <8 x float> undef, <8 x i32> zeroinitializer
  %213 = getelementptr inbounds float, float* %15, i64 %207
  %214 = bitcast float* %213 to <8 x float>*
  %wide.load24 = load <8 x float>, <8 x float>* %214, align 4, !tbaa !12, !llvm.access.group !16
  %215 = fmul <8 x float> %212, %wide.load24
  %216 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %205, <8 x float> %wide.load23, <8 x float> %215)
  %217 = add nsw i32 %mul.i.i.2, %202
  %218 = sext i32 %217 to i64
  %219 = getelementptr inbounds float, float* %7, i64 %218
  %220 = bitcast float* %219 to <8 x float>*
  %wide.load25 = load <8 x float>, <8 x float>* %220, align 4, !tbaa !12, !llvm.access.group !16
  %221 = fadd <8 x float> %wide.load25, %216
  %222 = bitcast float* %219 to <8 x float>*
  store <8 x float> %221, <8 x float>* %222, align 4, !tbaa !12, !llvm.access.group !16
  %223 = or i64 %mul.i.i.i, 8
  %224 = trunc i64 %223 to i32
  %225 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %226 = insertelement <8 x float> undef, float %225, i32 0
  %227 = shufflevector <8 x float> %226, <8 x float> undef, <8 x i32> zeroinitializer
  %228 = shl i64 %223, 32
  %229 = ashr exact i64 %228, 32
  %230 = getelementptr inbounds float, float* %11, i64 %229
  %231 = bitcast float* %230 to <8 x float>*
  %wide.load23.1 = load <8 x float>, <8 x float>* %231, align 4, !tbaa !12, !llvm.access.group !16
  %232 = load float, float* %arrayidx6.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %233 = insertelement <8 x float> undef, float %232, i32 0
  %234 = shufflevector <8 x float> %233, <8 x float> undef, <8 x i32> zeroinitializer
  %235 = getelementptr inbounds float, float* %15, i64 %229
  %236 = bitcast float* %235 to <8 x float>*
  %wide.load24.1 = load <8 x float>, <8 x float>* %236, align 4, !tbaa !12, !llvm.access.group !16
  %237 = fmul <8 x float> %234, %wide.load24.1
  %238 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %227, <8 x float> %wide.load23.1, <8 x float> %237)
  %239 = add nsw i32 %mul.i.i.2, %224
  %240 = sext i32 %239 to i64
  %241 = getelementptr inbounds float, float* %7, i64 %240
  %242 = bitcast float* %241 to <8 x float>*
  %wide.load25.1 = load <8 x float>, <8 x float>* %242, align 4, !tbaa !12, !llvm.access.group !16
  %243 = fadd <8 x float> %wide.load25.1, %238
  %244 = bitcast float* %241 to <8 x float>*
  store <8 x float> %243, <8 x float>* %244, align 4, !tbaa !12, !llvm.access.group !16
  %245 = or i64 %mul.i.i.i, 16
  %246 = trunc i64 %245 to i32
  %247 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %248 = insertelement <8 x float> undef, float %247, i32 0
  %249 = shufflevector <8 x float> %248, <8 x float> undef, <8 x i32> zeroinitializer
  %250 = shl i64 %245, 32
  %251 = ashr exact i64 %250, 32
  %252 = getelementptr inbounds float, float* %11, i64 %251
  %253 = bitcast float* %252 to <8 x float>*
  %wide.load23.2 = load <8 x float>, <8 x float>* %253, align 4, !tbaa !12, !llvm.access.group !16
  %254 = load float, float* %arrayidx6.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %255 = insertelement <8 x float> undef, float %254, i32 0
  %256 = shufflevector <8 x float> %255, <8 x float> undef, <8 x i32> zeroinitializer
  %257 = getelementptr inbounds float, float* %15, i64 %251
  %258 = bitcast float* %257 to <8 x float>*
  %wide.load24.2 = load <8 x float>, <8 x float>* %258, align 4, !tbaa !12, !llvm.access.group !16
  %259 = fmul <8 x float> %256, %wide.load24.2
  %260 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %249, <8 x float> %wide.load23.2, <8 x float> %259)
  %261 = add nsw i32 %mul.i.i.2, %246
  %262 = sext i32 %261 to i64
  %263 = getelementptr inbounds float, float* %7, i64 %262
  %264 = bitcast float* %263 to <8 x float>*
  %wide.load25.2 = load <8 x float>, <8 x float>* %264, align 4, !tbaa !12, !llvm.access.group !16
  %265 = fadd <8 x float> %wide.load25.2, %260
  %266 = bitcast float* %263 to <8 x float>*
  store <8 x float> %265, <8 x float>* %266, align 4, !tbaa !12, !llvm.access.group !16
  %267 = or i64 %mul.i.i.i, 24
  %268 = trunc i64 %267 to i32
  %269 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %270 = insertelement <8 x float> undef, float %269, i32 0
  %271 = shufflevector <8 x float> %270, <8 x float> undef, <8 x i32> zeroinitializer
  %272 = shl i64 %267, 32
  %273 = ashr exact i64 %272, 32
  %274 = getelementptr inbounds float, float* %11, i64 %273
  %275 = bitcast float* %274 to <8 x float>*
  %wide.load23.3 = load <8 x float>, <8 x float>* %275, align 4, !tbaa !12, !llvm.access.group !16
  %276 = load float, float* %arrayidx6.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %277 = insertelement <8 x float> undef, float %276, i32 0
  %278 = shufflevector <8 x float> %277, <8 x float> undef, <8 x i32> zeroinitializer
  %279 = getelementptr inbounds float, float* %15, i64 %273
  %280 = bitcast float* %279 to <8 x float>*
  %wide.load24.3 = load <8 x float>, <8 x float>* %280, align 4, !tbaa !12, !llvm.access.group !16
  %281 = fmul <8 x float> %278, %wide.load24.3
  %282 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %271, <8 x float> %wide.load23.3, <8 x float> %281)
  %283 = add nsw i32 %mul.i.i.2, %268
  %284 = sext i32 %283 to i64
  %285 = getelementptr inbounds float, float* %7, i64 %284
  %286 = bitcast float* %285 to <8 x float>*
  %wide.load25.3 = load <8 x float>, <8 x float>* %286, align 4, !tbaa !12, !llvm.access.group !16
  %287 = fadd <8 x float> %wide.load25.3, %282
  %288 = bitcast float* %285 to <8 x float>*
  store <8 x float> %287, <8 x float>* %288, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.3 = or i64 %mul3.i.i.i, 3
  %conv2.i.i.3 = trunc i64 %add6.i.i.i.3 to i32
  %sext.i.i.3 = shl i64 %add6.i.i.i.3, 32
  %idxprom.i.i.3 = ashr exact i64 %sext.i.i.3, 32
  %arrayidx.i.i.3 = getelementptr inbounds float, float* %19, i64 %idxprom.i.i.3
  %arrayidx6.i.i.3 = getelementptr inbounds float, float* %23, i64 %idxprom.i.i.3
  %mul.i.i.3 = mul nsw i32 %27, %conv2.i.i.3
  %289 = trunc i64 %mul.i.i.i to i32
  %290 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %291 = insertelement <8 x float> undef, float %290, i32 0
  %292 = shufflevector <8 x float> %291, <8 x float> undef, <8 x i32> zeroinitializer
  %293 = shl i64 %2, 37
  %294 = ashr exact i64 %293, 32
  %295 = getelementptr inbounds float, float* %11, i64 %294
  %296 = bitcast float* %295 to <8 x float>*
  %wide.load34 = load <8 x float>, <8 x float>* %296, align 4, !tbaa !12, !llvm.access.group !16
  %297 = load float, float* %arrayidx6.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %298 = insertelement <8 x float> undef, float %297, i32 0
  %299 = shufflevector <8 x float> %298, <8 x float> undef, <8 x i32> zeroinitializer
  %300 = getelementptr inbounds float, float* %15, i64 %294
  %301 = bitcast float* %300 to <8 x float>*
  %wide.load35 = load <8 x float>, <8 x float>* %301, align 4, !tbaa !12, !llvm.access.group !16
  %302 = fmul <8 x float> %299, %wide.load35
  %303 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %292, <8 x float> %wide.load34, <8 x float> %302)
  %304 = add nsw i32 %mul.i.i.3, %289
  %305 = sext i32 %304 to i64
  %306 = getelementptr inbounds float, float* %7, i64 %305
  %307 = bitcast float* %306 to <8 x float>*
  %wide.load36 = load <8 x float>, <8 x float>* %307, align 4, !tbaa !12, !llvm.access.group !16
  %308 = fadd <8 x float> %wide.load36, %303
  %309 = bitcast float* %306 to <8 x float>*
  store <8 x float> %308, <8 x float>* %309, align 4, !tbaa !12, !llvm.access.group !16
  %310 = or i64 %mul.i.i.i, 8
  %311 = trunc i64 %310 to i32
  %312 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %313 = insertelement <8 x float> undef, float %312, i32 0
  %314 = shufflevector <8 x float> %313, <8 x float> undef, <8 x i32> zeroinitializer
  %315 = shl i64 %310, 32
  %316 = ashr exact i64 %315, 32
  %317 = getelementptr inbounds float, float* %11, i64 %316
  %318 = bitcast float* %317 to <8 x float>*
  %wide.load34.1 = load <8 x float>, <8 x float>* %318, align 4, !tbaa !12, !llvm.access.group !16
  %319 = load float, float* %arrayidx6.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %320 = insertelement <8 x float> undef, float %319, i32 0
  %321 = shufflevector <8 x float> %320, <8 x float> undef, <8 x i32> zeroinitializer
  %322 = getelementptr inbounds float, float* %15, i64 %316
  %323 = bitcast float* %322 to <8 x float>*
  %wide.load35.1 = load <8 x float>, <8 x float>* %323, align 4, !tbaa !12, !llvm.access.group !16
  %324 = fmul <8 x float> %321, %wide.load35.1
  %325 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %314, <8 x float> %wide.load34.1, <8 x float> %324)
  %326 = add nsw i32 %mul.i.i.3, %311
  %327 = sext i32 %326 to i64
  %328 = getelementptr inbounds float, float* %7, i64 %327
  %329 = bitcast float* %328 to <8 x float>*
  %wide.load36.1 = load <8 x float>, <8 x float>* %329, align 4, !tbaa !12, !llvm.access.group !16
  %330 = fadd <8 x float> %wide.load36.1, %325
  %331 = bitcast float* %328 to <8 x float>*
  store <8 x float> %330, <8 x float>* %331, align 4, !tbaa !12, !llvm.access.group !16
  %332 = or i64 %mul.i.i.i, 16
  %333 = trunc i64 %332 to i32
  %334 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %335 = insertelement <8 x float> undef, float %334, i32 0
  %336 = shufflevector <8 x float> %335, <8 x float> undef, <8 x i32> zeroinitializer
  %337 = shl i64 %332, 32
  %338 = ashr exact i64 %337, 32
  %339 = getelementptr inbounds float, float* %11, i64 %338
  %340 = bitcast float* %339 to <8 x float>*
  %wide.load34.2 = load <8 x float>, <8 x float>* %340, align 4, !tbaa !12, !llvm.access.group !16
  %341 = load float, float* %arrayidx6.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %342 = insertelement <8 x float> undef, float %341, i32 0
  %343 = shufflevector <8 x float> %342, <8 x float> undef, <8 x i32> zeroinitializer
  %344 = getelementptr inbounds float, float* %15, i64 %338
  %345 = bitcast float* %344 to <8 x float>*
  %wide.load35.2 = load <8 x float>, <8 x float>* %345, align 4, !tbaa !12, !llvm.access.group !16
  %346 = fmul <8 x float> %343, %wide.load35.2
  %347 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %336, <8 x float> %wide.load34.2, <8 x float> %346)
  %348 = add nsw i32 %mul.i.i.3, %333
  %349 = sext i32 %348 to i64
  %350 = getelementptr inbounds float, float* %7, i64 %349
  %351 = bitcast float* %350 to <8 x float>*
  %wide.load36.2 = load <8 x float>, <8 x float>* %351, align 4, !tbaa !12, !llvm.access.group !16
  %352 = fadd <8 x float> %wide.load36.2, %347
  %353 = bitcast float* %350 to <8 x float>*
  store <8 x float> %352, <8 x float>* %353, align 4, !tbaa !12, !llvm.access.group !16
  %354 = or i64 %mul.i.i.i, 24
  %355 = trunc i64 %354 to i32
  %356 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %357 = insertelement <8 x float> undef, float %356, i32 0
  %358 = shufflevector <8 x float> %357, <8 x float> undef, <8 x i32> zeroinitializer
  %359 = shl i64 %354, 32
  %360 = ashr exact i64 %359, 32
  %361 = getelementptr inbounds float, float* %11, i64 %360
  %362 = bitcast float* %361 to <8 x float>*
  %wide.load34.3 = load <8 x float>, <8 x float>* %362, align 4, !tbaa !12, !llvm.access.group !16
  %363 = load float, float* %arrayidx6.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %364 = insertelement <8 x float> undef, float %363, i32 0
  %365 = shufflevector <8 x float> %364, <8 x float> undef, <8 x i32> zeroinitializer
  %366 = getelementptr inbounds float, float* %15, i64 %360
  %367 = bitcast float* %366 to <8 x float>*
  %wide.load35.3 = load <8 x float>, <8 x float>* %367, align 4, !tbaa !12, !llvm.access.group !16
  %368 = fmul <8 x float> %365, %wide.load35.3
  %369 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %358, <8 x float> %wide.load34.3, <8 x float> %368)
  %370 = add nsw i32 %mul.i.i.3, %355
  %371 = sext i32 %370 to i64
  %372 = getelementptr inbounds float, float* %7, i64 %371
  %373 = bitcast float* %372 to <8 x float>*
  %wide.load36.3 = load <8 x float>, <8 x float>* %373, align 4, !tbaa !12, !llvm.access.group !16
  %374 = fadd <8 x float> %wide.load36.3, %369
  %375 = bitcast float* %372 to <8 x float>*
  store <8 x float> %374, <8 x float>* %375, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.4 = or i64 %mul3.i.i.i, 4
  %conv2.i.i.4 = trunc i64 %add6.i.i.i.4 to i32
  %sext.i.i.4 = shl i64 %add6.i.i.i.4, 32
  %idxprom.i.i.4 = ashr exact i64 %sext.i.i.4, 32
  %arrayidx.i.i.4 = getelementptr inbounds float, float* %19, i64 %idxprom.i.i.4
  %arrayidx6.i.i.4 = getelementptr inbounds float, float* %23, i64 %idxprom.i.i.4
  %mul.i.i.4 = mul nsw i32 %27, %conv2.i.i.4
  %376 = trunc i64 %mul.i.i.i to i32
  %377 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %378 = insertelement <8 x float> undef, float %377, i32 0
  %379 = shufflevector <8 x float> %378, <8 x float> undef, <8 x i32> zeroinitializer
  %380 = shl i64 %2, 37
  %381 = ashr exact i64 %380, 32
  %382 = getelementptr inbounds float, float* %11, i64 %381
  %383 = bitcast float* %382 to <8 x float>*
  %wide.load45 = load <8 x float>, <8 x float>* %383, align 4, !tbaa !12, !llvm.access.group !16
  %384 = load float, float* %arrayidx6.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %385 = insertelement <8 x float> undef, float %384, i32 0
  %386 = shufflevector <8 x float> %385, <8 x float> undef, <8 x i32> zeroinitializer
  %387 = getelementptr inbounds float, float* %15, i64 %381
  %388 = bitcast float* %387 to <8 x float>*
  %wide.load46 = load <8 x float>, <8 x float>* %388, align 4, !tbaa !12, !llvm.access.group !16
  %389 = fmul <8 x float> %386, %wide.load46
  %390 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %379, <8 x float> %wide.load45, <8 x float> %389)
  %391 = add nsw i32 %mul.i.i.4, %376
  %392 = sext i32 %391 to i64
  %393 = getelementptr inbounds float, float* %7, i64 %392
  %394 = bitcast float* %393 to <8 x float>*
  %wide.load47 = load <8 x float>, <8 x float>* %394, align 4, !tbaa !12, !llvm.access.group !16
  %395 = fadd <8 x float> %wide.load47, %390
  %396 = bitcast float* %393 to <8 x float>*
  store <8 x float> %395, <8 x float>* %396, align 4, !tbaa !12, !llvm.access.group !16
  %397 = or i64 %mul.i.i.i, 8
  %398 = trunc i64 %397 to i32
  %399 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %400 = insertelement <8 x float> undef, float %399, i32 0
  %401 = shufflevector <8 x float> %400, <8 x float> undef, <8 x i32> zeroinitializer
  %402 = shl i64 %397, 32
  %403 = ashr exact i64 %402, 32
  %404 = getelementptr inbounds float, float* %11, i64 %403
  %405 = bitcast float* %404 to <8 x float>*
  %wide.load45.1 = load <8 x float>, <8 x float>* %405, align 4, !tbaa !12, !llvm.access.group !16
  %406 = load float, float* %arrayidx6.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %407 = insertelement <8 x float> undef, float %406, i32 0
  %408 = shufflevector <8 x float> %407, <8 x float> undef, <8 x i32> zeroinitializer
  %409 = getelementptr inbounds float, float* %15, i64 %403
  %410 = bitcast float* %409 to <8 x float>*
  %wide.load46.1 = load <8 x float>, <8 x float>* %410, align 4, !tbaa !12, !llvm.access.group !16
  %411 = fmul <8 x float> %408, %wide.load46.1
  %412 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %401, <8 x float> %wide.load45.1, <8 x float> %411)
  %413 = add nsw i32 %mul.i.i.4, %398
  %414 = sext i32 %413 to i64
  %415 = getelementptr inbounds float, float* %7, i64 %414
  %416 = bitcast float* %415 to <8 x float>*
  %wide.load47.1 = load <8 x float>, <8 x float>* %416, align 4, !tbaa !12, !llvm.access.group !16
  %417 = fadd <8 x float> %wide.load47.1, %412
  %418 = bitcast float* %415 to <8 x float>*
  store <8 x float> %417, <8 x float>* %418, align 4, !tbaa !12, !llvm.access.group !16
  %419 = or i64 %mul.i.i.i, 16
  %420 = trunc i64 %419 to i32
  %421 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %422 = insertelement <8 x float> undef, float %421, i32 0
  %423 = shufflevector <8 x float> %422, <8 x float> undef, <8 x i32> zeroinitializer
  %424 = shl i64 %419, 32
  %425 = ashr exact i64 %424, 32
  %426 = getelementptr inbounds float, float* %11, i64 %425
  %427 = bitcast float* %426 to <8 x float>*
  %wide.load45.2 = load <8 x float>, <8 x float>* %427, align 4, !tbaa !12, !llvm.access.group !16
  %428 = load float, float* %arrayidx6.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %429 = insertelement <8 x float> undef, float %428, i32 0
  %430 = shufflevector <8 x float> %429, <8 x float> undef, <8 x i32> zeroinitializer
  %431 = getelementptr inbounds float, float* %15, i64 %425
  %432 = bitcast float* %431 to <8 x float>*
  %wide.load46.2 = load <8 x float>, <8 x float>* %432, align 4, !tbaa !12, !llvm.access.group !16
  %433 = fmul <8 x float> %430, %wide.load46.2
  %434 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %423, <8 x float> %wide.load45.2, <8 x float> %433)
  %435 = add nsw i32 %mul.i.i.4, %420
  %436 = sext i32 %435 to i64
  %437 = getelementptr inbounds float, float* %7, i64 %436
  %438 = bitcast float* %437 to <8 x float>*
  %wide.load47.2 = load <8 x float>, <8 x float>* %438, align 4, !tbaa !12, !llvm.access.group !16
  %439 = fadd <8 x float> %wide.load47.2, %434
  %440 = bitcast float* %437 to <8 x float>*
  store <8 x float> %439, <8 x float>* %440, align 4, !tbaa !12, !llvm.access.group !16
  %441 = or i64 %mul.i.i.i, 24
  %442 = trunc i64 %441 to i32
  %443 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %444 = insertelement <8 x float> undef, float %443, i32 0
  %445 = shufflevector <8 x float> %444, <8 x float> undef, <8 x i32> zeroinitializer
  %446 = shl i64 %441, 32
  %447 = ashr exact i64 %446, 32
  %448 = getelementptr inbounds float, float* %11, i64 %447
  %449 = bitcast float* %448 to <8 x float>*
  %wide.load45.3 = load <8 x float>, <8 x float>* %449, align 4, !tbaa !12, !llvm.access.group !16
  %450 = load float, float* %arrayidx6.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %451 = insertelement <8 x float> undef, float %450, i32 0
  %452 = shufflevector <8 x float> %451, <8 x float> undef, <8 x i32> zeroinitializer
  %453 = getelementptr inbounds float, float* %15, i64 %447
  %454 = bitcast float* %453 to <8 x float>*
  %wide.load46.3 = load <8 x float>, <8 x float>* %454, align 4, !tbaa !12, !llvm.access.group !16
  %455 = fmul <8 x float> %452, %wide.load46.3
  %456 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %445, <8 x float> %wide.load45.3, <8 x float> %455)
  %457 = add nsw i32 %mul.i.i.4, %442
  %458 = sext i32 %457 to i64
  %459 = getelementptr inbounds float, float* %7, i64 %458
  %460 = bitcast float* %459 to <8 x float>*
  %wide.load47.3 = load <8 x float>, <8 x float>* %460, align 4, !tbaa !12, !llvm.access.group !16
  %461 = fadd <8 x float> %wide.load47.3, %456
  %462 = bitcast float* %459 to <8 x float>*
  store <8 x float> %461, <8 x float>* %462, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.5 = or i64 %mul3.i.i.i, 5
  %conv2.i.i.5 = trunc i64 %add6.i.i.i.5 to i32
  %sext.i.i.5 = shl i64 %add6.i.i.i.5, 32
  %idxprom.i.i.5 = ashr exact i64 %sext.i.i.5, 32
  %arrayidx.i.i.5 = getelementptr inbounds float, float* %19, i64 %idxprom.i.i.5
  %arrayidx6.i.i.5 = getelementptr inbounds float, float* %23, i64 %idxprom.i.i.5
  %mul.i.i.5 = mul nsw i32 %27, %conv2.i.i.5
  %463 = trunc i64 %mul.i.i.i to i32
  %464 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %465 = insertelement <8 x float> undef, float %464, i32 0
  %466 = shufflevector <8 x float> %465, <8 x float> undef, <8 x i32> zeroinitializer
  %467 = shl i64 %2, 37
  %468 = ashr exact i64 %467, 32
  %469 = getelementptr inbounds float, float* %11, i64 %468
  %470 = bitcast float* %469 to <8 x float>*
  %wide.load56 = load <8 x float>, <8 x float>* %470, align 4, !tbaa !12, !llvm.access.group !16
  %471 = load float, float* %arrayidx6.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %472 = insertelement <8 x float> undef, float %471, i32 0
  %473 = shufflevector <8 x float> %472, <8 x float> undef, <8 x i32> zeroinitializer
  %474 = getelementptr inbounds float, float* %15, i64 %468
  %475 = bitcast float* %474 to <8 x float>*
  %wide.load57 = load <8 x float>, <8 x float>* %475, align 4, !tbaa !12, !llvm.access.group !16
  %476 = fmul <8 x float> %473, %wide.load57
  %477 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %466, <8 x float> %wide.load56, <8 x float> %476)
  %478 = add nsw i32 %mul.i.i.5, %463
  %479 = sext i32 %478 to i64
  %480 = getelementptr inbounds float, float* %7, i64 %479
  %481 = bitcast float* %480 to <8 x float>*
  %wide.load58 = load <8 x float>, <8 x float>* %481, align 4, !tbaa !12, !llvm.access.group !16
  %482 = fadd <8 x float> %wide.load58, %477
  %483 = bitcast float* %480 to <8 x float>*
  store <8 x float> %482, <8 x float>* %483, align 4, !tbaa !12, !llvm.access.group !16
  %484 = or i64 %mul.i.i.i, 8
  %485 = trunc i64 %484 to i32
  %486 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %487 = insertelement <8 x float> undef, float %486, i32 0
  %488 = shufflevector <8 x float> %487, <8 x float> undef, <8 x i32> zeroinitializer
  %489 = shl i64 %484, 32
  %490 = ashr exact i64 %489, 32
  %491 = getelementptr inbounds float, float* %11, i64 %490
  %492 = bitcast float* %491 to <8 x float>*
  %wide.load56.1 = load <8 x float>, <8 x float>* %492, align 4, !tbaa !12, !llvm.access.group !16
  %493 = load float, float* %arrayidx6.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %494 = insertelement <8 x float> undef, float %493, i32 0
  %495 = shufflevector <8 x float> %494, <8 x float> undef, <8 x i32> zeroinitializer
  %496 = getelementptr inbounds float, float* %15, i64 %490
  %497 = bitcast float* %496 to <8 x float>*
  %wide.load57.1 = load <8 x float>, <8 x float>* %497, align 4, !tbaa !12, !llvm.access.group !16
  %498 = fmul <8 x float> %495, %wide.load57.1
  %499 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %488, <8 x float> %wide.load56.1, <8 x float> %498)
  %500 = add nsw i32 %mul.i.i.5, %485
  %501 = sext i32 %500 to i64
  %502 = getelementptr inbounds float, float* %7, i64 %501
  %503 = bitcast float* %502 to <8 x float>*
  %wide.load58.1 = load <8 x float>, <8 x float>* %503, align 4, !tbaa !12, !llvm.access.group !16
  %504 = fadd <8 x float> %wide.load58.1, %499
  %505 = bitcast float* %502 to <8 x float>*
  store <8 x float> %504, <8 x float>* %505, align 4, !tbaa !12, !llvm.access.group !16
  %506 = or i64 %mul.i.i.i, 16
  %507 = trunc i64 %506 to i32
  %508 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %509 = insertelement <8 x float> undef, float %508, i32 0
  %510 = shufflevector <8 x float> %509, <8 x float> undef, <8 x i32> zeroinitializer
  %511 = shl i64 %506, 32
  %512 = ashr exact i64 %511, 32
  %513 = getelementptr inbounds float, float* %11, i64 %512
  %514 = bitcast float* %513 to <8 x float>*
  %wide.load56.2 = load <8 x float>, <8 x float>* %514, align 4, !tbaa !12, !llvm.access.group !16
  %515 = load float, float* %arrayidx6.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %516 = insertelement <8 x float> undef, float %515, i32 0
  %517 = shufflevector <8 x float> %516, <8 x float> undef, <8 x i32> zeroinitializer
  %518 = getelementptr inbounds float, float* %15, i64 %512
  %519 = bitcast float* %518 to <8 x float>*
  %wide.load57.2 = load <8 x float>, <8 x float>* %519, align 4, !tbaa !12, !llvm.access.group !16
  %520 = fmul <8 x float> %517, %wide.load57.2
  %521 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %510, <8 x float> %wide.load56.2, <8 x float> %520)
  %522 = add nsw i32 %mul.i.i.5, %507
  %523 = sext i32 %522 to i64
  %524 = getelementptr inbounds float, float* %7, i64 %523
  %525 = bitcast float* %524 to <8 x float>*
  %wide.load58.2 = load <8 x float>, <8 x float>* %525, align 4, !tbaa !12, !llvm.access.group !16
  %526 = fadd <8 x float> %wide.load58.2, %521
  %527 = bitcast float* %524 to <8 x float>*
  store <8 x float> %526, <8 x float>* %527, align 4, !tbaa !12, !llvm.access.group !16
  %528 = or i64 %mul.i.i.i, 24
  %529 = trunc i64 %528 to i32
  %530 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %531 = insertelement <8 x float> undef, float %530, i32 0
  %532 = shufflevector <8 x float> %531, <8 x float> undef, <8 x i32> zeroinitializer
  %533 = shl i64 %528, 32
  %534 = ashr exact i64 %533, 32
  %535 = getelementptr inbounds float, float* %11, i64 %534
  %536 = bitcast float* %535 to <8 x float>*
  %wide.load56.3 = load <8 x float>, <8 x float>* %536, align 4, !tbaa !12, !llvm.access.group !16
  %537 = load float, float* %arrayidx6.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %538 = insertelement <8 x float> undef, float %537, i32 0
  %539 = shufflevector <8 x float> %538, <8 x float> undef, <8 x i32> zeroinitializer
  %540 = getelementptr inbounds float, float* %15, i64 %534
  %541 = bitcast float* %540 to <8 x float>*
  %wide.load57.3 = load <8 x float>, <8 x float>* %541, align 4, !tbaa !12, !llvm.access.group !16
  %542 = fmul <8 x float> %539, %wide.load57.3
  %543 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %532, <8 x float> %wide.load56.3, <8 x float> %542)
  %544 = add nsw i32 %mul.i.i.5, %529
  %545 = sext i32 %544 to i64
  %546 = getelementptr inbounds float, float* %7, i64 %545
  %547 = bitcast float* %546 to <8 x float>*
  %wide.load58.3 = load <8 x float>, <8 x float>* %547, align 4, !tbaa !12, !llvm.access.group !16
  %548 = fadd <8 x float> %wide.load58.3, %543
  %549 = bitcast float* %546 to <8 x float>*
  store <8 x float> %548, <8 x float>* %549, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.6 = or i64 %mul3.i.i.i, 6
  %conv2.i.i.6 = trunc i64 %add6.i.i.i.6 to i32
  %sext.i.i.6 = shl i64 %add6.i.i.i.6, 32
  %idxprom.i.i.6 = ashr exact i64 %sext.i.i.6, 32
  %arrayidx.i.i.6 = getelementptr inbounds float, float* %19, i64 %idxprom.i.i.6
  %arrayidx6.i.i.6 = getelementptr inbounds float, float* %23, i64 %idxprom.i.i.6
  %mul.i.i.6 = mul nsw i32 %27, %conv2.i.i.6
  %550 = trunc i64 %mul.i.i.i to i32
  %551 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %552 = insertelement <8 x float> undef, float %551, i32 0
  %553 = shufflevector <8 x float> %552, <8 x float> undef, <8 x i32> zeroinitializer
  %554 = shl i64 %2, 37
  %555 = ashr exact i64 %554, 32
  %556 = getelementptr inbounds float, float* %11, i64 %555
  %557 = bitcast float* %556 to <8 x float>*
  %wide.load67 = load <8 x float>, <8 x float>* %557, align 4, !tbaa !12, !llvm.access.group !16
  %558 = load float, float* %arrayidx6.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %559 = insertelement <8 x float> undef, float %558, i32 0
  %560 = shufflevector <8 x float> %559, <8 x float> undef, <8 x i32> zeroinitializer
  %561 = getelementptr inbounds float, float* %15, i64 %555
  %562 = bitcast float* %561 to <8 x float>*
  %wide.load68 = load <8 x float>, <8 x float>* %562, align 4, !tbaa !12, !llvm.access.group !16
  %563 = fmul <8 x float> %560, %wide.load68
  %564 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %553, <8 x float> %wide.load67, <8 x float> %563)
  %565 = add nsw i32 %mul.i.i.6, %550
  %566 = sext i32 %565 to i64
  %567 = getelementptr inbounds float, float* %7, i64 %566
  %568 = bitcast float* %567 to <8 x float>*
  %wide.load69 = load <8 x float>, <8 x float>* %568, align 4, !tbaa !12, !llvm.access.group !16
  %569 = fadd <8 x float> %wide.load69, %564
  %570 = bitcast float* %567 to <8 x float>*
  store <8 x float> %569, <8 x float>* %570, align 4, !tbaa !12, !llvm.access.group !16
  %571 = or i64 %mul.i.i.i, 8
  %572 = trunc i64 %571 to i32
  %573 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %574 = insertelement <8 x float> undef, float %573, i32 0
  %575 = shufflevector <8 x float> %574, <8 x float> undef, <8 x i32> zeroinitializer
  %576 = shl i64 %571, 32
  %577 = ashr exact i64 %576, 32
  %578 = getelementptr inbounds float, float* %11, i64 %577
  %579 = bitcast float* %578 to <8 x float>*
  %wide.load67.1 = load <8 x float>, <8 x float>* %579, align 4, !tbaa !12, !llvm.access.group !16
  %580 = load float, float* %arrayidx6.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %581 = insertelement <8 x float> undef, float %580, i32 0
  %582 = shufflevector <8 x float> %581, <8 x float> undef, <8 x i32> zeroinitializer
  %583 = getelementptr inbounds float, float* %15, i64 %577
  %584 = bitcast float* %583 to <8 x float>*
  %wide.load68.1 = load <8 x float>, <8 x float>* %584, align 4, !tbaa !12, !llvm.access.group !16
  %585 = fmul <8 x float> %582, %wide.load68.1
  %586 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %575, <8 x float> %wide.load67.1, <8 x float> %585)
  %587 = add nsw i32 %mul.i.i.6, %572
  %588 = sext i32 %587 to i64
  %589 = getelementptr inbounds float, float* %7, i64 %588
  %590 = bitcast float* %589 to <8 x float>*
  %wide.load69.1 = load <8 x float>, <8 x float>* %590, align 4, !tbaa !12, !llvm.access.group !16
  %591 = fadd <8 x float> %wide.load69.1, %586
  %592 = bitcast float* %589 to <8 x float>*
  store <8 x float> %591, <8 x float>* %592, align 4, !tbaa !12, !llvm.access.group !16
  %593 = or i64 %mul.i.i.i, 16
  %594 = trunc i64 %593 to i32
  %595 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %596 = insertelement <8 x float> undef, float %595, i32 0
  %597 = shufflevector <8 x float> %596, <8 x float> undef, <8 x i32> zeroinitializer
  %598 = shl i64 %593, 32
  %599 = ashr exact i64 %598, 32
  %600 = getelementptr inbounds float, float* %11, i64 %599
  %601 = bitcast float* %600 to <8 x float>*
  %wide.load67.2 = load <8 x float>, <8 x float>* %601, align 4, !tbaa !12, !llvm.access.group !16
  %602 = load float, float* %arrayidx6.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %603 = insertelement <8 x float> undef, float %602, i32 0
  %604 = shufflevector <8 x float> %603, <8 x float> undef, <8 x i32> zeroinitializer
  %605 = getelementptr inbounds float, float* %15, i64 %599
  %606 = bitcast float* %605 to <8 x float>*
  %wide.load68.2 = load <8 x float>, <8 x float>* %606, align 4, !tbaa !12, !llvm.access.group !16
  %607 = fmul <8 x float> %604, %wide.load68.2
  %608 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %597, <8 x float> %wide.load67.2, <8 x float> %607)
  %609 = add nsw i32 %mul.i.i.6, %594
  %610 = sext i32 %609 to i64
  %611 = getelementptr inbounds float, float* %7, i64 %610
  %612 = bitcast float* %611 to <8 x float>*
  %wide.load69.2 = load <8 x float>, <8 x float>* %612, align 4, !tbaa !12, !llvm.access.group !16
  %613 = fadd <8 x float> %wide.load69.2, %608
  %614 = bitcast float* %611 to <8 x float>*
  store <8 x float> %613, <8 x float>* %614, align 4, !tbaa !12, !llvm.access.group !16
  %615 = or i64 %mul.i.i.i, 24
  %616 = trunc i64 %615 to i32
  %617 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %618 = insertelement <8 x float> undef, float %617, i32 0
  %619 = shufflevector <8 x float> %618, <8 x float> undef, <8 x i32> zeroinitializer
  %620 = shl i64 %615, 32
  %621 = ashr exact i64 %620, 32
  %622 = getelementptr inbounds float, float* %11, i64 %621
  %623 = bitcast float* %622 to <8 x float>*
  %wide.load67.3 = load <8 x float>, <8 x float>* %623, align 4, !tbaa !12, !llvm.access.group !16
  %624 = load float, float* %arrayidx6.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %625 = insertelement <8 x float> undef, float %624, i32 0
  %626 = shufflevector <8 x float> %625, <8 x float> undef, <8 x i32> zeroinitializer
  %627 = getelementptr inbounds float, float* %15, i64 %621
  %628 = bitcast float* %627 to <8 x float>*
  %wide.load68.3 = load <8 x float>, <8 x float>* %628, align 4, !tbaa !12, !llvm.access.group !16
  %629 = fmul <8 x float> %626, %wide.load68.3
  %630 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %619, <8 x float> %wide.load67.3, <8 x float> %629)
  %631 = add nsw i32 %mul.i.i.6, %616
  %632 = sext i32 %631 to i64
  %633 = getelementptr inbounds float, float* %7, i64 %632
  %634 = bitcast float* %633 to <8 x float>*
  %wide.load69.3 = load <8 x float>, <8 x float>* %634, align 4, !tbaa !12, !llvm.access.group !16
  %635 = fadd <8 x float> %wide.load69.3, %630
  %636 = bitcast float* %633 to <8 x float>*
  store <8 x float> %635, <8 x float>* %636, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.7 = or i64 %mul3.i.i.i, 7
  %conv2.i.i.7 = trunc i64 %add6.i.i.i.7 to i32
  %sext.i.i.7 = shl i64 %add6.i.i.i.7, 32
  %idxprom.i.i.7 = ashr exact i64 %sext.i.i.7, 32
  %arrayidx.i.i.7 = getelementptr inbounds float, float* %19, i64 %idxprom.i.i.7
  %arrayidx6.i.i.7 = getelementptr inbounds float, float* %23, i64 %idxprom.i.i.7
  %mul.i.i.7 = mul nsw i32 %27, %conv2.i.i.7
  %637 = trunc i64 %mul.i.i.i to i32
  %638 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %639 = insertelement <8 x float> undef, float %638, i32 0
  %640 = shufflevector <8 x float> %639, <8 x float> undef, <8 x i32> zeroinitializer
  %641 = shl i64 %2, 37
  %642 = ashr exact i64 %641, 32
  %643 = getelementptr inbounds float, float* %11, i64 %642
  %644 = bitcast float* %643 to <8 x float>*
  %wide.load78 = load <8 x float>, <8 x float>* %644, align 4, !tbaa !12, !llvm.access.group !16
  %645 = load float, float* %arrayidx6.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %646 = insertelement <8 x float> undef, float %645, i32 0
  %647 = shufflevector <8 x float> %646, <8 x float> undef, <8 x i32> zeroinitializer
  %648 = getelementptr inbounds float, float* %15, i64 %642
  %649 = bitcast float* %648 to <8 x float>*
  %wide.load79 = load <8 x float>, <8 x float>* %649, align 4, !tbaa !12, !llvm.access.group !16
  %650 = fmul <8 x float> %647, %wide.load79
  %651 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %640, <8 x float> %wide.load78, <8 x float> %650)
  %652 = add nsw i32 %mul.i.i.7, %637
  %653 = sext i32 %652 to i64
  %654 = getelementptr inbounds float, float* %7, i64 %653
  %655 = bitcast float* %654 to <8 x float>*
  %wide.load80 = load <8 x float>, <8 x float>* %655, align 4, !tbaa !12, !llvm.access.group !16
  %656 = fadd <8 x float> %wide.load80, %651
  %657 = bitcast float* %654 to <8 x float>*
  store <8 x float> %656, <8 x float>* %657, align 4, !tbaa !12, !llvm.access.group !16
  %658 = or i64 %mul.i.i.i, 8
  %659 = trunc i64 %658 to i32
  %660 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %661 = insertelement <8 x float> undef, float %660, i32 0
  %662 = shufflevector <8 x float> %661, <8 x float> undef, <8 x i32> zeroinitializer
  %663 = shl i64 %658, 32
  %664 = ashr exact i64 %663, 32
  %665 = getelementptr inbounds float, float* %11, i64 %664
  %666 = bitcast float* %665 to <8 x float>*
  %wide.load78.1 = load <8 x float>, <8 x float>* %666, align 4, !tbaa !12, !llvm.access.group !16
  %667 = load float, float* %arrayidx6.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %668 = insertelement <8 x float> undef, float %667, i32 0
  %669 = shufflevector <8 x float> %668, <8 x float> undef, <8 x i32> zeroinitializer
  %670 = getelementptr inbounds float, float* %15, i64 %664
  %671 = bitcast float* %670 to <8 x float>*
  %wide.load79.1 = load <8 x float>, <8 x float>* %671, align 4, !tbaa !12, !llvm.access.group !16
  %672 = fmul <8 x float> %669, %wide.load79.1
  %673 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %662, <8 x float> %wide.load78.1, <8 x float> %672)
  %674 = add nsw i32 %mul.i.i.7, %659
  %675 = sext i32 %674 to i64
  %676 = getelementptr inbounds float, float* %7, i64 %675
  %677 = bitcast float* %676 to <8 x float>*
  %wide.load80.1 = load <8 x float>, <8 x float>* %677, align 4, !tbaa !12, !llvm.access.group !16
  %678 = fadd <8 x float> %wide.load80.1, %673
  %679 = bitcast float* %676 to <8 x float>*
  store <8 x float> %678, <8 x float>* %679, align 4, !tbaa !12, !llvm.access.group !16
  %680 = or i64 %mul.i.i.i, 16
  %681 = trunc i64 %680 to i32
  %682 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %683 = insertelement <8 x float> undef, float %682, i32 0
  %684 = shufflevector <8 x float> %683, <8 x float> undef, <8 x i32> zeroinitializer
  %685 = shl i64 %680, 32
  %686 = ashr exact i64 %685, 32
  %687 = getelementptr inbounds float, float* %11, i64 %686
  %688 = bitcast float* %687 to <8 x float>*
  %wide.load78.2 = load <8 x float>, <8 x float>* %688, align 4, !tbaa !12, !llvm.access.group !16
  %689 = load float, float* %arrayidx6.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %690 = insertelement <8 x float> undef, float %689, i32 0
  %691 = shufflevector <8 x float> %690, <8 x float> undef, <8 x i32> zeroinitializer
  %692 = getelementptr inbounds float, float* %15, i64 %686
  %693 = bitcast float* %692 to <8 x float>*
  %wide.load79.2 = load <8 x float>, <8 x float>* %693, align 4, !tbaa !12, !llvm.access.group !16
  %694 = fmul <8 x float> %691, %wide.load79.2
  %695 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %684, <8 x float> %wide.load78.2, <8 x float> %694)
  %696 = add nsw i32 %mul.i.i.7, %681
  %697 = sext i32 %696 to i64
  %698 = getelementptr inbounds float, float* %7, i64 %697
  %699 = bitcast float* %698 to <8 x float>*
  %wide.load80.2 = load <8 x float>, <8 x float>* %699, align 4, !tbaa !12, !llvm.access.group !16
  %700 = fadd <8 x float> %wide.load80.2, %695
  %701 = bitcast float* %698 to <8 x float>*
  store <8 x float> %700, <8 x float>* %701, align 4, !tbaa !12, !llvm.access.group !16
  %702 = or i64 %mul.i.i.i, 24
  %703 = trunc i64 %702 to i32
  %704 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %705 = insertelement <8 x float> undef, float %704, i32 0
  %706 = shufflevector <8 x float> %705, <8 x float> undef, <8 x i32> zeroinitializer
  %707 = shl i64 %702, 32
  %708 = ashr exact i64 %707, 32
  %709 = getelementptr inbounds float, float* %11, i64 %708
  %710 = bitcast float* %709 to <8 x float>*
  %wide.load78.3 = load <8 x float>, <8 x float>* %710, align 4, !tbaa !12, !llvm.access.group !16
  %711 = load float, float* %arrayidx6.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %712 = insertelement <8 x float> undef, float %711, i32 0
  %713 = shufflevector <8 x float> %712, <8 x float> undef, <8 x i32> zeroinitializer
  %714 = getelementptr inbounds float, float* %15, i64 %708
  %715 = bitcast float* %714 to <8 x float>*
  %wide.load79.3 = load <8 x float>, <8 x float>* %715, align 4, !tbaa !12, !llvm.access.group !16
  %716 = fmul <8 x float> %713, %wide.load79.3
  %717 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %706, <8 x float> %wide.load78.3, <8 x float> %716)
  %718 = add nsw i32 %mul.i.i.7, %703
  %719 = sext i32 %718 to i64
  %720 = getelementptr inbounds float, float* %7, i64 %719
  %721 = bitcast float* %720 to <8 x float>*
  %wide.load80.3 = load <8 x float>, <8 x float>* %721, align 4, !tbaa !12, !llvm.access.group !16
  %722 = fadd <8 x float> %wide.load80.3, %717
  %723 = bitcast float* %720 to <8 x float>*
  store <8 x float> %722, <8 x float>* %723, align 4, !tbaa !12, !llvm.access.group !16
  ret void
}

; Function Attrs: nofree nounwind
define void @_pocl_kernel_gemver_kernel1_workgroup_fast(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #1 {
pregion_for_entry.pregion_for_init.i.i:
  %5 = bitcast i8** %0 to float**
  %6 = load float*, float** %5, align 8
  %7 = getelementptr i8*, i8** %0, i64 1
  %8 = bitcast i8** %7 to float**
  %9 = load float*, float** %8, align 8
  %10 = getelementptr i8*, i8** %0, i64 2
  %11 = bitcast i8** %10 to float**
  %12 = load float*, float** %11, align 8
  %13 = getelementptr i8*, i8** %0, i64 3
  %14 = bitcast i8** %13 to float**
  %15 = load float*, float** %14, align 8
  %16 = getelementptr i8*, i8** %0, i64 4
  %17 = bitcast i8** %16 to float**
  %18 = load float*, float** %17, align 8
  %19 = getelementptr i8*, i8** %0, i64 5
  %20 = bitcast i8** %19 to i32**
  %21 = load i32*, i32** %20, align 8
  %22 = load i32, i32* %21, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %conv2.i.i = trunc i64 %mul3.i.i.i to i32
  %sext.i.i = shl i64 %3, 35
  %idxprom.i.i = ashr exact i64 %sext.i.i, 32
  %arrayidx.i.i = getelementptr inbounds float, float* %15, i64 %idxprom.i.i
  %arrayidx6.i.i = getelementptr inbounds float, float* %18, i64 %idxprom.i.i
  %mul.i.i = mul nsw i32 %22, %conv2.i.i
  %23 = trunc i64 %mul.i.i.i to i32
  %24 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %25 = insertelement <8 x float> undef, float %24, i32 0
  %26 = shufflevector <8 x float> %25, <8 x float> undef, <8 x i32> zeroinitializer
  %27 = shl i64 %2, 37
  %28 = ashr exact i64 %27, 32
  %29 = getelementptr inbounds float, float* %9, i64 %28
  %30 = bitcast float* %29 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %30, align 4, !tbaa !12, !llvm.access.group !16
  %31 = load float, float* %arrayidx6.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %32 = insertelement <8 x float> undef, float %31, i32 0
  %33 = shufflevector <8 x float> %32, <8 x float> undef, <8 x i32> zeroinitializer
  %34 = getelementptr inbounds float, float* %12, i64 %28
  %35 = bitcast float* %34 to <8 x float>*
  %wide.load2 = load <8 x float>, <8 x float>* %35, align 4, !tbaa !12, !llvm.access.group !16
  %36 = fmul <8 x float> %33, %wide.load2
  %37 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %26, <8 x float> %wide.load, <8 x float> %36)
  %38 = add nsw i32 %mul.i.i, %23
  %39 = sext i32 %38 to i64
  %40 = getelementptr inbounds float, float* %6, i64 %39
  %41 = bitcast float* %40 to <8 x float>*
  %wide.load3 = load <8 x float>, <8 x float>* %41, align 4, !tbaa !12, !llvm.access.group !16
  %42 = fadd <8 x float> %wide.load3, %37
  %43 = bitcast float* %40 to <8 x float>*
  store <8 x float> %42, <8 x float>* %43, align 4, !tbaa !12, !llvm.access.group !16
  %44 = or i64 %mul.i.i.i, 8
  %45 = trunc i64 %44 to i32
  %46 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %47 = insertelement <8 x float> undef, float %46, i32 0
  %48 = shufflevector <8 x float> %47, <8 x float> undef, <8 x i32> zeroinitializer
  %49 = shl i64 %44, 32
  %50 = ashr exact i64 %49, 32
  %51 = getelementptr inbounds float, float* %9, i64 %50
  %52 = bitcast float* %51 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %52, align 4, !tbaa !12, !llvm.access.group !16
  %53 = load float, float* %arrayidx6.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %54 = insertelement <8 x float> undef, float %53, i32 0
  %55 = shufflevector <8 x float> %54, <8 x float> undef, <8 x i32> zeroinitializer
  %56 = getelementptr inbounds float, float* %12, i64 %50
  %57 = bitcast float* %56 to <8 x float>*
  %wide.load2.1 = load <8 x float>, <8 x float>* %57, align 4, !tbaa !12, !llvm.access.group !16
  %58 = fmul <8 x float> %55, %wide.load2.1
  %59 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %48, <8 x float> %wide.load.1, <8 x float> %58)
  %60 = add nsw i32 %mul.i.i, %45
  %61 = sext i32 %60 to i64
  %62 = getelementptr inbounds float, float* %6, i64 %61
  %63 = bitcast float* %62 to <8 x float>*
  %wide.load3.1 = load <8 x float>, <8 x float>* %63, align 4, !tbaa !12, !llvm.access.group !16
  %64 = fadd <8 x float> %wide.load3.1, %59
  %65 = bitcast float* %62 to <8 x float>*
  store <8 x float> %64, <8 x float>* %65, align 4, !tbaa !12, !llvm.access.group !16
  %66 = or i64 %mul.i.i.i, 16
  %67 = trunc i64 %66 to i32
  %68 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %69 = insertelement <8 x float> undef, float %68, i32 0
  %70 = shufflevector <8 x float> %69, <8 x float> undef, <8 x i32> zeroinitializer
  %71 = shl i64 %66, 32
  %72 = ashr exact i64 %71, 32
  %73 = getelementptr inbounds float, float* %9, i64 %72
  %74 = bitcast float* %73 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %74, align 4, !tbaa !12, !llvm.access.group !16
  %75 = load float, float* %arrayidx6.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %76 = insertelement <8 x float> undef, float %75, i32 0
  %77 = shufflevector <8 x float> %76, <8 x float> undef, <8 x i32> zeroinitializer
  %78 = getelementptr inbounds float, float* %12, i64 %72
  %79 = bitcast float* %78 to <8 x float>*
  %wide.load2.2 = load <8 x float>, <8 x float>* %79, align 4, !tbaa !12, !llvm.access.group !16
  %80 = fmul <8 x float> %77, %wide.load2.2
  %81 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %70, <8 x float> %wide.load.2, <8 x float> %80)
  %82 = add nsw i32 %mul.i.i, %67
  %83 = sext i32 %82 to i64
  %84 = getelementptr inbounds float, float* %6, i64 %83
  %85 = bitcast float* %84 to <8 x float>*
  %wide.load3.2 = load <8 x float>, <8 x float>* %85, align 4, !tbaa !12, !llvm.access.group !16
  %86 = fadd <8 x float> %wide.load3.2, %81
  %87 = bitcast float* %84 to <8 x float>*
  store <8 x float> %86, <8 x float>* %87, align 4, !tbaa !12, !llvm.access.group !16
  %88 = or i64 %mul.i.i.i, 24
  %89 = trunc i64 %88 to i32
  %90 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %91 = insertelement <8 x float> undef, float %90, i32 0
  %92 = shufflevector <8 x float> %91, <8 x float> undef, <8 x i32> zeroinitializer
  %93 = shl i64 %88, 32
  %94 = ashr exact i64 %93, 32
  %95 = getelementptr inbounds float, float* %9, i64 %94
  %96 = bitcast float* %95 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %96, align 4, !tbaa !12, !llvm.access.group !16
  %97 = load float, float* %arrayidx6.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %98 = insertelement <8 x float> undef, float %97, i32 0
  %99 = shufflevector <8 x float> %98, <8 x float> undef, <8 x i32> zeroinitializer
  %100 = getelementptr inbounds float, float* %12, i64 %94
  %101 = bitcast float* %100 to <8 x float>*
  %wide.load2.3 = load <8 x float>, <8 x float>* %101, align 4, !tbaa !12, !llvm.access.group !16
  %102 = fmul <8 x float> %99, %wide.load2.3
  %103 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %92, <8 x float> %wide.load.3, <8 x float> %102)
  %104 = add nsw i32 %mul.i.i, %89
  %105 = sext i32 %104 to i64
  %106 = getelementptr inbounds float, float* %6, i64 %105
  %107 = bitcast float* %106 to <8 x float>*
  %wide.load3.3 = load <8 x float>, <8 x float>* %107, align 4, !tbaa !12, !llvm.access.group !16
  %108 = fadd <8 x float> %wide.load3.3, %103
  %109 = bitcast float* %106 to <8 x float>*
  store <8 x float> %108, <8 x float>* %109, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.1 = or i64 %mul3.i.i.i, 1
  %conv2.i.i.1 = trunc i64 %add6.i.i.i.1 to i32
  %sext.i.i.1 = shl i64 %add6.i.i.i.1, 32
  %idxprom.i.i.1 = ashr exact i64 %sext.i.i.1, 32
  %arrayidx.i.i.1 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.1
  %arrayidx6.i.i.1 = getelementptr inbounds float, float* %18, i64 %idxprom.i.i.1
  %mul.i.i.1 = mul nsw i32 %22, %conv2.i.i.1
  %110 = trunc i64 %mul.i.i.i to i32
  %111 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %112 = insertelement <8 x float> undef, float %111, i32 0
  %113 = shufflevector <8 x float> %112, <8 x float> undef, <8 x i32> zeroinitializer
  %114 = shl i64 %2, 37
  %115 = ashr exact i64 %114, 32
  %116 = getelementptr inbounds float, float* %9, i64 %115
  %117 = bitcast float* %116 to <8 x float>*
  %wide.load12 = load <8 x float>, <8 x float>* %117, align 4, !tbaa !12, !llvm.access.group !16
  %118 = load float, float* %arrayidx6.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %119 = insertelement <8 x float> undef, float %118, i32 0
  %120 = shufflevector <8 x float> %119, <8 x float> undef, <8 x i32> zeroinitializer
  %121 = getelementptr inbounds float, float* %12, i64 %115
  %122 = bitcast float* %121 to <8 x float>*
  %wide.load13 = load <8 x float>, <8 x float>* %122, align 4, !tbaa !12, !llvm.access.group !16
  %123 = fmul <8 x float> %120, %wide.load13
  %124 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %113, <8 x float> %wide.load12, <8 x float> %123)
  %125 = add nsw i32 %mul.i.i.1, %110
  %126 = sext i32 %125 to i64
  %127 = getelementptr inbounds float, float* %6, i64 %126
  %128 = bitcast float* %127 to <8 x float>*
  %wide.load14 = load <8 x float>, <8 x float>* %128, align 4, !tbaa !12, !llvm.access.group !16
  %129 = fadd <8 x float> %wide.load14, %124
  %130 = bitcast float* %127 to <8 x float>*
  store <8 x float> %129, <8 x float>* %130, align 4, !tbaa !12, !llvm.access.group !16
  %131 = or i64 %mul.i.i.i, 8
  %132 = trunc i64 %131 to i32
  %133 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %134 = insertelement <8 x float> undef, float %133, i32 0
  %135 = shufflevector <8 x float> %134, <8 x float> undef, <8 x i32> zeroinitializer
  %136 = shl i64 %131, 32
  %137 = ashr exact i64 %136, 32
  %138 = getelementptr inbounds float, float* %9, i64 %137
  %139 = bitcast float* %138 to <8 x float>*
  %wide.load12.1 = load <8 x float>, <8 x float>* %139, align 4, !tbaa !12, !llvm.access.group !16
  %140 = load float, float* %arrayidx6.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %141 = insertelement <8 x float> undef, float %140, i32 0
  %142 = shufflevector <8 x float> %141, <8 x float> undef, <8 x i32> zeroinitializer
  %143 = getelementptr inbounds float, float* %12, i64 %137
  %144 = bitcast float* %143 to <8 x float>*
  %wide.load13.1 = load <8 x float>, <8 x float>* %144, align 4, !tbaa !12, !llvm.access.group !16
  %145 = fmul <8 x float> %142, %wide.load13.1
  %146 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %135, <8 x float> %wide.load12.1, <8 x float> %145)
  %147 = add nsw i32 %mul.i.i.1, %132
  %148 = sext i32 %147 to i64
  %149 = getelementptr inbounds float, float* %6, i64 %148
  %150 = bitcast float* %149 to <8 x float>*
  %wide.load14.1 = load <8 x float>, <8 x float>* %150, align 4, !tbaa !12, !llvm.access.group !16
  %151 = fadd <8 x float> %wide.load14.1, %146
  %152 = bitcast float* %149 to <8 x float>*
  store <8 x float> %151, <8 x float>* %152, align 4, !tbaa !12, !llvm.access.group !16
  %153 = or i64 %mul.i.i.i, 16
  %154 = trunc i64 %153 to i32
  %155 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %156 = insertelement <8 x float> undef, float %155, i32 0
  %157 = shufflevector <8 x float> %156, <8 x float> undef, <8 x i32> zeroinitializer
  %158 = shl i64 %153, 32
  %159 = ashr exact i64 %158, 32
  %160 = getelementptr inbounds float, float* %9, i64 %159
  %161 = bitcast float* %160 to <8 x float>*
  %wide.load12.2 = load <8 x float>, <8 x float>* %161, align 4, !tbaa !12, !llvm.access.group !16
  %162 = load float, float* %arrayidx6.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %163 = insertelement <8 x float> undef, float %162, i32 0
  %164 = shufflevector <8 x float> %163, <8 x float> undef, <8 x i32> zeroinitializer
  %165 = getelementptr inbounds float, float* %12, i64 %159
  %166 = bitcast float* %165 to <8 x float>*
  %wide.load13.2 = load <8 x float>, <8 x float>* %166, align 4, !tbaa !12, !llvm.access.group !16
  %167 = fmul <8 x float> %164, %wide.load13.2
  %168 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %157, <8 x float> %wide.load12.2, <8 x float> %167)
  %169 = add nsw i32 %mul.i.i.1, %154
  %170 = sext i32 %169 to i64
  %171 = getelementptr inbounds float, float* %6, i64 %170
  %172 = bitcast float* %171 to <8 x float>*
  %wide.load14.2 = load <8 x float>, <8 x float>* %172, align 4, !tbaa !12, !llvm.access.group !16
  %173 = fadd <8 x float> %wide.load14.2, %168
  %174 = bitcast float* %171 to <8 x float>*
  store <8 x float> %173, <8 x float>* %174, align 4, !tbaa !12, !llvm.access.group !16
  %175 = or i64 %mul.i.i.i, 24
  %176 = trunc i64 %175 to i32
  %177 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %178 = insertelement <8 x float> undef, float %177, i32 0
  %179 = shufflevector <8 x float> %178, <8 x float> undef, <8 x i32> zeroinitializer
  %180 = shl i64 %175, 32
  %181 = ashr exact i64 %180, 32
  %182 = getelementptr inbounds float, float* %9, i64 %181
  %183 = bitcast float* %182 to <8 x float>*
  %wide.load12.3 = load <8 x float>, <8 x float>* %183, align 4, !tbaa !12, !llvm.access.group !16
  %184 = load float, float* %arrayidx6.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %185 = insertelement <8 x float> undef, float %184, i32 0
  %186 = shufflevector <8 x float> %185, <8 x float> undef, <8 x i32> zeroinitializer
  %187 = getelementptr inbounds float, float* %12, i64 %181
  %188 = bitcast float* %187 to <8 x float>*
  %wide.load13.3 = load <8 x float>, <8 x float>* %188, align 4, !tbaa !12, !llvm.access.group !16
  %189 = fmul <8 x float> %186, %wide.load13.3
  %190 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %179, <8 x float> %wide.load12.3, <8 x float> %189)
  %191 = add nsw i32 %mul.i.i.1, %176
  %192 = sext i32 %191 to i64
  %193 = getelementptr inbounds float, float* %6, i64 %192
  %194 = bitcast float* %193 to <8 x float>*
  %wide.load14.3 = load <8 x float>, <8 x float>* %194, align 4, !tbaa !12, !llvm.access.group !16
  %195 = fadd <8 x float> %wide.load14.3, %190
  %196 = bitcast float* %193 to <8 x float>*
  store <8 x float> %195, <8 x float>* %196, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.2 = or i64 %mul3.i.i.i, 2
  %conv2.i.i.2 = trunc i64 %add6.i.i.i.2 to i32
  %sext.i.i.2 = shl i64 %add6.i.i.i.2, 32
  %idxprom.i.i.2 = ashr exact i64 %sext.i.i.2, 32
  %arrayidx.i.i.2 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.2
  %arrayidx6.i.i.2 = getelementptr inbounds float, float* %18, i64 %idxprom.i.i.2
  %mul.i.i.2 = mul nsw i32 %22, %conv2.i.i.2
  %197 = trunc i64 %mul.i.i.i to i32
  %198 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %199 = insertelement <8 x float> undef, float %198, i32 0
  %200 = shufflevector <8 x float> %199, <8 x float> undef, <8 x i32> zeroinitializer
  %201 = shl i64 %2, 37
  %202 = ashr exact i64 %201, 32
  %203 = getelementptr inbounds float, float* %9, i64 %202
  %204 = bitcast float* %203 to <8 x float>*
  %wide.load23 = load <8 x float>, <8 x float>* %204, align 4, !tbaa !12, !llvm.access.group !16
  %205 = load float, float* %arrayidx6.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %206 = insertelement <8 x float> undef, float %205, i32 0
  %207 = shufflevector <8 x float> %206, <8 x float> undef, <8 x i32> zeroinitializer
  %208 = getelementptr inbounds float, float* %12, i64 %202
  %209 = bitcast float* %208 to <8 x float>*
  %wide.load24 = load <8 x float>, <8 x float>* %209, align 4, !tbaa !12, !llvm.access.group !16
  %210 = fmul <8 x float> %207, %wide.load24
  %211 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %200, <8 x float> %wide.load23, <8 x float> %210)
  %212 = add nsw i32 %mul.i.i.2, %197
  %213 = sext i32 %212 to i64
  %214 = getelementptr inbounds float, float* %6, i64 %213
  %215 = bitcast float* %214 to <8 x float>*
  %wide.load25 = load <8 x float>, <8 x float>* %215, align 4, !tbaa !12, !llvm.access.group !16
  %216 = fadd <8 x float> %wide.load25, %211
  %217 = bitcast float* %214 to <8 x float>*
  store <8 x float> %216, <8 x float>* %217, align 4, !tbaa !12, !llvm.access.group !16
  %218 = or i64 %mul.i.i.i, 8
  %219 = trunc i64 %218 to i32
  %220 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %221 = insertelement <8 x float> undef, float %220, i32 0
  %222 = shufflevector <8 x float> %221, <8 x float> undef, <8 x i32> zeroinitializer
  %223 = shl i64 %218, 32
  %224 = ashr exact i64 %223, 32
  %225 = getelementptr inbounds float, float* %9, i64 %224
  %226 = bitcast float* %225 to <8 x float>*
  %wide.load23.1 = load <8 x float>, <8 x float>* %226, align 4, !tbaa !12, !llvm.access.group !16
  %227 = load float, float* %arrayidx6.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %228 = insertelement <8 x float> undef, float %227, i32 0
  %229 = shufflevector <8 x float> %228, <8 x float> undef, <8 x i32> zeroinitializer
  %230 = getelementptr inbounds float, float* %12, i64 %224
  %231 = bitcast float* %230 to <8 x float>*
  %wide.load24.1 = load <8 x float>, <8 x float>* %231, align 4, !tbaa !12, !llvm.access.group !16
  %232 = fmul <8 x float> %229, %wide.load24.1
  %233 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %222, <8 x float> %wide.load23.1, <8 x float> %232)
  %234 = add nsw i32 %mul.i.i.2, %219
  %235 = sext i32 %234 to i64
  %236 = getelementptr inbounds float, float* %6, i64 %235
  %237 = bitcast float* %236 to <8 x float>*
  %wide.load25.1 = load <8 x float>, <8 x float>* %237, align 4, !tbaa !12, !llvm.access.group !16
  %238 = fadd <8 x float> %wide.load25.1, %233
  %239 = bitcast float* %236 to <8 x float>*
  store <8 x float> %238, <8 x float>* %239, align 4, !tbaa !12, !llvm.access.group !16
  %240 = or i64 %mul.i.i.i, 16
  %241 = trunc i64 %240 to i32
  %242 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %243 = insertelement <8 x float> undef, float %242, i32 0
  %244 = shufflevector <8 x float> %243, <8 x float> undef, <8 x i32> zeroinitializer
  %245 = shl i64 %240, 32
  %246 = ashr exact i64 %245, 32
  %247 = getelementptr inbounds float, float* %9, i64 %246
  %248 = bitcast float* %247 to <8 x float>*
  %wide.load23.2 = load <8 x float>, <8 x float>* %248, align 4, !tbaa !12, !llvm.access.group !16
  %249 = load float, float* %arrayidx6.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %250 = insertelement <8 x float> undef, float %249, i32 0
  %251 = shufflevector <8 x float> %250, <8 x float> undef, <8 x i32> zeroinitializer
  %252 = getelementptr inbounds float, float* %12, i64 %246
  %253 = bitcast float* %252 to <8 x float>*
  %wide.load24.2 = load <8 x float>, <8 x float>* %253, align 4, !tbaa !12, !llvm.access.group !16
  %254 = fmul <8 x float> %251, %wide.load24.2
  %255 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %244, <8 x float> %wide.load23.2, <8 x float> %254)
  %256 = add nsw i32 %mul.i.i.2, %241
  %257 = sext i32 %256 to i64
  %258 = getelementptr inbounds float, float* %6, i64 %257
  %259 = bitcast float* %258 to <8 x float>*
  %wide.load25.2 = load <8 x float>, <8 x float>* %259, align 4, !tbaa !12, !llvm.access.group !16
  %260 = fadd <8 x float> %wide.load25.2, %255
  %261 = bitcast float* %258 to <8 x float>*
  store <8 x float> %260, <8 x float>* %261, align 4, !tbaa !12, !llvm.access.group !16
  %262 = or i64 %mul.i.i.i, 24
  %263 = trunc i64 %262 to i32
  %264 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %265 = insertelement <8 x float> undef, float %264, i32 0
  %266 = shufflevector <8 x float> %265, <8 x float> undef, <8 x i32> zeroinitializer
  %267 = shl i64 %262, 32
  %268 = ashr exact i64 %267, 32
  %269 = getelementptr inbounds float, float* %9, i64 %268
  %270 = bitcast float* %269 to <8 x float>*
  %wide.load23.3 = load <8 x float>, <8 x float>* %270, align 4, !tbaa !12, !llvm.access.group !16
  %271 = load float, float* %arrayidx6.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %272 = insertelement <8 x float> undef, float %271, i32 0
  %273 = shufflevector <8 x float> %272, <8 x float> undef, <8 x i32> zeroinitializer
  %274 = getelementptr inbounds float, float* %12, i64 %268
  %275 = bitcast float* %274 to <8 x float>*
  %wide.load24.3 = load <8 x float>, <8 x float>* %275, align 4, !tbaa !12, !llvm.access.group !16
  %276 = fmul <8 x float> %273, %wide.load24.3
  %277 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %266, <8 x float> %wide.load23.3, <8 x float> %276)
  %278 = add nsw i32 %mul.i.i.2, %263
  %279 = sext i32 %278 to i64
  %280 = getelementptr inbounds float, float* %6, i64 %279
  %281 = bitcast float* %280 to <8 x float>*
  %wide.load25.3 = load <8 x float>, <8 x float>* %281, align 4, !tbaa !12, !llvm.access.group !16
  %282 = fadd <8 x float> %wide.load25.3, %277
  %283 = bitcast float* %280 to <8 x float>*
  store <8 x float> %282, <8 x float>* %283, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.3 = or i64 %mul3.i.i.i, 3
  %conv2.i.i.3 = trunc i64 %add6.i.i.i.3 to i32
  %sext.i.i.3 = shl i64 %add6.i.i.i.3, 32
  %idxprom.i.i.3 = ashr exact i64 %sext.i.i.3, 32
  %arrayidx.i.i.3 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.3
  %arrayidx6.i.i.3 = getelementptr inbounds float, float* %18, i64 %idxprom.i.i.3
  %mul.i.i.3 = mul nsw i32 %22, %conv2.i.i.3
  %284 = trunc i64 %mul.i.i.i to i32
  %285 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %286 = insertelement <8 x float> undef, float %285, i32 0
  %287 = shufflevector <8 x float> %286, <8 x float> undef, <8 x i32> zeroinitializer
  %288 = shl i64 %2, 37
  %289 = ashr exact i64 %288, 32
  %290 = getelementptr inbounds float, float* %9, i64 %289
  %291 = bitcast float* %290 to <8 x float>*
  %wide.load34 = load <8 x float>, <8 x float>* %291, align 4, !tbaa !12, !llvm.access.group !16
  %292 = load float, float* %arrayidx6.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %293 = insertelement <8 x float> undef, float %292, i32 0
  %294 = shufflevector <8 x float> %293, <8 x float> undef, <8 x i32> zeroinitializer
  %295 = getelementptr inbounds float, float* %12, i64 %289
  %296 = bitcast float* %295 to <8 x float>*
  %wide.load35 = load <8 x float>, <8 x float>* %296, align 4, !tbaa !12, !llvm.access.group !16
  %297 = fmul <8 x float> %294, %wide.load35
  %298 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %287, <8 x float> %wide.load34, <8 x float> %297)
  %299 = add nsw i32 %mul.i.i.3, %284
  %300 = sext i32 %299 to i64
  %301 = getelementptr inbounds float, float* %6, i64 %300
  %302 = bitcast float* %301 to <8 x float>*
  %wide.load36 = load <8 x float>, <8 x float>* %302, align 4, !tbaa !12, !llvm.access.group !16
  %303 = fadd <8 x float> %wide.load36, %298
  %304 = bitcast float* %301 to <8 x float>*
  store <8 x float> %303, <8 x float>* %304, align 4, !tbaa !12, !llvm.access.group !16
  %305 = or i64 %mul.i.i.i, 8
  %306 = trunc i64 %305 to i32
  %307 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %308 = insertelement <8 x float> undef, float %307, i32 0
  %309 = shufflevector <8 x float> %308, <8 x float> undef, <8 x i32> zeroinitializer
  %310 = shl i64 %305, 32
  %311 = ashr exact i64 %310, 32
  %312 = getelementptr inbounds float, float* %9, i64 %311
  %313 = bitcast float* %312 to <8 x float>*
  %wide.load34.1 = load <8 x float>, <8 x float>* %313, align 4, !tbaa !12, !llvm.access.group !16
  %314 = load float, float* %arrayidx6.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %315 = insertelement <8 x float> undef, float %314, i32 0
  %316 = shufflevector <8 x float> %315, <8 x float> undef, <8 x i32> zeroinitializer
  %317 = getelementptr inbounds float, float* %12, i64 %311
  %318 = bitcast float* %317 to <8 x float>*
  %wide.load35.1 = load <8 x float>, <8 x float>* %318, align 4, !tbaa !12, !llvm.access.group !16
  %319 = fmul <8 x float> %316, %wide.load35.1
  %320 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %309, <8 x float> %wide.load34.1, <8 x float> %319)
  %321 = add nsw i32 %mul.i.i.3, %306
  %322 = sext i32 %321 to i64
  %323 = getelementptr inbounds float, float* %6, i64 %322
  %324 = bitcast float* %323 to <8 x float>*
  %wide.load36.1 = load <8 x float>, <8 x float>* %324, align 4, !tbaa !12, !llvm.access.group !16
  %325 = fadd <8 x float> %wide.load36.1, %320
  %326 = bitcast float* %323 to <8 x float>*
  store <8 x float> %325, <8 x float>* %326, align 4, !tbaa !12, !llvm.access.group !16
  %327 = or i64 %mul.i.i.i, 16
  %328 = trunc i64 %327 to i32
  %329 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %330 = insertelement <8 x float> undef, float %329, i32 0
  %331 = shufflevector <8 x float> %330, <8 x float> undef, <8 x i32> zeroinitializer
  %332 = shl i64 %327, 32
  %333 = ashr exact i64 %332, 32
  %334 = getelementptr inbounds float, float* %9, i64 %333
  %335 = bitcast float* %334 to <8 x float>*
  %wide.load34.2 = load <8 x float>, <8 x float>* %335, align 4, !tbaa !12, !llvm.access.group !16
  %336 = load float, float* %arrayidx6.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %337 = insertelement <8 x float> undef, float %336, i32 0
  %338 = shufflevector <8 x float> %337, <8 x float> undef, <8 x i32> zeroinitializer
  %339 = getelementptr inbounds float, float* %12, i64 %333
  %340 = bitcast float* %339 to <8 x float>*
  %wide.load35.2 = load <8 x float>, <8 x float>* %340, align 4, !tbaa !12, !llvm.access.group !16
  %341 = fmul <8 x float> %338, %wide.load35.2
  %342 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %331, <8 x float> %wide.load34.2, <8 x float> %341)
  %343 = add nsw i32 %mul.i.i.3, %328
  %344 = sext i32 %343 to i64
  %345 = getelementptr inbounds float, float* %6, i64 %344
  %346 = bitcast float* %345 to <8 x float>*
  %wide.load36.2 = load <8 x float>, <8 x float>* %346, align 4, !tbaa !12, !llvm.access.group !16
  %347 = fadd <8 x float> %wide.load36.2, %342
  %348 = bitcast float* %345 to <8 x float>*
  store <8 x float> %347, <8 x float>* %348, align 4, !tbaa !12, !llvm.access.group !16
  %349 = or i64 %mul.i.i.i, 24
  %350 = trunc i64 %349 to i32
  %351 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %352 = insertelement <8 x float> undef, float %351, i32 0
  %353 = shufflevector <8 x float> %352, <8 x float> undef, <8 x i32> zeroinitializer
  %354 = shl i64 %349, 32
  %355 = ashr exact i64 %354, 32
  %356 = getelementptr inbounds float, float* %9, i64 %355
  %357 = bitcast float* %356 to <8 x float>*
  %wide.load34.3 = load <8 x float>, <8 x float>* %357, align 4, !tbaa !12, !llvm.access.group !16
  %358 = load float, float* %arrayidx6.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %359 = insertelement <8 x float> undef, float %358, i32 0
  %360 = shufflevector <8 x float> %359, <8 x float> undef, <8 x i32> zeroinitializer
  %361 = getelementptr inbounds float, float* %12, i64 %355
  %362 = bitcast float* %361 to <8 x float>*
  %wide.load35.3 = load <8 x float>, <8 x float>* %362, align 4, !tbaa !12, !llvm.access.group !16
  %363 = fmul <8 x float> %360, %wide.load35.3
  %364 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %353, <8 x float> %wide.load34.3, <8 x float> %363)
  %365 = add nsw i32 %mul.i.i.3, %350
  %366 = sext i32 %365 to i64
  %367 = getelementptr inbounds float, float* %6, i64 %366
  %368 = bitcast float* %367 to <8 x float>*
  %wide.load36.3 = load <8 x float>, <8 x float>* %368, align 4, !tbaa !12, !llvm.access.group !16
  %369 = fadd <8 x float> %wide.load36.3, %364
  %370 = bitcast float* %367 to <8 x float>*
  store <8 x float> %369, <8 x float>* %370, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.4 = or i64 %mul3.i.i.i, 4
  %conv2.i.i.4 = trunc i64 %add6.i.i.i.4 to i32
  %sext.i.i.4 = shl i64 %add6.i.i.i.4, 32
  %idxprom.i.i.4 = ashr exact i64 %sext.i.i.4, 32
  %arrayidx.i.i.4 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.4
  %arrayidx6.i.i.4 = getelementptr inbounds float, float* %18, i64 %idxprom.i.i.4
  %mul.i.i.4 = mul nsw i32 %22, %conv2.i.i.4
  %371 = trunc i64 %mul.i.i.i to i32
  %372 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %373 = insertelement <8 x float> undef, float %372, i32 0
  %374 = shufflevector <8 x float> %373, <8 x float> undef, <8 x i32> zeroinitializer
  %375 = shl i64 %2, 37
  %376 = ashr exact i64 %375, 32
  %377 = getelementptr inbounds float, float* %9, i64 %376
  %378 = bitcast float* %377 to <8 x float>*
  %wide.load45 = load <8 x float>, <8 x float>* %378, align 4, !tbaa !12, !llvm.access.group !16
  %379 = load float, float* %arrayidx6.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %380 = insertelement <8 x float> undef, float %379, i32 0
  %381 = shufflevector <8 x float> %380, <8 x float> undef, <8 x i32> zeroinitializer
  %382 = getelementptr inbounds float, float* %12, i64 %376
  %383 = bitcast float* %382 to <8 x float>*
  %wide.load46 = load <8 x float>, <8 x float>* %383, align 4, !tbaa !12, !llvm.access.group !16
  %384 = fmul <8 x float> %381, %wide.load46
  %385 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %374, <8 x float> %wide.load45, <8 x float> %384)
  %386 = add nsw i32 %mul.i.i.4, %371
  %387 = sext i32 %386 to i64
  %388 = getelementptr inbounds float, float* %6, i64 %387
  %389 = bitcast float* %388 to <8 x float>*
  %wide.load47 = load <8 x float>, <8 x float>* %389, align 4, !tbaa !12, !llvm.access.group !16
  %390 = fadd <8 x float> %wide.load47, %385
  %391 = bitcast float* %388 to <8 x float>*
  store <8 x float> %390, <8 x float>* %391, align 4, !tbaa !12, !llvm.access.group !16
  %392 = or i64 %mul.i.i.i, 8
  %393 = trunc i64 %392 to i32
  %394 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %395 = insertelement <8 x float> undef, float %394, i32 0
  %396 = shufflevector <8 x float> %395, <8 x float> undef, <8 x i32> zeroinitializer
  %397 = shl i64 %392, 32
  %398 = ashr exact i64 %397, 32
  %399 = getelementptr inbounds float, float* %9, i64 %398
  %400 = bitcast float* %399 to <8 x float>*
  %wide.load45.1 = load <8 x float>, <8 x float>* %400, align 4, !tbaa !12, !llvm.access.group !16
  %401 = load float, float* %arrayidx6.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %402 = insertelement <8 x float> undef, float %401, i32 0
  %403 = shufflevector <8 x float> %402, <8 x float> undef, <8 x i32> zeroinitializer
  %404 = getelementptr inbounds float, float* %12, i64 %398
  %405 = bitcast float* %404 to <8 x float>*
  %wide.load46.1 = load <8 x float>, <8 x float>* %405, align 4, !tbaa !12, !llvm.access.group !16
  %406 = fmul <8 x float> %403, %wide.load46.1
  %407 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %396, <8 x float> %wide.load45.1, <8 x float> %406)
  %408 = add nsw i32 %mul.i.i.4, %393
  %409 = sext i32 %408 to i64
  %410 = getelementptr inbounds float, float* %6, i64 %409
  %411 = bitcast float* %410 to <8 x float>*
  %wide.load47.1 = load <8 x float>, <8 x float>* %411, align 4, !tbaa !12, !llvm.access.group !16
  %412 = fadd <8 x float> %wide.load47.1, %407
  %413 = bitcast float* %410 to <8 x float>*
  store <8 x float> %412, <8 x float>* %413, align 4, !tbaa !12, !llvm.access.group !16
  %414 = or i64 %mul.i.i.i, 16
  %415 = trunc i64 %414 to i32
  %416 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %417 = insertelement <8 x float> undef, float %416, i32 0
  %418 = shufflevector <8 x float> %417, <8 x float> undef, <8 x i32> zeroinitializer
  %419 = shl i64 %414, 32
  %420 = ashr exact i64 %419, 32
  %421 = getelementptr inbounds float, float* %9, i64 %420
  %422 = bitcast float* %421 to <8 x float>*
  %wide.load45.2 = load <8 x float>, <8 x float>* %422, align 4, !tbaa !12, !llvm.access.group !16
  %423 = load float, float* %arrayidx6.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %424 = insertelement <8 x float> undef, float %423, i32 0
  %425 = shufflevector <8 x float> %424, <8 x float> undef, <8 x i32> zeroinitializer
  %426 = getelementptr inbounds float, float* %12, i64 %420
  %427 = bitcast float* %426 to <8 x float>*
  %wide.load46.2 = load <8 x float>, <8 x float>* %427, align 4, !tbaa !12, !llvm.access.group !16
  %428 = fmul <8 x float> %425, %wide.load46.2
  %429 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %418, <8 x float> %wide.load45.2, <8 x float> %428)
  %430 = add nsw i32 %mul.i.i.4, %415
  %431 = sext i32 %430 to i64
  %432 = getelementptr inbounds float, float* %6, i64 %431
  %433 = bitcast float* %432 to <8 x float>*
  %wide.load47.2 = load <8 x float>, <8 x float>* %433, align 4, !tbaa !12, !llvm.access.group !16
  %434 = fadd <8 x float> %wide.load47.2, %429
  %435 = bitcast float* %432 to <8 x float>*
  store <8 x float> %434, <8 x float>* %435, align 4, !tbaa !12, !llvm.access.group !16
  %436 = or i64 %mul.i.i.i, 24
  %437 = trunc i64 %436 to i32
  %438 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %439 = insertelement <8 x float> undef, float %438, i32 0
  %440 = shufflevector <8 x float> %439, <8 x float> undef, <8 x i32> zeroinitializer
  %441 = shl i64 %436, 32
  %442 = ashr exact i64 %441, 32
  %443 = getelementptr inbounds float, float* %9, i64 %442
  %444 = bitcast float* %443 to <8 x float>*
  %wide.load45.3 = load <8 x float>, <8 x float>* %444, align 4, !tbaa !12, !llvm.access.group !16
  %445 = load float, float* %arrayidx6.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %446 = insertelement <8 x float> undef, float %445, i32 0
  %447 = shufflevector <8 x float> %446, <8 x float> undef, <8 x i32> zeroinitializer
  %448 = getelementptr inbounds float, float* %12, i64 %442
  %449 = bitcast float* %448 to <8 x float>*
  %wide.load46.3 = load <8 x float>, <8 x float>* %449, align 4, !tbaa !12, !llvm.access.group !16
  %450 = fmul <8 x float> %447, %wide.load46.3
  %451 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %440, <8 x float> %wide.load45.3, <8 x float> %450)
  %452 = add nsw i32 %mul.i.i.4, %437
  %453 = sext i32 %452 to i64
  %454 = getelementptr inbounds float, float* %6, i64 %453
  %455 = bitcast float* %454 to <8 x float>*
  %wide.load47.3 = load <8 x float>, <8 x float>* %455, align 4, !tbaa !12, !llvm.access.group !16
  %456 = fadd <8 x float> %wide.load47.3, %451
  %457 = bitcast float* %454 to <8 x float>*
  store <8 x float> %456, <8 x float>* %457, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.5 = or i64 %mul3.i.i.i, 5
  %conv2.i.i.5 = trunc i64 %add6.i.i.i.5 to i32
  %sext.i.i.5 = shl i64 %add6.i.i.i.5, 32
  %idxprom.i.i.5 = ashr exact i64 %sext.i.i.5, 32
  %arrayidx.i.i.5 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.5
  %arrayidx6.i.i.5 = getelementptr inbounds float, float* %18, i64 %idxprom.i.i.5
  %mul.i.i.5 = mul nsw i32 %22, %conv2.i.i.5
  %458 = trunc i64 %mul.i.i.i to i32
  %459 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %460 = insertelement <8 x float> undef, float %459, i32 0
  %461 = shufflevector <8 x float> %460, <8 x float> undef, <8 x i32> zeroinitializer
  %462 = shl i64 %2, 37
  %463 = ashr exact i64 %462, 32
  %464 = getelementptr inbounds float, float* %9, i64 %463
  %465 = bitcast float* %464 to <8 x float>*
  %wide.load56 = load <8 x float>, <8 x float>* %465, align 4, !tbaa !12, !llvm.access.group !16
  %466 = load float, float* %arrayidx6.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %467 = insertelement <8 x float> undef, float %466, i32 0
  %468 = shufflevector <8 x float> %467, <8 x float> undef, <8 x i32> zeroinitializer
  %469 = getelementptr inbounds float, float* %12, i64 %463
  %470 = bitcast float* %469 to <8 x float>*
  %wide.load57 = load <8 x float>, <8 x float>* %470, align 4, !tbaa !12, !llvm.access.group !16
  %471 = fmul <8 x float> %468, %wide.load57
  %472 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %461, <8 x float> %wide.load56, <8 x float> %471)
  %473 = add nsw i32 %mul.i.i.5, %458
  %474 = sext i32 %473 to i64
  %475 = getelementptr inbounds float, float* %6, i64 %474
  %476 = bitcast float* %475 to <8 x float>*
  %wide.load58 = load <8 x float>, <8 x float>* %476, align 4, !tbaa !12, !llvm.access.group !16
  %477 = fadd <8 x float> %wide.load58, %472
  %478 = bitcast float* %475 to <8 x float>*
  store <8 x float> %477, <8 x float>* %478, align 4, !tbaa !12, !llvm.access.group !16
  %479 = or i64 %mul.i.i.i, 8
  %480 = trunc i64 %479 to i32
  %481 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %482 = insertelement <8 x float> undef, float %481, i32 0
  %483 = shufflevector <8 x float> %482, <8 x float> undef, <8 x i32> zeroinitializer
  %484 = shl i64 %479, 32
  %485 = ashr exact i64 %484, 32
  %486 = getelementptr inbounds float, float* %9, i64 %485
  %487 = bitcast float* %486 to <8 x float>*
  %wide.load56.1 = load <8 x float>, <8 x float>* %487, align 4, !tbaa !12, !llvm.access.group !16
  %488 = load float, float* %arrayidx6.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %489 = insertelement <8 x float> undef, float %488, i32 0
  %490 = shufflevector <8 x float> %489, <8 x float> undef, <8 x i32> zeroinitializer
  %491 = getelementptr inbounds float, float* %12, i64 %485
  %492 = bitcast float* %491 to <8 x float>*
  %wide.load57.1 = load <8 x float>, <8 x float>* %492, align 4, !tbaa !12, !llvm.access.group !16
  %493 = fmul <8 x float> %490, %wide.load57.1
  %494 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %483, <8 x float> %wide.load56.1, <8 x float> %493)
  %495 = add nsw i32 %mul.i.i.5, %480
  %496 = sext i32 %495 to i64
  %497 = getelementptr inbounds float, float* %6, i64 %496
  %498 = bitcast float* %497 to <8 x float>*
  %wide.load58.1 = load <8 x float>, <8 x float>* %498, align 4, !tbaa !12, !llvm.access.group !16
  %499 = fadd <8 x float> %wide.load58.1, %494
  %500 = bitcast float* %497 to <8 x float>*
  store <8 x float> %499, <8 x float>* %500, align 4, !tbaa !12, !llvm.access.group !16
  %501 = or i64 %mul.i.i.i, 16
  %502 = trunc i64 %501 to i32
  %503 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %504 = insertelement <8 x float> undef, float %503, i32 0
  %505 = shufflevector <8 x float> %504, <8 x float> undef, <8 x i32> zeroinitializer
  %506 = shl i64 %501, 32
  %507 = ashr exact i64 %506, 32
  %508 = getelementptr inbounds float, float* %9, i64 %507
  %509 = bitcast float* %508 to <8 x float>*
  %wide.load56.2 = load <8 x float>, <8 x float>* %509, align 4, !tbaa !12, !llvm.access.group !16
  %510 = load float, float* %arrayidx6.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %511 = insertelement <8 x float> undef, float %510, i32 0
  %512 = shufflevector <8 x float> %511, <8 x float> undef, <8 x i32> zeroinitializer
  %513 = getelementptr inbounds float, float* %12, i64 %507
  %514 = bitcast float* %513 to <8 x float>*
  %wide.load57.2 = load <8 x float>, <8 x float>* %514, align 4, !tbaa !12, !llvm.access.group !16
  %515 = fmul <8 x float> %512, %wide.load57.2
  %516 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %505, <8 x float> %wide.load56.2, <8 x float> %515)
  %517 = add nsw i32 %mul.i.i.5, %502
  %518 = sext i32 %517 to i64
  %519 = getelementptr inbounds float, float* %6, i64 %518
  %520 = bitcast float* %519 to <8 x float>*
  %wide.load58.2 = load <8 x float>, <8 x float>* %520, align 4, !tbaa !12, !llvm.access.group !16
  %521 = fadd <8 x float> %wide.load58.2, %516
  %522 = bitcast float* %519 to <8 x float>*
  store <8 x float> %521, <8 x float>* %522, align 4, !tbaa !12, !llvm.access.group !16
  %523 = or i64 %mul.i.i.i, 24
  %524 = trunc i64 %523 to i32
  %525 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %526 = insertelement <8 x float> undef, float %525, i32 0
  %527 = shufflevector <8 x float> %526, <8 x float> undef, <8 x i32> zeroinitializer
  %528 = shl i64 %523, 32
  %529 = ashr exact i64 %528, 32
  %530 = getelementptr inbounds float, float* %9, i64 %529
  %531 = bitcast float* %530 to <8 x float>*
  %wide.load56.3 = load <8 x float>, <8 x float>* %531, align 4, !tbaa !12, !llvm.access.group !16
  %532 = load float, float* %arrayidx6.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %533 = insertelement <8 x float> undef, float %532, i32 0
  %534 = shufflevector <8 x float> %533, <8 x float> undef, <8 x i32> zeroinitializer
  %535 = getelementptr inbounds float, float* %12, i64 %529
  %536 = bitcast float* %535 to <8 x float>*
  %wide.load57.3 = load <8 x float>, <8 x float>* %536, align 4, !tbaa !12, !llvm.access.group !16
  %537 = fmul <8 x float> %534, %wide.load57.3
  %538 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %527, <8 x float> %wide.load56.3, <8 x float> %537)
  %539 = add nsw i32 %mul.i.i.5, %524
  %540 = sext i32 %539 to i64
  %541 = getelementptr inbounds float, float* %6, i64 %540
  %542 = bitcast float* %541 to <8 x float>*
  %wide.load58.3 = load <8 x float>, <8 x float>* %542, align 4, !tbaa !12, !llvm.access.group !16
  %543 = fadd <8 x float> %wide.load58.3, %538
  %544 = bitcast float* %541 to <8 x float>*
  store <8 x float> %543, <8 x float>* %544, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.6 = or i64 %mul3.i.i.i, 6
  %conv2.i.i.6 = trunc i64 %add6.i.i.i.6 to i32
  %sext.i.i.6 = shl i64 %add6.i.i.i.6, 32
  %idxprom.i.i.6 = ashr exact i64 %sext.i.i.6, 32
  %arrayidx.i.i.6 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.6
  %arrayidx6.i.i.6 = getelementptr inbounds float, float* %18, i64 %idxprom.i.i.6
  %mul.i.i.6 = mul nsw i32 %22, %conv2.i.i.6
  %545 = trunc i64 %mul.i.i.i to i32
  %546 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %547 = insertelement <8 x float> undef, float %546, i32 0
  %548 = shufflevector <8 x float> %547, <8 x float> undef, <8 x i32> zeroinitializer
  %549 = shl i64 %2, 37
  %550 = ashr exact i64 %549, 32
  %551 = getelementptr inbounds float, float* %9, i64 %550
  %552 = bitcast float* %551 to <8 x float>*
  %wide.load67 = load <8 x float>, <8 x float>* %552, align 4, !tbaa !12, !llvm.access.group !16
  %553 = load float, float* %arrayidx6.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %554 = insertelement <8 x float> undef, float %553, i32 0
  %555 = shufflevector <8 x float> %554, <8 x float> undef, <8 x i32> zeroinitializer
  %556 = getelementptr inbounds float, float* %12, i64 %550
  %557 = bitcast float* %556 to <8 x float>*
  %wide.load68 = load <8 x float>, <8 x float>* %557, align 4, !tbaa !12, !llvm.access.group !16
  %558 = fmul <8 x float> %555, %wide.load68
  %559 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %548, <8 x float> %wide.load67, <8 x float> %558)
  %560 = add nsw i32 %mul.i.i.6, %545
  %561 = sext i32 %560 to i64
  %562 = getelementptr inbounds float, float* %6, i64 %561
  %563 = bitcast float* %562 to <8 x float>*
  %wide.load69 = load <8 x float>, <8 x float>* %563, align 4, !tbaa !12, !llvm.access.group !16
  %564 = fadd <8 x float> %wide.load69, %559
  %565 = bitcast float* %562 to <8 x float>*
  store <8 x float> %564, <8 x float>* %565, align 4, !tbaa !12, !llvm.access.group !16
  %566 = or i64 %mul.i.i.i, 8
  %567 = trunc i64 %566 to i32
  %568 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %569 = insertelement <8 x float> undef, float %568, i32 0
  %570 = shufflevector <8 x float> %569, <8 x float> undef, <8 x i32> zeroinitializer
  %571 = shl i64 %566, 32
  %572 = ashr exact i64 %571, 32
  %573 = getelementptr inbounds float, float* %9, i64 %572
  %574 = bitcast float* %573 to <8 x float>*
  %wide.load67.1 = load <8 x float>, <8 x float>* %574, align 4, !tbaa !12, !llvm.access.group !16
  %575 = load float, float* %arrayidx6.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %576 = insertelement <8 x float> undef, float %575, i32 0
  %577 = shufflevector <8 x float> %576, <8 x float> undef, <8 x i32> zeroinitializer
  %578 = getelementptr inbounds float, float* %12, i64 %572
  %579 = bitcast float* %578 to <8 x float>*
  %wide.load68.1 = load <8 x float>, <8 x float>* %579, align 4, !tbaa !12, !llvm.access.group !16
  %580 = fmul <8 x float> %577, %wide.load68.1
  %581 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %570, <8 x float> %wide.load67.1, <8 x float> %580)
  %582 = add nsw i32 %mul.i.i.6, %567
  %583 = sext i32 %582 to i64
  %584 = getelementptr inbounds float, float* %6, i64 %583
  %585 = bitcast float* %584 to <8 x float>*
  %wide.load69.1 = load <8 x float>, <8 x float>* %585, align 4, !tbaa !12, !llvm.access.group !16
  %586 = fadd <8 x float> %wide.load69.1, %581
  %587 = bitcast float* %584 to <8 x float>*
  store <8 x float> %586, <8 x float>* %587, align 4, !tbaa !12, !llvm.access.group !16
  %588 = or i64 %mul.i.i.i, 16
  %589 = trunc i64 %588 to i32
  %590 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %591 = insertelement <8 x float> undef, float %590, i32 0
  %592 = shufflevector <8 x float> %591, <8 x float> undef, <8 x i32> zeroinitializer
  %593 = shl i64 %588, 32
  %594 = ashr exact i64 %593, 32
  %595 = getelementptr inbounds float, float* %9, i64 %594
  %596 = bitcast float* %595 to <8 x float>*
  %wide.load67.2 = load <8 x float>, <8 x float>* %596, align 4, !tbaa !12, !llvm.access.group !16
  %597 = load float, float* %arrayidx6.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %598 = insertelement <8 x float> undef, float %597, i32 0
  %599 = shufflevector <8 x float> %598, <8 x float> undef, <8 x i32> zeroinitializer
  %600 = getelementptr inbounds float, float* %12, i64 %594
  %601 = bitcast float* %600 to <8 x float>*
  %wide.load68.2 = load <8 x float>, <8 x float>* %601, align 4, !tbaa !12, !llvm.access.group !16
  %602 = fmul <8 x float> %599, %wide.load68.2
  %603 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %592, <8 x float> %wide.load67.2, <8 x float> %602)
  %604 = add nsw i32 %mul.i.i.6, %589
  %605 = sext i32 %604 to i64
  %606 = getelementptr inbounds float, float* %6, i64 %605
  %607 = bitcast float* %606 to <8 x float>*
  %wide.load69.2 = load <8 x float>, <8 x float>* %607, align 4, !tbaa !12, !llvm.access.group !16
  %608 = fadd <8 x float> %wide.load69.2, %603
  %609 = bitcast float* %606 to <8 x float>*
  store <8 x float> %608, <8 x float>* %609, align 4, !tbaa !12, !llvm.access.group !16
  %610 = or i64 %mul.i.i.i, 24
  %611 = trunc i64 %610 to i32
  %612 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %613 = insertelement <8 x float> undef, float %612, i32 0
  %614 = shufflevector <8 x float> %613, <8 x float> undef, <8 x i32> zeroinitializer
  %615 = shl i64 %610, 32
  %616 = ashr exact i64 %615, 32
  %617 = getelementptr inbounds float, float* %9, i64 %616
  %618 = bitcast float* %617 to <8 x float>*
  %wide.load67.3 = load <8 x float>, <8 x float>* %618, align 4, !tbaa !12, !llvm.access.group !16
  %619 = load float, float* %arrayidx6.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %620 = insertelement <8 x float> undef, float %619, i32 0
  %621 = shufflevector <8 x float> %620, <8 x float> undef, <8 x i32> zeroinitializer
  %622 = getelementptr inbounds float, float* %12, i64 %616
  %623 = bitcast float* %622 to <8 x float>*
  %wide.load68.3 = load <8 x float>, <8 x float>* %623, align 4, !tbaa !12, !llvm.access.group !16
  %624 = fmul <8 x float> %621, %wide.load68.3
  %625 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %614, <8 x float> %wide.load67.3, <8 x float> %624)
  %626 = add nsw i32 %mul.i.i.6, %611
  %627 = sext i32 %626 to i64
  %628 = getelementptr inbounds float, float* %6, i64 %627
  %629 = bitcast float* %628 to <8 x float>*
  %wide.load69.3 = load <8 x float>, <8 x float>* %629, align 4, !tbaa !12, !llvm.access.group !16
  %630 = fadd <8 x float> %wide.load69.3, %625
  %631 = bitcast float* %628 to <8 x float>*
  store <8 x float> %630, <8 x float>* %631, align 4, !tbaa !12, !llvm.access.group !16
  %add6.i.i.i.7 = or i64 %mul3.i.i.i, 7
  %conv2.i.i.7 = trunc i64 %add6.i.i.i.7 to i32
  %sext.i.i.7 = shl i64 %add6.i.i.i.7, 32
  %idxprom.i.i.7 = ashr exact i64 %sext.i.i.7, 32
  %arrayidx.i.i.7 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.7
  %arrayidx6.i.i.7 = getelementptr inbounds float, float* %18, i64 %idxprom.i.i.7
  %mul.i.i.7 = mul nsw i32 %22, %conv2.i.i.7
  %632 = trunc i64 %mul.i.i.i to i32
  %633 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %634 = insertelement <8 x float> undef, float %633, i32 0
  %635 = shufflevector <8 x float> %634, <8 x float> undef, <8 x i32> zeroinitializer
  %636 = shl i64 %2, 37
  %637 = ashr exact i64 %636, 32
  %638 = getelementptr inbounds float, float* %9, i64 %637
  %639 = bitcast float* %638 to <8 x float>*
  %wide.load78 = load <8 x float>, <8 x float>* %639, align 4, !tbaa !12, !llvm.access.group !16
  %640 = load float, float* %arrayidx6.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %641 = insertelement <8 x float> undef, float %640, i32 0
  %642 = shufflevector <8 x float> %641, <8 x float> undef, <8 x i32> zeroinitializer
  %643 = getelementptr inbounds float, float* %12, i64 %637
  %644 = bitcast float* %643 to <8 x float>*
  %wide.load79 = load <8 x float>, <8 x float>* %644, align 4, !tbaa !12, !llvm.access.group !16
  %645 = fmul <8 x float> %642, %wide.load79
  %646 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %635, <8 x float> %wide.load78, <8 x float> %645)
  %647 = add nsw i32 %mul.i.i.7, %632
  %648 = sext i32 %647 to i64
  %649 = getelementptr inbounds float, float* %6, i64 %648
  %650 = bitcast float* %649 to <8 x float>*
  %wide.load80 = load <8 x float>, <8 x float>* %650, align 4, !tbaa !12, !llvm.access.group !16
  %651 = fadd <8 x float> %wide.load80, %646
  %652 = bitcast float* %649 to <8 x float>*
  store <8 x float> %651, <8 x float>* %652, align 4, !tbaa !12, !llvm.access.group !16
  %653 = or i64 %mul.i.i.i, 8
  %654 = trunc i64 %653 to i32
  %655 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %656 = insertelement <8 x float> undef, float %655, i32 0
  %657 = shufflevector <8 x float> %656, <8 x float> undef, <8 x i32> zeroinitializer
  %658 = shl i64 %653, 32
  %659 = ashr exact i64 %658, 32
  %660 = getelementptr inbounds float, float* %9, i64 %659
  %661 = bitcast float* %660 to <8 x float>*
  %wide.load78.1 = load <8 x float>, <8 x float>* %661, align 4, !tbaa !12, !llvm.access.group !16
  %662 = load float, float* %arrayidx6.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %663 = insertelement <8 x float> undef, float %662, i32 0
  %664 = shufflevector <8 x float> %663, <8 x float> undef, <8 x i32> zeroinitializer
  %665 = getelementptr inbounds float, float* %12, i64 %659
  %666 = bitcast float* %665 to <8 x float>*
  %wide.load79.1 = load <8 x float>, <8 x float>* %666, align 4, !tbaa !12, !llvm.access.group !16
  %667 = fmul <8 x float> %664, %wide.load79.1
  %668 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %657, <8 x float> %wide.load78.1, <8 x float> %667)
  %669 = add nsw i32 %mul.i.i.7, %654
  %670 = sext i32 %669 to i64
  %671 = getelementptr inbounds float, float* %6, i64 %670
  %672 = bitcast float* %671 to <8 x float>*
  %wide.load80.1 = load <8 x float>, <8 x float>* %672, align 4, !tbaa !12, !llvm.access.group !16
  %673 = fadd <8 x float> %wide.load80.1, %668
  %674 = bitcast float* %671 to <8 x float>*
  store <8 x float> %673, <8 x float>* %674, align 4, !tbaa !12, !llvm.access.group !16
  %675 = or i64 %mul.i.i.i, 16
  %676 = trunc i64 %675 to i32
  %677 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %678 = insertelement <8 x float> undef, float %677, i32 0
  %679 = shufflevector <8 x float> %678, <8 x float> undef, <8 x i32> zeroinitializer
  %680 = shl i64 %675, 32
  %681 = ashr exact i64 %680, 32
  %682 = getelementptr inbounds float, float* %9, i64 %681
  %683 = bitcast float* %682 to <8 x float>*
  %wide.load78.2 = load <8 x float>, <8 x float>* %683, align 4, !tbaa !12, !llvm.access.group !16
  %684 = load float, float* %arrayidx6.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %685 = insertelement <8 x float> undef, float %684, i32 0
  %686 = shufflevector <8 x float> %685, <8 x float> undef, <8 x i32> zeroinitializer
  %687 = getelementptr inbounds float, float* %12, i64 %681
  %688 = bitcast float* %687 to <8 x float>*
  %wide.load79.2 = load <8 x float>, <8 x float>* %688, align 4, !tbaa !12, !llvm.access.group !16
  %689 = fmul <8 x float> %686, %wide.load79.2
  %690 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %679, <8 x float> %wide.load78.2, <8 x float> %689)
  %691 = add nsw i32 %mul.i.i.7, %676
  %692 = sext i32 %691 to i64
  %693 = getelementptr inbounds float, float* %6, i64 %692
  %694 = bitcast float* %693 to <8 x float>*
  %wide.load80.2 = load <8 x float>, <8 x float>* %694, align 4, !tbaa !12, !llvm.access.group !16
  %695 = fadd <8 x float> %wide.load80.2, %690
  %696 = bitcast float* %693 to <8 x float>*
  store <8 x float> %695, <8 x float>* %696, align 4, !tbaa !12, !llvm.access.group !16
  %697 = or i64 %mul.i.i.i, 24
  %698 = trunc i64 %697 to i32
  %699 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %700 = insertelement <8 x float> undef, float %699, i32 0
  %701 = shufflevector <8 x float> %700, <8 x float> undef, <8 x i32> zeroinitializer
  %702 = shl i64 %697, 32
  %703 = ashr exact i64 %702, 32
  %704 = getelementptr inbounds float, float* %9, i64 %703
  %705 = bitcast float* %704 to <8 x float>*
  %wide.load78.3 = load <8 x float>, <8 x float>* %705, align 4, !tbaa !12, !llvm.access.group !16
  %706 = load float, float* %arrayidx6.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %707 = insertelement <8 x float> undef, float %706, i32 0
  %708 = shufflevector <8 x float> %707, <8 x float> undef, <8 x i32> zeroinitializer
  %709 = getelementptr inbounds float, float* %12, i64 %703
  %710 = bitcast float* %709 to <8 x float>*
  %wide.load79.3 = load <8 x float>, <8 x float>* %710, align 4, !tbaa !12, !llvm.access.group !16
  %711 = fmul <8 x float> %708, %wide.load79.3
  %712 = call <8 x float> @llvm.fmuladd.v8f32(<8 x float> %701, <8 x float> %wide.load78.3, <8 x float> %711)
  %713 = add nsw i32 %mul.i.i.7, %698
  %714 = sext i32 %713 to i64
  %715 = getelementptr inbounds float, float* %6, i64 %714
  %716 = bitcast float* %715 to <8 x float>*
  %wide.load80.3 = load <8 x float>, <8 x float>* %716, align 4, !tbaa !12, !llvm.access.group !16
  %717 = fadd <8 x float> %wide.load80.3, %712
  %718 = bitcast float* %715 to <8 x float>*
  store <8 x float> %717, <8 x float>* %718, align 4, !tbaa !12, !llvm.access.group !16
  ret void
}

; Function Attrs: nounwind readnone speculatable willreturn
declare <8 x float> @llvm.fmuladd.v8f32(<8 x float>, <8 x float>, <8 x float>) #2

attributes #0 = { alwaysinline nofree norecurse nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "stackrealign" "target-cpu"="skylake" "target-features"="+adx,+aes,+avx,+avx2,+bmi,+bmi2,+clflushopt,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+prfchw,+rdrnd,+rdseed,+sahf,+sgx,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "uniform-work-group-size"="true" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nofree nounwind }
attributes #2 = { nounwind readnone speculatable willreturn }

!llvm.module.flags = !{!0, !1, !2}
!opencl.ocl.version = !{!3}
!llvm.ident = !{!4}
!opencl.spir.version = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 1, i32 2}
!4 = !{!"clang version 11.0.0 (git@github.com:llvm/llvm-project.git 91e89f9a5115b0f83b8f026e1ad0e6d1f885fa9b)"}
!5 = !{i32 1, i32 1, i32 1, i32 1, i32 1, i32 0}
!6 = !{!"none", !"none", !"none", !"none", !"none", !"none"}
!7 = !{!"DATA_TYPE*", !"DATA_TYPE*", !"DATA_TYPE*", !"DATA_TYPE*", !"DATA_TYPE*", !"int"}
!8 = !{!"float*", !"float*", !"float*", !"float*", !"float*", !"int"}
!9 = !{!"", !"", !"", !"", !"", !""}
!10 = !{!"A", !"V1", !"V2", !"U1", !"U2", !"n"}
!11 = !{i32 1}
!12 = !{!13, !13, i64 0}
!13 = !{!"float", !14, i64 0}
!14 = !{!"omnipotent char", !15, i64 0}
!15 = !{!"Simple C/C++ TBAA"}
!16 = !{!17, !18}
!17 = distinct !{}
!18 = distinct !{}
