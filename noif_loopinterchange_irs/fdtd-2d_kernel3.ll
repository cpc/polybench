; ModuleID = './JA/PPGOFAKNODIDBFJENOEJALAJOIPFMPCLGFIGI/fdtd_kernel3/32-8-1-goffs0-smallgrid/parallel.bc'
source_filename = "parallel_bc"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind readnone speculatable willreturn
declare double @llvm.fmuladd.f64(double, double, double) #0

; Function Attrs: alwaysinline nofree norecurse nounwind
define void @_pocl_kernel_fdtd_kernel3(float* nocapture readonly %0, float* nocapture readonly %1, float* nocapture %2, i32 %3, i32 %4, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %5, i64 %6, i64 %7, i64 %8) local_unnamed_addr #1 !kernel_arg_addr_space !5 !kernel_arg_access_qual !6 !kernel_arg_type !7 !kernel_arg_base_type !8 !kernel_arg_type_qual !9 !kernel_arg_name !10 !pocl_generated !11 {
pregion_for_entry.pregion_for_init.i:
  %mul.i.i = shl i64 %6, 5
  %mul3.i.i = shl i64 %7, 3
  %conv2.i = trunc i64 %mul3.i.i to i32
  %mul.i = mul nsw i32 %conv2.i, %4
  %add13.i = or i32 %conv2.i, 1
  %mul14.i = mul nsw i32 %add13.i, %4
  %9 = trunc i64 %7 to i32
  %10 = mul i32 %9, %4
  %11 = shl i32 %10, 3
  %12 = trunc i64 %6 to i32
  %13 = shl i32 %12, 5
  %14 = add i32 %11, %13
  %15 = icmp sgt i32 %14, 2147483616
  %16 = add i32 %11, %13
  %17 = or i32 %16, 1
  %18 = icmp sgt i32 %17, 2147483616
  %19 = or i1 %15, %18
  %20 = mul i32 %add13.i, %4
  %21 = add nsw i32 %20, %13
  %22 = icmp sgt i32 %21, 2147483616
  %23 = or i1 %19, %22
  br i1 %23, label %pregion_for_entry.entry.i.preheader, label %vector.body

pregion_for_entry.entry.i.preheader:              ; preds = %pregion_for_entry.pregion_for_init.i
  br label %pregion_for_entry.entry.i

vector.body:                                      ; preds = %pregion_for_entry.pregion_for_init.i
  %24 = trunc i64 %mul.i.i to i32
  %25 = add i32 %mul.i, %24
  %26 = sext i32 %25 to i64
  %27 = getelementptr inbounds float, float* %2, i64 %26
  %28 = bitcast float* %27 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %28, align 4, !tbaa !12, !llvm.access.group !16
  %29 = fpext <8 x float> %wide.load to <8 x double>
  %30 = or i32 %25, 1
  %31 = sext i32 %30 to i64
  %32 = getelementptr inbounds float, float* %0, i64 %31
  %33 = bitcast float* %32 to <8 x float>*
  %wide.load8 = load <8 x float>, <8 x float>* %33, align 4, !tbaa !12, !llvm.access.group !16
  %34 = getelementptr inbounds float, float* %0, i64 %26
  %35 = bitcast float* %34 to <8 x float>*
  %wide.load9 = load <8 x float>, <8 x float>* %35, align 4, !tbaa !12, !llvm.access.group !16
  %36 = fsub <8 x float> %wide.load8, %wide.load9
  %37 = add nsw i32 %mul14.i, %24
  %38 = sext i32 %37 to i64
  %39 = getelementptr inbounds float, float* %1, i64 %38
  %40 = bitcast float* %39 to <8 x float>*
  %wide.load10 = load <8 x float>, <8 x float>* %40, align 4, !tbaa !12, !llvm.access.group !16
  %41 = fadd <8 x float> %36, %wide.load10
  %42 = getelementptr inbounds float, float* %1, i64 %26
  %43 = bitcast float* %42 to <8 x float>*
  %wide.load11 = load <8 x float>, <8 x float>* %43, align 4, !tbaa !12, !llvm.access.group !16
  %44 = fsub <8 x float> %41, %wide.load11
  %45 = fpext <8 x float> %44 to <8 x double>
  %46 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %45, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %29)
  %47 = fptrunc <8 x double> %46 to <8 x float>
  %48 = bitcast float* %27 to <8 x float>*
  store <8 x float> %47, <8 x float>* %48, align 4, !tbaa !12, !llvm.access.group !16
  %49 = trunc i64 %mul.i.i to i32
  %50 = or i32 %49, 8
  %51 = add i32 %mul.i, %50
  %52 = sext i32 %51 to i64
  %53 = getelementptr inbounds float, float* %2, i64 %52
  %54 = bitcast float* %53 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %54, align 4, !tbaa !12, !llvm.access.group !16
  %55 = fpext <8 x float> %wide.load.1 to <8 x double>
  %56 = or i32 %51, 1
  %57 = sext i32 %56 to i64
  %58 = getelementptr inbounds float, float* %0, i64 %57
  %59 = bitcast float* %58 to <8 x float>*
  %wide.load8.1 = load <8 x float>, <8 x float>* %59, align 4, !tbaa !12, !llvm.access.group !16
  %60 = getelementptr inbounds float, float* %0, i64 %52
  %61 = bitcast float* %60 to <8 x float>*
  %wide.load9.1 = load <8 x float>, <8 x float>* %61, align 4, !tbaa !12, !llvm.access.group !16
  %62 = fsub <8 x float> %wide.load8.1, %wide.load9.1
  %63 = add nsw i32 %mul14.i, %50
  %64 = sext i32 %63 to i64
  %65 = getelementptr inbounds float, float* %1, i64 %64
  %66 = bitcast float* %65 to <8 x float>*
  %wide.load10.1 = load <8 x float>, <8 x float>* %66, align 4, !tbaa !12, !llvm.access.group !16
  %67 = fadd <8 x float> %62, %wide.load10.1
  %68 = getelementptr inbounds float, float* %1, i64 %52
  %69 = bitcast float* %68 to <8 x float>*
  %wide.load11.1 = load <8 x float>, <8 x float>* %69, align 4, !tbaa !12, !llvm.access.group !16
  %70 = fsub <8 x float> %67, %wide.load11.1
  %71 = fpext <8 x float> %70 to <8 x double>
  %72 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %71, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %55)
  %73 = fptrunc <8 x double> %72 to <8 x float>
  %74 = bitcast float* %53 to <8 x float>*
  store <8 x float> %73, <8 x float>* %74, align 4, !tbaa !12, !llvm.access.group !16
  %75 = trunc i64 %mul.i.i to i32
  %76 = or i32 %75, 16
  %77 = add i32 %mul.i, %76
  %78 = sext i32 %77 to i64
  %79 = getelementptr inbounds float, float* %2, i64 %78
  %80 = bitcast float* %79 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %80, align 4, !tbaa !12, !llvm.access.group !16
  %81 = fpext <8 x float> %wide.load.2 to <8 x double>
  %82 = or i32 %77, 1
  %83 = sext i32 %82 to i64
  %84 = getelementptr inbounds float, float* %0, i64 %83
  %85 = bitcast float* %84 to <8 x float>*
  %wide.load8.2 = load <8 x float>, <8 x float>* %85, align 4, !tbaa !12, !llvm.access.group !16
  %86 = getelementptr inbounds float, float* %0, i64 %78
  %87 = bitcast float* %86 to <8 x float>*
  %wide.load9.2 = load <8 x float>, <8 x float>* %87, align 4, !tbaa !12, !llvm.access.group !16
  %88 = fsub <8 x float> %wide.load8.2, %wide.load9.2
  %89 = add nsw i32 %mul14.i, %76
  %90 = sext i32 %89 to i64
  %91 = getelementptr inbounds float, float* %1, i64 %90
  %92 = bitcast float* %91 to <8 x float>*
  %wide.load10.2 = load <8 x float>, <8 x float>* %92, align 4, !tbaa !12, !llvm.access.group !16
  %93 = fadd <8 x float> %88, %wide.load10.2
  %94 = getelementptr inbounds float, float* %1, i64 %78
  %95 = bitcast float* %94 to <8 x float>*
  %wide.load11.2 = load <8 x float>, <8 x float>* %95, align 4, !tbaa !12, !llvm.access.group !16
  %96 = fsub <8 x float> %93, %wide.load11.2
  %97 = fpext <8 x float> %96 to <8 x double>
  %98 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %97, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %81)
  %99 = fptrunc <8 x double> %98 to <8 x float>
  %100 = bitcast float* %79 to <8 x float>*
  store <8 x float> %99, <8 x float>* %100, align 4, !tbaa !12, !llvm.access.group !16
  %101 = trunc i64 %mul.i.i to i32
  %102 = or i32 %101, 24
  %103 = add i32 %mul.i, %102
  %104 = sext i32 %103 to i64
  %105 = getelementptr inbounds float, float* %2, i64 %104
  %106 = bitcast float* %105 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %106, align 4, !tbaa !12, !llvm.access.group !16
  %107 = fpext <8 x float> %wide.load.3 to <8 x double>
  %108 = or i32 %103, 1
  %109 = sext i32 %108 to i64
  %110 = getelementptr inbounds float, float* %0, i64 %109
  %111 = bitcast float* %110 to <8 x float>*
  %wide.load8.3 = load <8 x float>, <8 x float>* %111, align 4, !tbaa !12, !llvm.access.group !16
  %112 = getelementptr inbounds float, float* %0, i64 %104
  %113 = bitcast float* %112 to <8 x float>*
  %wide.load9.3 = load <8 x float>, <8 x float>* %113, align 4, !tbaa !12, !llvm.access.group !16
  %114 = fsub <8 x float> %wide.load8.3, %wide.load9.3
  %115 = add nsw i32 %mul14.i, %102
  %116 = sext i32 %115 to i64
  %117 = getelementptr inbounds float, float* %1, i64 %116
  %118 = bitcast float* %117 to <8 x float>*
  %wide.load10.3 = load <8 x float>, <8 x float>* %118, align 4, !tbaa !12, !llvm.access.group !16
  %119 = fadd <8 x float> %114, %wide.load10.3
  %120 = getelementptr inbounds float, float* %1, i64 %104
  %121 = bitcast float* %120 to <8 x float>*
  %wide.load11.3 = load <8 x float>, <8 x float>* %121, align 4, !tbaa !12, !llvm.access.group !16
  %122 = fsub <8 x float> %119, %wide.load11.3
  %123 = fpext <8 x float> %122 to <8 x double>
  %124 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %123, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %107)
  %125 = fptrunc <8 x double> %124 to <8 x float>
  %126 = bitcast float* %105 to <8 x float>*
  store <8 x float> %125, <8 x float>* %126, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i

pregion_for_end.i.loopexit:                       ; preds = %pregion_for_entry.entry.i
  br label %pregion_for_end.i

pregion_for_end.i:                                ; preds = %pregion_for_end.i.loopexit, %vector.body
  %127 = trunc i64 %mul3.i.i to i32
  %conv2.i.1 = or i32 %127, 1
  %mul.i.1 = mul nsw i32 %conv2.i.1, %4
  %add13.i.1 = add nuw nsw i32 %conv2.i.1, 1
  %mul14.i.1 = mul nsw i32 %add13.i.1, %4
  %128 = mul i32 %add13.i, %4
  %129 = trunc i64 %6 to i32
  %130 = shl i32 %129, 5
  %131 = add nsw i32 %128, %130
  %132 = icmp sgt i32 %131, 2147483616
  %133 = add i32 %128, %130
  %134 = add i32 %133, 1
  %135 = add i32 %133, 32
  %136 = icmp slt i32 %135, %134
  %137 = or i1 %132, %136
  %138 = or i32 %conv2.i, 2
  %139 = mul i32 %138, %4
  %140 = add nsw i32 %139, %130
  %141 = icmp sgt i32 %140, 2147483616
  %142 = or i1 %137, %141
  br i1 %142, label %pregion_for_entry.entry.i.1.preheader, label %vector.body14

pregion_for_entry.entry.i.1.preheader:            ; preds = %pregion_for_end.i
  br label %pregion_for_entry.entry.i.1

vector.body14:                                    ; preds = %pregion_for_end.i
  %143 = trunc i64 %mul.i.i to i32
  %144 = add i32 %mul.i.1, %143
  %145 = sext i32 %144 to i64
  %146 = getelementptr inbounds float, float* %2, i64 %145
  %147 = bitcast float* %146 to <8 x float>*
  %wide.load31 = load <8 x float>, <8 x float>* %147, align 4, !tbaa !12, !llvm.access.group !16
  %148 = fpext <8 x float> %wide.load31 to <8 x double>
  %149 = add i32 %144, 1
  %150 = sext i32 %149 to i64
  %151 = getelementptr inbounds float, float* %0, i64 %150
  %152 = bitcast float* %151 to <8 x float>*
  %wide.load32 = load <8 x float>, <8 x float>* %152, align 4, !tbaa !12, !llvm.access.group !16
  %153 = getelementptr inbounds float, float* %0, i64 %145
  %154 = bitcast float* %153 to <8 x float>*
  %wide.load33 = load <8 x float>, <8 x float>* %154, align 4, !tbaa !12, !llvm.access.group !16
  %155 = fsub <8 x float> %wide.load32, %wide.load33
  %156 = add nsw i32 %mul14.i.1, %143
  %157 = sext i32 %156 to i64
  %158 = getelementptr inbounds float, float* %1, i64 %157
  %159 = bitcast float* %158 to <8 x float>*
  %wide.load34 = load <8 x float>, <8 x float>* %159, align 4, !tbaa !12, !llvm.access.group !16
  %160 = fadd <8 x float> %155, %wide.load34
  %161 = getelementptr inbounds float, float* %1, i64 %145
  %162 = bitcast float* %161 to <8 x float>*
  %wide.load35 = load <8 x float>, <8 x float>* %162, align 4, !tbaa !12, !llvm.access.group !16
  %163 = fsub <8 x float> %160, %wide.load35
  %164 = fpext <8 x float> %163 to <8 x double>
  %165 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %164, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %148)
  %166 = fptrunc <8 x double> %165 to <8 x float>
  %167 = bitcast float* %146 to <8 x float>*
  store <8 x float> %166, <8 x float>* %167, align 4, !tbaa !12, !llvm.access.group !16
  %168 = trunc i64 %mul.i.i to i32
  %169 = or i32 %168, 8
  %170 = add i32 %mul.i.1, %169
  %171 = sext i32 %170 to i64
  %172 = getelementptr inbounds float, float* %2, i64 %171
  %173 = bitcast float* %172 to <8 x float>*
  %wide.load31.1 = load <8 x float>, <8 x float>* %173, align 4, !tbaa !12, !llvm.access.group !16
  %174 = fpext <8 x float> %wide.load31.1 to <8 x double>
  %175 = add i32 %170, 1
  %176 = sext i32 %175 to i64
  %177 = getelementptr inbounds float, float* %0, i64 %176
  %178 = bitcast float* %177 to <8 x float>*
  %wide.load32.1 = load <8 x float>, <8 x float>* %178, align 4, !tbaa !12, !llvm.access.group !16
  %179 = getelementptr inbounds float, float* %0, i64 %171
  %180 = bitcast float* %179 to <8 x float>*
  %wide.load33.1 = load <8 x float>, <8 x float>* %180, align 4, !tbaa !12, !llvm.access.group !16
  %181 = fsub <8 x float> %wide.load32.1, %wide.load33.1
  %182 = add nsw i32 %mul14.i.1, %169
  %183 = sext i32 %182 to i64
  %184 = getelementptr inbounds float, float* %1, i64 %183
  %185 = bitcast float* %184 to <8 x float>*
  %wide.load34.1 = load <8 x float>, <8 x float>* %185, align 4, !tbaa !12, !llvm.access.group !16
  %186 = fadd <8 x float> %181, %wide.load34.1
  %187 = getelementptr inbounds float, float* %1, i64 %171
  %188 = bitcast float* %187 to <8 x float>*
  %wide.load35.1 = load <8 x float>, <8 x float>* %188, align 4, !tbaa !12, !llvm.access.group !16
  %189 = fsub <8 x float> %186, %wide.load35.1
  %190 = fpext <8 x float> %189 to <8 x double>
  %191 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %190, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %174)
  %192 = fptrunc <8 x double> %191 to <8 x float>
  %193 = bitcast float* %172 to <8 x float>*
  store <8 x float> %192, <8 x float>* %193, align 4, !tbaa !12, !llvm.access.group !16
  %194 = trunc i64 %mul.i.i to i32
  %195 = or i32 %194, 16
  %196 = add i32 %mul.i.1, %195
  %197 = sext i32 %196 to i64
  %198 = getelementptr inbounds float, float* %2, i64 %197
  %199 = bitcast float* %198 to <8 x float>*
  %wide.load31.2 = load <8 x float>, <8 x float>* %199, align 4, !tbaa !12, !llvm.access.group !16
  %200 = fpext <8 x float> %wide.load31.2 to <8 x double>
  %201 = add i32 %196, 1
  %202 = sext i32 %201 to i64
  %203 = getelementptr inbounds float, float* %0, i64 %202
  %204 = bitcast float* %203 to <8 x float>*
  %wide.load32.2 = load <8 x float>, <8 x float>* %204, align 4, !tbaa !12, !llvm.access.group !16
  %205 = getelementptr inbounds float, float* %0, i64 %197
  %206 = bitcast float* %205 to <8 x float>*
  %wide.load33.2 = load <8 x float>, <8 x float>* %206, align 4, !tbaa !12, !llvm.access.group !16
  %207 = fsub <8 x float> %wide.load32.2, %wide.load33.2
  %208 = add nsw i32 %mul14.i.1, %195
  %209 = sext i32 %208 to i64
  %210 = getelementptr inbounds float, float* %1, i64 %209
  %211 = bitcast float* %210 to <8 x float>*
  %wide.load34.2 = load <8 x float>, <8 x float>* %211, align 4, !tbaa !12, !llvm.access.group !16
  %212 = fadd <8 x float> %207, %wide.load34.2
  %213 = getelementptr inbounds float, float* %1, i64 %197
  %214 = bitcast float* %213 to <8 x float>*
  %wide.load35.2 = load <8 x float>, <8 x float>* %214, align 4, !tbaa !12, !llvm.access.group !16
  %215 = fsub <8 x float> %212, %wide.load35.2
  %216 = fpext <8 x float> %215 to <8 x double>
  %217 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %216, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %200)
  %218 = fptrunc <8 x double> %217 to <8 x float>
  %219 = bitcast float* %198 to <8 x float>*
  store <8 x float> %218, <8 x float>* %219, align 4, !tbaa !12, !llvm.access.group !16
  %220 = trunc i64 %mul.i.i to i32
  %221 = or i32 %220, 24
  %222 = add i32 %mul.i.1, %221
  %223 = sext i32 %222 to i64
  %224 = getelementptr inbounds float, float* %2, i64 %223
  %225 = bitcast float* %224 to <8 x float>*
  %wide.load31.3 = load <8 x float>, <8 x float>* %225, align 4, !tbaa !12, !llvm.access.group !16
  %226 = fpext <8 x float> %wide.load31.3 to <8 x double>
  %227 = add i32 %222, 1
  %228 = sext i32 %227 to i64
  %229 = getelementptr inbounds float, float* %0, i64 %228
  %230 = bitcast float* %229 to <8 x float>*
  %wide.load32.3 = load <8 x float>, <8 x float>* %230, align 4, !tbaa !12, !llvm.access.group !16
  %231 = getelementptr inbounds float, float* %0, i64 %223
  %232 = bitcast float* %231 to <8 x float>*
  %wide.load33.3 = load <8 x float>, <8 x float>* %232, align 4, !tbaa !12, !llvm.access.group !16
  %233 = fsub <8 x float> %wide.load32.3, %wide.load33.3
  %234 = add nsw i32 %mul14.i.1, %221
  %235 = sext i32 %234 to i64
  %236 = getelementptr inbounds float, float* %1, i64 %235
  %237 = bitcast float* %236 to <8 x float>*
  %wide.load34.3 = load <8 x float>, <8 x float>* %237, align 4, !tbaa !12, !llvm.access.group !16
  %238 = fadd <8 x float> %233, %wide.load34.3
  %239 = getelementptr inbounds float, float* %1, i64 %223
  %240 = bitcast float* %239 to <8 x float>*
  %wide.load35.3 = load <8 x float>, <8 x float>* %240, align 4, !tbaa !12, !llvm.access.group !16
  %241 = fsub <8 x float> %238, %wide.load35.3
  %242 = fpext <8 x float> %241 to <8 x double>
  %243 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %242, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %226)
  %244 = fptrunc <8 x double> %243 to <8 x float>
  %245 = bitcast float* %224 to <8 x float>*
  store <8 x float> %244, <8 x float>* %245, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.1

pregion_for_entry.entry.i:                        ; preds = %pregion_for_entry.entry.i, %pregion_for_entry.entry.i.preheader
  %_local_id_x.0 = phi i64 [ %259, %pregion_for_entry.entry.i ], [ 0, %pregion_for_entry.entry.i.preheader ]
  %add1.i.i = add nuw nsw i64 %_local_id_x.0, %mul.i.i
  %conv.i = trunc i64 %add1.i.i to i32
  %add.i = add i32 %mul.i, %conv.i
  %idxprom.i = sext i32 %add.i to i64
  %arrayidx.i = getelementptr inbounds float, float* %2, i64 %idxprom.i
  %246 = load float, float* %arrayidx.i, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i = fpext float %246 to double
  %add6.i = or i32 %add.i, 1
  %idxprom7.i = sext i32 %add6.i to i64
  %arrayidx8.i = getelementptr inbounds float, float* %0, i64 %idxprom7.i
  %247 = load float, float* %arrayidx8.i, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i = getelementptr inbounds float, float* %0, i64 %idxprom.i
  %248 = load float, float* %arrayidx12.i, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i = fsub float %247, %248
  %add15.i = add nsw i32 %mul14.i, %conv.i
  %idxprom16.i = sext i32 %add15.i to i64
  %arrayidx17.i = getelementptr inbounds float, float* %1, i64 %idxprom16.i
  %249 = load float, float* %arrayidx17.i, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i = fadd float %sub.i, %249
  %arrayidx22.i = getelementptr inbounds float, float* %1, i64 %idxprom.i
  %250 = load float, float* %arrayidx22.i, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i = fsub float %add18.i, %250
  %conv24.i = fpext float %sub23.i to double
  %251 = tail call double @llvm.fmuladd.f64(double %conv24.i, double 0xBFE6666666666666, double %conv3.i) #3
  %conv26.i = fptrunc double %251 to float
  store float %conv26.i, float* %arrayidx.i, align 4, !tbaa !12, !llvm.access.group !16
  %252 = or i64 %_local_id_x.0, 1
  %add1.i.i.1189 = add nuw nsw i64 %252, %mul.i.i
  %conv.i.1190 = trunc i64 %add1.i.i.1189 to i32
  %add.i.1191 = add i32 %mul.i, %conv.i.1190
  %idxprom.i.1192 = sext i32 %add.i.1191 to i64
  %arrayidx.i.1193 = getelementptr inbounds float, float* %2, i64 %idxprom.i.1192
  %253 = load float, float* %arrayidx.i.1193, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.1194 = fpext float %253 to double
  %add6.i.1195 = add i32 %add.i.1191, 1
  %idxprom7.i.1196 = sext i32 %add6.i.1195 to i64
  %arrayidx8.i.1197 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.1196
  %254 = load float, float* %arrayidx8.i.1197, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.1198 = getelementptr inbounds float, float* %0, i64 %idxprom.i.1192
  %255 = load float, float* %arrayidx12.i.1198, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.1199 = fsub float %254, %255
  %add15.i.1200 = add nsw i32 %mul14.i, %conv.i.1190
  %idxprom16.i.1201 = sext i32 %add15.i.1200 to i64
  %arrayidx17.i.1202 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.1201
  %256 = load float, float* %arrayidx17.i.1202, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.1203 = fadd float %sub.i.1199, %256
  %arrayidx22.i.1204 = getelementptr inbounds float, float* %1, i64 %idxprom.i.1192
  %257 = load float, float* %arrayidx22.i.1204, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.1205 = fsub float %add18.i.1203, %257
  %conv24.i.1206 = fpext float %sub23.i.1205 to double
  %258 = tail call double @llvm.fmuladd.f64(double %conv24.i.1206, double 0xBFE6666666666666, double %conv3.i.1194) #3
  %conv26.i.1207 = fptrunc double %258 to float
  store float %conv26.i.1207, float* %arrayidx.i.1193, align 4, !tbaa !12, !llvm.access.group !16
  %259 = add nuw nsw i64 %_local_id_x.0, 2
  %exitcond.not.1 = icmp eq i64 %259, 32
  br i1 %exitcond.not.1, label %pregion_for_end.i.loopexit, label %pregion_for_entry.entry.i, !llvm.loop !19

pregion_for_entry.entry.i.1:                      ; preds = %pregion_for_entry.entry.i.1, %pregion_for_entry.entry.i.1.preheader
  %_local_id_x.0.1 = phi i64 [ %273, %pregion_for_entry.entry.i.1 ], [ 0, %pregion_for_entry.entry.i.1.preheader ]
  %add1.i.i.1 = add nuw nsw i64 %_local_id_x.0.1, %mul.i.i
  %conv.i.1 = trunc i64 %add1.i.i.1 to i32
  %add.i.1 = add i32 %mul.i.1, %conv.i.1
  %idxprom.i.1 = sext i32 %add.i.1 to i64
  %arrayidx.i.1 = getelementptr inbounds float, float* %2, i64 %idxprom.i.1
  %260 = load float, float* %arrayidx.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.1 = fpext float %260 to double
  %add6.i.1 = add i32 %add.i.1, 1
  %idxprom7.i.1 = sext i32 %add6.i.1 to i64
  %arrayidx8.i.1 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.1
  %261 = load float, float* %arrayidx8.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.1 = getelementptr inbounds float, float* %0, i64 %idxprom.i.1
  %262 = load float, float* %arrayidx12.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.1 = fsub float %261, %262
  %add15.i.1 = add nsw i32 %mul14.i.1, %conv.i.1
  %idxprom16.i.1 = sext i32 %add15.i.1 to i64
  %arrayidx17.i.1 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.1
  %263 = load float, float* %arrayidx17.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.1 = fadd float %sub.i.1, %263
  %arrayidx22.i.1 = getelementptr inbounds float, float* %1, i64 %idxprom.i.1
  %264 = load float, float* %arrayidx22.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.1 = fsub float %add18.i.1, %264
  %conv24.i.1 = fpext float %sub23.i.1 to double
  %265 = tail call double @llvm.fmuladd.f64(double %conv24.i.1, double 0xBFE6666666666666, double %conv3.i.1) #3
  %conv26.i.1 = fptrunc double %265 to float
  store float %conv26.i.1, float* %arrayidx.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %266 = or i64 %_local_id_x.0.1, 1
  %add1.i.i.1.1 = add nuw nsw i64 %266, %mul.i.i
  %conv.i.1.1 = trunc i64 %add1.i.i.1.1 to i32
  %add.i.1.1 = add i32 %mul.i.1, %conv.i.1.1
  %idxprom.i.1.1 = sext i32 %add.i.1.1 to i64
  %arrayidx.i.1.1 = getelementptr inbounds float, float* %2, i64 %idxprom.i.1.1
  %267 = load float, float* %arrayidx.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.1.1 = fpext float %267 to double
  %add6.i.1.1 = add i32 %add.i.1.1, 1
  %idxprom7.i.1.1 = sext i32 %add6.i.1.1 to i64
  %arrayidx8.i.1.1 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.1.1
  %268 = load float, float* %arrayidx8.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.1.1 = getelementptr inbounds float, float* %0, i64 %idxprom.i.1.1
  %269 = load float, float* %arrayidx12.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.1.1 = fsub float %268, %269
  %add15.i.1.1 = add nsw i32 %mul14.i.1, %conv.i.1.1
  %idxprom16.i.1.1 = sext i32 %add15.i.1.1 to i64
  %arrayidx17.i.1.1 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.1.1
  %270 = load float, float* %arrayidx17.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.1.1 = fadd float %sub.i.1.1, %270
  %arrayidx22.i.1.1 = getelementptr inbounds float, float* %1, i64 %idxprom.i.1.1
  %271 = load float, float* %arrayidx22.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.1.1 = fsub float %add18.i.1.1, %271
  %conv24.i.1.1 = fpext float %sub23.i.1.1 to double
  %272 = tail call double @llvm.fmuladd.f64(double %conv24.i.1.1, double 0xBFE6666666666666, double %conv3.i.1.1) #3
  %conv26.i.1.1 = fptrunc double %272 to float
  store float %conv26.i.1.1, float* %arrayidx.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %273 = add nuw nsw i64 %_local_id_x.0.1, 2
  %exitcond.1.not.1 = icmp eq i64 %273, 32
  br i1 %exitcond.1.not.1, label %pregion_for_end.i.1.loopexit, label %pregion_for_entry.entry.i.1, !llvm.loop !22

pregion_for_end.i.1.loopexit:                     ; preds = %pregion_for_entry.entry.i.1
  br label %pregion_for_end.i.1

pregion_for_end.i.1:                              ; preds = %pregion_for_end.i.1.loopexit, %vector.body14
  %274 = trunc i64 %mul3.i.i to i32
  %conv2.i.2 = or i32 %274, 2
  %mul.i.2 = mul nsw i32 %conv2.i.2, %4
  %add13.i.2 = or i32 %274, 3
  %mul14.i.2 = mul nsw i32 %add13.i.2, %4
  %275 = mul i32 %conv2.i.2, %4
  %276 = trunc i64 %6 to i32
  %277 = shl i32 %276, 5
  %278 = add nsw i32 %275, %277
  %279 = icmp sgt i32 %278, 2147483616
  %280 = add i32 %275, %277
  %281 = or i32 %280, 1
  %282 = icmp sgt i32 %281, 2147483616
  %283 = or i1 %279, %282
  %284 = mul i32 %add13.i.2, %4
  %285 = add nsw i32 %284, %277
  %286 = icmp sgt i32 %285, 2147483616
  %287 = or i1 %283, %286
  br i1 %287, label %pregion_for_entry.entry.i.2.preheader, label %vector.body38

pregion_for_entry.entry.i.2.preheader:            ; preds = %pregion_for_end.i.1
  br label %pregion_for_entry.entry.i.2

vector.body38:                                    ; preds = %pregion_for_end.i.1
  %288 = trunc i64 %mul.i.i to i32
  %289 = add i32 %mul.i.2, %288
  %290 = sext i32 %289 to i64
  %291 = getelementptr inbounds float, float* %2, i64 %290
  %292 = bitcast float* %291 to <8 x float>*
  %wide.load55 = load <8 x float>, <8 x float>* %292, align 4, !tbaa !12, !llvm.access.group !16
  %293 = fpext <8 x float> %wide.load55 to <8 x double>
  %294 = or i32 %289, 1
  %295 = sext i32 %294 to i64
  %296 = getelementptr inbounds float, float* %0, i64 %295
  %297 = bitcast float* %296 to <8 x float>*
  %wide.load56 = load <8 x float>, <8 x float>* %297, align 4, !tbaa !12, !llvm.access.group !16
  %298 = getelementptr inbounds float, float* %0, i64 %290
  %299 = bitcast float* %298 to <8 x float>*
  %wide.load57 = load <8 x float>, <8 x float>* %299, align 4, !tbaa !12, !llvm.access.group !16
  %300 = fsub <8 x float> %wide.load56, %wide.load57
  %301 = add nsw i32 %mul14.i.2, %288
  %302 = sext i32 %301 to i64
  %303 = getelementptr inbounds float, float* %1, i64 %302
  %304 = bitcast float* %303 to <8 x float>*
  %wide.load58 = load <8 x float>, <8 x float>* %304, align 4, !tbaa !12, !llvm.access.group !16
  %305 = fadd <8 x float> %300, %wide.load58
  %306 = getelementptr inbounds float, float* %1, i64 %290
  %307 = bitcast float* %306 to <8 x float>*
  %wide.load59 = load <8 x float>, <8 x float>* %307, align 4, !tbaa !12, !llvm.access.group !16
  %308 = fsub <8 x float> %305, %wide.load59
  %309 = fpext <8 x float> %308 to <8 x double>
  %310 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %309, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %293)
  %311 = fptrunc <8 x double> %310 to <8 x float>
  %312 = bitcast float* %291 to <8 x float>*
  store <8 x float> %311, <8 x float>* %312, align 4, !tbaa !12, !llvm.access.group !16
  %313 = trunc i64 %mul.i.i to i32
  %314 = or i32 %313, 8
  %315 = add i32 %mul.i.2, %314
  %316 = sext i32 %315 to i64
  %317 = getelementptr inbounds float, float* %2, i64 %316
  %318 = bitcast float* %317 to <8 x float>*
  %wide.load55.1 = load <8 x float>, <8 x float>* %318, align 4, !tbaa !12, !llvm.access.group !16
  %319 = fpext <8 x float> %wide.load55.1 to <8 x double>
  %320 = or i32 %315, 1
  %321 = sext i32 %320 to i64
  %322 = getelementptr inbounds float, float* %0, i64 %321
  %323 = bitcast float* %322 to <8 x float>*
  %wide.load56.1 = load <8 x float>, <8 x float>* %323, align 4, !tbaa !12, !llvm.access.group !16
  %324 = getelementptr inbounds float, float* %0, i64 %316
  %325 = bitcast float* %324 to <8 x float>*
  %wide.load57.1 = load <8 x float>, <8 x float>* %325, align 4, !tbaa !12, !llvm.access.group !16
  %326 = fsub <8 x float> %wide.load56.1, %wide.load57.1
  %327 = add nsw i32 %mul14.i.2, %314
  %328 = sext i32 %327 to i64
  %329 = getelementptr inbounds float, float* %1, i64 %328
  %330 = bitcast float* %329 to <8 x float>*
  %wide.load58.1 = load <8 x float>, <8 x float>* %330, align 4, !tbaa !12, !llvm.access.group !16
  %331 = fadd <8 x float> %326, %wide.load58.1
  %332 = getelementptr inbounds float, float* %1, i64 %316
  %333 = bitcast float* %332 to <8 x float>*
  %wide.load59.1 = load <8 x float>, <8 x float>* %333, align 4, !tbaa !12, !llvm.access.group !16
  %334 = fsub <8 x float> %331, %wide.load59.1
  %335 = fpext <8 x float> %334 to <8 x double>
  %336 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %335, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %319)
  %337 = fptrunc <8 x double> %336 to <8 x float>
  %338 = bitcast float* %317 to <8 x float>*
  store <8 x float> %337, <8 x float>* %338, align 4, !tbaa !12, !llvm.access.group !16
  %339 = trunc i64 %mul.i.i to i32
  %340 = or i32 %339, 16
  %341 = add i32 %mul.i.2, %340
  %342 = sext i32 %341 to i64
  %343 = getelementptr inbounds float, float* %2, i64 %342
  %344 = bitcast float* %343 to <8 x float>*
  %wide.load55.2 = load <8 x float>, <8 x float>* %344, align 4, !tbaa !12, !llvm.access.group !16
  %345 = fpext <8 x float> %wide.load55.2 to <8 x double>
  %346 = or i32 %341, 1
  %347 = sext i32 %346 to i64
  %348 = getelementptr inbounds float, float* %0, i64 %347
  %349 = bitcast float* %348 to <8 x float>*
  %wide.load56.2 = load <8 x float>, <8 x float>* %349, align 4, !tbaa !12, !llvm.access.group !16
  %350 = getelementptr inbounds float, float* %0, i64 %342
  %351 = bitcast float* %350 to <8 x float>*
  %wide.load57.2 = load <8 x float>, <8 x float>* %351, align 4, !tbaa !12, !llvm.access.group !16
  %352 = fsub <8 x float> %wide.load56.2, %wide.load57.2
  %353 = add nsw i32 %mul14.i.2, %340
  %354 = sext i32 %353 to i64
  %355 = getelementptr inbounds float, float* %1, i64 %354
  %356 = bitcast float* %355 to <8 x float>*
  %wide.load58.2 = load <8 x float>, <8 x float>* %356, align 4, !tbaa !12, !llvm.access.group !16
  %357 = fadd <8 x float> %352, %wide.load58.2
  %358 = getelementptr inbounds float, float* %1, i64 %342
  %359 = bitcast float* %358 to <8 x float>*
  %wide.load59.2 = load <8 x float>, <8 x float>* %359, align 4, !tbaa !12, !llvm.access.group !16
  %360 = fsub <8 x float> %357, %wide.load59.2
  %361 = fpext <8 x float> %360 to <8 x double>
  %362 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %361, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %345)
  %363 = fptrunc <8 x double> %362 to <8 x float>
  %364 = bitcast float* %343 to <8 x float>*
  store <8 x float> %363, <8 x float>* %364, align 4, !tbaa !12, !llvm.access.group !16
  %365 = trunc i64 %mul.i.i to i32
  %366 = or i32 %365, 24
  %367 = add i32 %mul.i.2, %366
  %368 = sext i32 %367 to i64
  %369 = getelementptr inbounds float, float* %2, i64 %368
  %370 = bitcast float* %369 to <8 x float>*
  %wide.load55.3 = load <8 x float>, <8 x float>* %370, align 4, !tbaa !12, !llvm.access.group !16
  %371 = fpext <8 x float> %wide.load55.3 to <8 x double>
  %372 = or i32 %367, 1
  %373 = sext i32 %372 to i64
  %374 = getelementptr inbounds float, float* %0, i64 %373
  %375 = bitcast float* %374 to <8 x float>*
  %wide.load56.3 = load <8 x float>, <8 x float>* %375, align 4, !tbaa !12, !llvm.access.group !16
  %376 = getelementptr inbounds float, float* %0, i64 %368
  %377 = bitcast float* %376 to <8 x float>*
  %wide.load57.3 = load <8 x float>, <8 x float>* %377, align 4, !tbaa !12, !llvm.access.group !16
  %378 = fsub <8 x float> %wide.load56.3, %wide.load57.3
  %379 = add nsw i32 %mul14.i.2, %366
  %380 = sext i32 %379 to i64
  %381 = getelementptr inbounds float, float* %1, i64 %380
  %382 = bitcast float* %381 to <8 x float>*
  %wide.load58.3 = load <8 x float>, <8 x float>* %382, align 4, !tbaa !12, !llvm.access.group !16
  %383 = fadd <8 x float> %378, %wide.load58.3
  %384 = getelementptr inbounds float, float* %1, i64 %368
  %385 = bitcast float* %384 to <8 x float>*
  %wide.load59.3 = load <8 x float>, <8 x float>* %385, align 4, !tbaa !12, !llvm.access.group !16
  %386 = fsub <8 x float> %383, %wide.load59.3
  %387 = fpext <8 x float> %386 to <8 x double>
  %388 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %387, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %371)
  %389 = fptrunc <8 x double> %388 to <8 x float>
  %390 = bitcast float* %369 to <8 x float>*
  store <8 x float> %389, <8 x float>* %390, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.2

pregion_for_entry.entry.i.2:                      ; preds = %pregion_for_entry.entry.i.2, %pregion_for_entry.entry.i.2.preheader
  %_local_id_x.0.2 = phi i64 [ %404, %pregion_for_entry.entry.i.2 ], [ 0, %pregion_for_entry.entry.i.2.preheader ]
  %add1.i.i.2 = add nuw nsw i64 %_local_id_x.0.2, %mul.i.i
  %conv.i.2 = trunc i64 %add1.i.i.2 to i32
  %add.i.2 = add i32 %mul.i.2, %conv.i.2
  %idxprom.i.2 = sext i32 %add.i.2 to i64
  %arrayidx.i.2 = getelementptr inbounds float, float* %2, i64 %idxprom.i.2
  %391 = load float, float* %arrayidx.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.2 = fpext float %391 to double
  %add6.i.2 = or i32 %add.i.2, 1
  %idxprom7.i.2 = sext i32 %add6.i.2 to i64
  %arrayidx8.i.2 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.2
  %392 = load float, float* %arrayidx8.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.2 = getelementptr inbounds float, float* %0, i64 %idxprom.i.2
  %393 = load float, float* %arrayidx12.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.2 = fsub float %392, %393
  %add15.i.2 = add nsw i32 %mul14.i.2, %conv.i.2
  %idxprom16.i.2 = sext i32 %add15.i.2 to i64
  %arrayidx17.i.2 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.2
  %394 = load float, float* %arrayidx17.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.2 = fadd float %sub.i.2, %394
  %arrayidx22.i.2 = getelementptr inbounds float, float* %1, i64 %idxprom.i.2
  %395 = load float, float* %arrayidx22.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.2 = fsub float %add18.i.2, %395
  %conv24.i.2 = fpext float %sub23.i.2 to double
  %396 = tail call double @llvm.fmuladd.f64(double %conv24.i.2, double 0xBFE6666666666666, double %conv3.i.2) #3
  %conv26.i.2 = fptrunc double %396 to float
  store float %conv26.i.2, float* %arrayidx.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %397 = or i64 %_local_id_x.0.2, 1
  %add1.i.i.2.1 = add nuw nsw i64 %397, %mul.i.i
  %conv.i.2.1 = trunc i64 %add1.i.i.2.1 to i32
  %add.i.2.1 = add i32 %mul.i.2, %conv.i.2.1
  %idxprom.i.2.1 = sext i32 %add.i.2.1 to i64
  %arrayidx.i.2.1 = getelementptr inbounds float, float* %2, i64 %idxprom.i.2.1
  %398 = load float, float* %arrayidx.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.2.1 = fpext float %398 to double
  %add6.i.2.1 = add i32 %add.i.2.1, 1
  %idxprom7.i.2.1 = sext i32 %add6.i.2.1 to i64
  %arrayidx8.i.2.1 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.2.1
  %399 = load float, float* %arrayidx8.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.2.1 = getelementptr inbounds float, float* %0, i64 %idxprom.i.2.1
  %400 = load float, float* %arrayidx12.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.2.1 = fsub float %399, %400
  %add15.i.2.1 = add nsw i32 %mul14.i.2, %conv.i.2.1
  %idxprom16.i.2.1 = sext i32 %add15.i.2.1 to i64
  %arrayidx17.i.2.1 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.2.1
  %401 = load float, float* %arrayidx17.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.2.1 = fadd float %sub.i.2.1, %401
  %arrayidx22.i.2.1 = getelementptr inbounds float, float* %1, i64 %idxprom.i.2.1
  %402 = load float, float* %arrayidx22.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.2.1 = fsub float %add18.i.2.1, %402
  %conv24.i.2.1 = fpext float %sub23.i.2.1 to double
  %403 = tail call double @llvm.fmuladd.f64(double %conv24.i.2.1, double 0xBFE6666666666666, double %conv3.i.2.1) #3
  %conv26.i.2.1 = fptrunc double %403 to float
  store float %conv26.i.2.1, float* %arrayidx.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %404 = add nuw nsw i64 %_local_id_x.0.2, 2
  %exitcond.2.not.1 = icmp eq i64 %404, 32
  br i1 %exitcond.2.not.1, label %pregion_for_end.i.2.loopexit, label %pregion_for_entry.entry.i.2, !llvm.loop !23

pregion_for_end.i.2.loopexit:                     ; preds = %pregion_for_entry.entry.i.2
  br label %pregion_for_end.i.2

pregion_for_end.i.2:                              ; preds = %pregion_for_end.i.2.loopexit, %vector.body38
  %405 = trunc i64 %mul3.i.i to i32
  %conv2.i.3 = or i32 %405, 3
  %mul.i.3 = mul nsw i32 %conv2.i.3, %4
  %add13.i.3 = add nuw nsw i32 %conv2.i.3, 1
  %mul14.i.3 = mul nsw i32 %add13.i.3, %4
  %406 = mul i32 %add13.i.2, %4
  %407 = trunc i64 %6 to i32
  %408 = shl i32 %407, 5
  %409 = add nsw i32 %406, %408
  %410 = icmp sgt i32 %409, 2147483616
  %411 = add i32 %406, %408
  %412 = add i32 %411, 1
  %413 = add i32 %411, 32
  %414 = icmp slt i32 %413, %412
  %415 = or i1 %410, %414
  %416 = or i32 %conv2.i, 4
  %417 = mul i32 %416, %4
  %418 = add nsw i32 %417, %408
  %419 = icmp sgt i32 %418, 2147483616
  %420 = or i1 %415, %419
  br i1 %420, label %pregion_for_entry.entry.i.3.preheader, label %vector.body62

pregion_for_entry.entry.i.3.preheader:            ; preds = %pregion_for_end.i.2
  br label %pregion_for_entry.entry.i.3

vector.body62:                                    ; preds = %pregion_for_end.i.2
  %421 = trunc i64 %mul.i.i to i32
  %422 = add i32 %mul.i.3, %421
  %423 = sext i32 %422 to i64
  %424 = getelementptr inbounds float, float* %2, i64 %423
  %425 = bitcast float* %424 to <8 x float>*
  %wide.load79 = load <8 x float>, <8 x float>* %425, align 4, !tbaa !12, !llvm.access.group !16
  %426 = fpext <8 x float> %wide.load79 to <8 x double>
  %427 = add i32 %422, 1
  %428 = sext i32 %427 to i64
  %429 = getelementptr inbounds float, float* %0, i64 %428
  %430 = bitcast float* %429 to <8 x float>*
  %wide.load80 = load <8 x float>, <8 x float>* %430, align 4, !tbaa !12, !llvm.access.group !16
  %431 = getelementptr inbounds float, float* %0, i64 %423
  %432 = bitcast float* %431 to <8 x float>*
  %wide.load81 = load <8 x float>, <8 x float>* %432, align 4, !tbaa !12, !llvm.access.group !16
  %433 = fsub <8 x float> %wide.load80, %wide.load81
  %434 = add nsw i32 %mul14.i.3, %421
  %435 = sext i32 %434 to i64
  %436 = getelementptr inbounds float, float* %1, i64 %435
  %437 = bitcast float* %436 to <8 x float>*
  %wide.load82 = load <8 x float>, <8 x float>* %437, align 4, !tbaa !12, !llvm.access.group !16
  %438 = fadd <8 x float> %433, %wide.load82
  %439 = getelementptr inbounds float, float* %1, i64 %423
  %440 = bitcast float* %439 to <8 x float>*
  %wide.load83 = load <8 x float>, <8 x float>* %440, align 4, !tbaa !12, !llvm.access.group !16
  %441 = fsub <8 x float> %438, %wide.load83
  %442 = fpext <8 x float> %441 to <8 x double>
  %443 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %442, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %426)
  %444 = fptrunc <8 x double> %443 to <8 x float>
  %445 = bitcast float* %424 to <8 x float>*
  store <8 x float> %444, <8 x float>* %445, align 4, !tbaa !12, !llvm.access.group !16
  %446 = trunc i64 %mul.i.i to i32
  %447 = or i32 %446, 8
  %448 = add i32 %mul.i.3, %447
  %449 = sext i32 %448 to i64
  %450 = getelementptr inbounds float, float* %2, i64 %449
  %451 = bitcast float* %450 to <8 x float>*
  %wide.load79.1 = load <8 x float>, <8 x float>* %451, align 4, !tbaa !12, !llvm.access.group !16
  %452 = fpext <8 x float> %wide.load79.1 to <8 x double>
  %453 = add i32 %448, 1
  %454 = sext i32 %453 to i64
  %455 = getelementptr inbounds float, float* %0, i64 %454
  %456 = bitcast float* %455 to <8 x float>*
  %wide.load80.1 = load <8 x float>, <8 x float>* %456, align 4, !tbaa !12, !llvm.access.group !16
  %457 = getelementptr inbounds float, float* %0, i64 %449
  %458 = bitcast float* %457 to <8 x float>*
  %wide.load81.1 = load <8 x float>, <8 x float>* %458, align 4, !tbaa !12, !llvm.access.group !16
  %459 = fsub <8 x float> %wide.load80.1, %wide.load81.1
  %460 = add nsw i32 %mul14.i.3, %447
  %461 = sext i32 %460 to i64
  %462 = getelementptr inbounds float, float* %1, i64 %461
  %463 = bitcast float* %462 to <8 x float>*
  %wide.load82.1 = load <8 x float>, <8 x float>* %463, align 4, !tbaa !12, !llvm.access.group !16
  %464 = fadd <8 x float> %459, %wide.load82.1
  %465 = getelementptr inbounds float, float* %1, i64 %449
  %466 = bitcast float* %465 to <8 x float>*
  %wide.load83.1 = load <8 x float>, <8 x float>* %466, align 4, !tbaa !12, !llvm.access.group !16
  %467 = fsub <8 x float> %464, %wide.load83.1
  %468 = fpext <8 x float> %467 to <8 x double>
  %469 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %468, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %452)
  %470 = fptrunc <8 x double> %469 to <8 x float>
  %471 = bitcast float* %450 to <8 x float>*
  store <8 x float> %470, <8 x float>* %471, align 4, !tbaa !12, !llvm.access.group !16
  %472 = trunc i64 %mul.i.i to i32
  %473 = or i32 %472, 16
  %474 = add i32 %mul.i.3, %473
  %475 = sext i32 %474 to i64
  %476 = getelementptr inbounds float, float* %2, i64 %475
  %477 = bitcast float* %476 to <8 x float>*
  %wide.load79.2 = load <8 x float>, <8 x float>* %477, align 4, !tbaa !12, !llvm.access.group !16
  %478 = fpext <8 x float> %wide.load79.2 to <8 x double>
  %479 = add i32 %474, 1
  %480 = sext i32 %479 to i64
  %481 = getelementptr inbounds float, float* %0, i64 %480
  %482 = bitcast float* %481 to <8 x float>*
  %wide.load80.2 = load <8 x float>, <8 x float>* %482, align 4, !tbaa !12, !llvm.access.group !16
  %483 = getelementptr inbounds float, float* %0, i64 %475
  %484 = bitcast float* %483 to <8 x float>*
  %wide.load81.2 = load <8 x float>, <8 x float>* %484, align 4, !tbaa !12, !llvm.access.group !16
  %485 = fsub <8 x float> %wide.load80.2, %wide.load81.2
  %486 = add nsw i32 %mul14.i.3, %473
  %487 = sext i32 %486 to i64
  %488 = getelementptr inbounds float, float* %1, i64 %487
  %489 = bitcast float* %488 to <8 x float>*
  %wide.load82.2 = load <8 x float>, <8 x float>* %489, align 4, !tbaa !12, !llvm.access.group !16
  %490 = fadd <8 x float> %485, %wide.load82.2
  %491 = getelementptr inbounds float, float* %1, i64 %475
  %492 = bitcast float* %491 to <8 x float>*
  %wide.load83.2 = load <8 x float>, <8 x float>* %492, align 4, !tbaa !12, !llvm.access.group !16
  %493 = fsub <8 x float> %490, %wide.load83.2
  %494 = fpext <8 x float> %493 to <8 x double>
  %495 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %494, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %478)
  %496 = fptrunc <8 x double> %495 to <8 x float>
  %497 = bitcast float* %476 to <8 x float>*
  store <8 x float> %496, <8 x float>* %497, align 4, !tbaa !12, !llvm.access.group !16
  %498 = trunc i64 %mul.i.i to i32
  %499 = or i32 %498, 24
  %500 = add i32 %mul.i.3, %499
  %501 = sext i32 %500 to i64
  %502 = getelementptr inbounds float, float* %2, i64 %501
  %503 = bitcast float* %502 to <8 x float>*
  %wide.load79.3 = load <8 x float>, <8 x float>* %503, align 4, !tbaa !12, !llvm.access.group !16
  %504 = fpext <8 x float> %wide.load79.3 to <8 x double>
  %505 = add i32 %500, 1
  %506 = sext i32 %505 to i64
  %507 = getelementptr inbounds float, float* %0, i64 %506
  %508 = bitcast float* %507 to <8 x float>*
  %wide.load80.3 = load <8 x float>, <8 x float>* %508, align 4, !tbaa !12, !llvm.access.group !16
  %509 = getelementptr inbounds float, float* %0, i64 %501
  %510 = bitcast float* %509 to <8 x float>*
  %wide.load81.3 = load <8 x float>, <8 x float>* %510, align 4, !tbaa !12, !llvm.access.group !16
  %511 = fsub <8 x float> %wide.load80.3, %wide.load81.3
  %512 = add nsw i32 %mul14.i.3, %499
  %513 = sext i32 %512 to i64
  %514 = getelementptr inbounds float, float* %1, i64 %513
  %515 = bitcast float* %514 to <8 x float>*
  %wide.load82.3 = load <8 x float>, <8 x float>* %515, align 4, !tbaa !12, !llvm.access.group !16
  %516 = fadd <8 x float> %511, %wide.load82.3
  %517 = getelementptr inbounds float, float* %1, i64 %501
  %518 = bitcast float* %517 to <8 x float>*
  %wide.load83.3 = load <8 x float>, <8 x float>* %518, align 4, !tbaa !12, !llvm.access.group !16
  %519 = fsub <8 x float> %516, %wide.load83.3
  %520 = fpext <8 x float> %519 to <8 x double>
  %521 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %520, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %504)
  %522 = fptrunc <8 x double> %521 to <8 x float>
  %523 = bitcast float* %502 to <8 x float>*
  store <8 x float> %522, <8 x float>* %523, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.3

pregion_for_entry.entry.i.3:                      ; preds = %pregion_for_entry.entry.i.3, %pregion_for_entry.entry.i.3.preheader
  %_local_id_x.0.3 = phi i64 [ %537, %pregion_for_entry.entry.i.3 ], [ 0, %pregion_for_entry.entry.i.3.preheader ]
  %add1.i.i.3 = add nuw nsw i64 %_local_id_x.0.3, %mul.i.i
  %conv.i.3 = trunc i64 %add1.i.i.3 to i32
  %add.i.3 = add i32 %mul.i.3, %conv.i.3
  %idxprom.i.3 = sext i32 %add.i.3 to i64
  %arrayidx.i.3 = getelementptr inbounds float, float* %2, i64 %idxprom.i.3
  %524 = load float, float* %arrayidx.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.3 = fpext float %524 to double
  %add6.i.3 = add i32 %add.i.3, 1
  %idxprom7.i.3 = sext i32 %add6.i.3 to i64
  %arrayidx8.i.3 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.3
  %525 = load float, float* %arrayidx8.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.3 = getelementptr inbounds float, float* %0, i64 %idxprom.i.3
  %526 = load float, float* %arrayidx12.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.3 = fsub float %525, %526
  %add15.i.3 = add nsw i32 %mul14.i.3, %conv.i.3
  %idxprom16.i.3 = sext i32 %add15.i.3 to i64
  %arrayidx17.i.3 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.3
  %527 = load float, float* %arrayidx17.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.3 = fadd float %sub.i.3, %527
  %arrayidx22.i.3 = getelementptr inbounds float, float* %1, i64 %idxprom.i.3
  %528 = load float, float* %arrayidx22.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.3 = fsub float %add18.i.3, %528
  %conv24.i.3 = fpext float %sub23.i.3 to double
  %529 = tail call double @llvm.fmuladd.f64(double %conv24.i.3, double 0xBFE6666666666666, double %conv3.i.3) #3
  %conv26.i.3 = fptrunc double %529 to float
  store float %conv26.i.3, float* %arrayidx.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %530 = or i64 %_local_id_x.0.3, 1
  %add1.i.i.3.1 = add nuw nsw i64 %530, %mul.i.i
  %conv.i.3.1 = trunc i64 %add1.i.i.3.1 to i32
  %add.i.3.1 = add i32 %mul.i.3, %conv.i.3.1
  %idxprom.i.3.1 = sext i32 %add.i.3.1 to i64
  %arrayidx.i.3.1 = getelementptr inbounds float, float* %2, i64 %idxprom.i.3.1
  %531 = load float, float* %arrayidx.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.3.1 = fpext float %531 to double
  %add6.i.3.1 = add i32 %add.i.3.1, 1
  %idxprom7.i.3.1 = sext i32 %add6.i.3.1 to i64
  %arrayidx8.i.3.1 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.3.1
  %532 = load float, float* %arrayidx8.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.3.1 = getelementptr inbounds float, float* %0, i64 %idxprom.i.3.1
  %533 = load float, float* %arrayidx12.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.3.1 = fsub float %532, %533
  %add15.i.3.1 = add nsw i32 %mul14.i.3, %conv.i.3.1
  %idxprom16.i.3.1 = sext i32 %add15.i.3.1 to i64
  %arrayidx17.i.3.1 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.3.1
  %534 = load float, float* %arrayidx17.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.3.1 = fadd float %sub.i.3.1, %534
  %arrayidx22.i.3.1 = getelementptr inbounds float, float* %1, i64 %idxprom.i.3.1
  %535 = load float, float* %arrayidx22.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.3.1 = fsub float %add18.i.3.1, %535
  %conv24.i.3.1 = fpext float %sub23.i.3.1 to double
  %536 = tail call double @llvm.fmuladd.f64(double %conv24.i.3.1, double 0xBFE6666666666666, double %conv3.i.3.1) #3
  %conv26.i.3.1 = fptrunc double %536 to float
  store float %conv26.i.3.1, float* %arrayidx.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %537 = add nuw nsw i64 %_local_id_x.0.3, 2
  %exitcond.3.not.1 = icmp eq i64 %537, 32
  br i1 %exitcond.3.not.1, label %pregion_for_end.i.3.loopexit, label %pregion_for_entry.entry.i.3, !llvm.loop !24

pregion_for_end.i.3.loopexit:                     ; preds = %pregion_for_entry.entry.i.3
  br label %pregion_for_end.i.3

pregion_for_end.i.3:                              ; preds = %pregion_for_end.i.3.loopexit, %vector.body62
  %538 = trunc i64 %mul3.i.i to i32
  %conv2.i.4 = or i32 %538, 4
  %mul.i.4 = mul nsw i32 %conv2.i.4, %4
  %add13.i.4 = or i32 %538, 5
  %mul14.i.4 = mul nsw i32 %add13.i.4, %4
  %539 = mul i32 %conv2.i.4, %4
  %540 = trunc i64 %6 to i32
  %541 = shl i32 %540, 5
  %542 = add nsw i32 %539, %541
  %543 = icmp sgt i32 %542, 2147483616
  %544 = add i32 %539, %541
  %545 = or i32 %544, 1
  %546 = icmp sgt i32 %545, 2147483616
  %547 = or i1 %543, %546
  %548 = mul i32 %add13.i.4, %4
  %549 = add nsw i32 %548, %541
  %550 = icmp sgt i32 %549, 2147483616
  %551 = or i1 %547, %550
  br i1 %551, label %pregion_for_entry.entry.i.4.preheader, label %vector.body86

pregion_for_entry.entry.i.4.preheader:            ; preds = %pregion_for_end.i.3
  br label %pregion_for_entry.entry.i.4

vector.body86:                                    ; preds = %pregion_for_end.i.3
  %552 = trunc i64 %mul.i.i to i32
  %553 = add i32 %mul.i.4, %552
  %554 = sext i32 %553 to i64
  %555 = getelementptr inbounds float, float* %2, i64 %554
  %556 = bitcast float* %555 to <8 x float>*
  %wide.load103 = load <8 x float>, <8 x float>* %556, align 4, !tbaa !12, !llvm.access.group !16
  %557 = fpext <8 x float> %wide.load103 to <8 x double>
  %558 = or i32 %553, 1
  %559 = sext i32 %558 to i64
  %560 = getelementptr inbounds float, float* %0, i64 %559
  %561 = bitcast float* %560 to <8 x float>*
  %wide.load104 = load <8 x float>, <8 x float>* %561, align 4, !tbaa !12, !llvm.access.group !16
  %562 = getelementptr inbounds float, float* %0, i64 %554
  %563 = bitcast float* %562 to <8 x float>*
  %wide.load105 = load <8 x float>, <8 x float>* %563, align 4, !tbaa !12, !llvm.access.group !16
  %564 = fsub <8 x float> %wide.load104, %wide.load105
  %565 = add nsw i32 %mul14.i.4, %552
  %566 = sext i32 %565 to i64
  %567 = getelementptr inbounds float, float* %1, i64 %566
  %568 = bitcast float* %567 to <8 x float>*
  %wide.load106 = load <8 x float>, <8 x float>* %568, align 4, !tbaa !12, !llvm.access.group !16
  %569 = fadd <8 x float> %564, %wide.load106
  %570 = getelementptr inbounds float, float* %1, i64 %554
  %571 = bitcast float* %570 to <8 x float>*
  %wide.load107 = load <8 x float>, <8 x float>* %571, align 4, !tbaa !12, !llvm.access.group !16
  %572 = fsub <8 x float> %569, %wide.load107
  %573 = fpext <8 x float> %572 to <8 x double>
  %574 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %573, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %557)
  %575 = fptrunc <8 x double> %574 to <8 x float>
  %576 = bitcast float* %555 to <8 x float>*
  store <8 x float> %575, <8 x float>* %576, align 4, !tbaa !12, !llvm.access.group !16
  %577 = trunc i64 %mul.i.i to i32
  %578 = or i32 %577, 8
  %579 = add i32 %mul.i.4, %578
  %580 = sext i32 %579 to i64
  %581 = getelementptr inbounds float, float* %2, i64 %580
  %582 = bitcast float* %581 to <8 x float>*
  %wide.load103.1 = load <8 x float>, <8 x float>* %582, align 4, !tbaa !12, !llvm.access.group !16
  %583 = fpext <8 x float> %wide.load103.1 to <8 x double>
  %584 = or i32 %579, 1
  %585 = sext i32 %584 to i64
  %586 = getelementptr inbounds float, float* %0, i64 %585
  %587 = bitcast float* %586 to <8 x float>*
  %wide.load104.1 = load <8 x float>, <8 x float>* %587, align 4, !tbaa !12, !llvm.access.group !16
  %588 = getelementptr inbounds float, float* %0, i64 %580
  %589 = bitcast float* %588 to <8 x float>*
  %wide.load105.1 = load <8 x float>, <8 x float>* %589, align 4, !tbaa !12, !llvm.access.group !16
  %590 = fsub <8 x float> %wide.load104.1, %wide.load105.1
  %591 = add nsw i32 %mul14.i.4, %578
  %592 = sext i32 %591 to i64
  %593 = getelementptr inbounds float, float* %1, i64 %592
  %594 = bitcast float* %593 to <8 x float>*
  %wide.load106.1 = load <8 x float>, <8 x float>* %594, align 4, !tbaa !12, !llvm.access.group !16
  %595 = fadd <8 x float> %590, %wide.load106.1
  %596 = getelementptr inbounds float, float* %1, i64 %580
  %597 = bitcast float* %596 to <8 x float>*
  %wide.load107.1 = load <8 x float>, <8 x float>* %597, align 4, !tbaa !12, !llvm.access.group !16
  %598 = fsub <8 x float> %595, %wide.load107.1
  %599 = fpext <8 x float> %598 to <8 x double>
  %600 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %599, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %583)
  %601 = fptrunc <8 x double> %600 to <8 x float>
  %602 = bitcast float* %581 to <8 x float>*
  store <8 x float> %601, <8 x float>* %602, align 4, !tbaa !12, !llvm.access.group !16
  %603 = trunc i64 %mul.i.i to i32
  %604 = or i32 %603, 16
  %605 = add i32 %mul.i.4, %604
  %606 = sext i32 %605 to i64
  %607 = getelementptr inbounds float, float* %2, i64 %606
  %608 = bitcast float* %607 to <8 x float>*
  %wide.load103.2 = load <8 x float>, <8 x float>* %608, align 4, !tbaa !12, !llvm.access.group !16
  %609 = fpext <8 x float> %wide.load103.2 to <8 x double>
  %610 = or i32 %605, 1
  %611 = sext i32 %610 to i64
  %612 = getelementptr inbounds float, float* %0, i64 %611
  %613 = bitcast float* %612 to <8 x float>*
  %wide.load104.2 = load <8 x float>, <8 x float>* %613, align 4, !tbaa !12, !llvm.access.group !16
  %614 = getelementptr inbounds float, float* %0, i64 %606
  %615 = bitcast float* %614 to <8 x float>*
  %wide.load105.2 = load <8 x float>, <8 x float>* %615, align 4, !tbaa !12, !llvm.access.group !16
  %616 = fsub <8 x float> %wide.load104.2, %wide.load105.2
  %617 = add nsw i32 %mul14.i.4, %604
  %618 = sext i32 %617 to i64
  %619 = getelementptr inbounds float, float* %1, i64 %618
  %620 = bitcast float* %619 to <8 x float>*
  %wide.load106.2 = load <8 x float>, <8 x float>* %620, align 4, !tbaa !12, !llvm.access.group !16
  %621 = fadd <8 x float> %616, %wide.load106.2
  %622 = getelementptr inbounds float, float* %1, i64 %606
  %623 = bitcast float* %622 to <8 x float>*
  %wide.load107.2 = load <8 x float>, <8 x float>* %623, align 4, !tbaa !12, !llvm.access.group !16
  %624 = fsub <8 x float> %621, %wide.load107.2
  %625 = fpext <8 x float> %624 to <8 x double>
  %626 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %625, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %609)
  %627 = fptrunc <8 x double> %626 to <8 x float>
  %628 = bitcast float* %607 to <8 x float>*
  store <8 x float> %627, <8 x float>* %628, align 4, !tbaa !12, !llvm.access.group !16
  %629 = trunc i64 %mul.i.i to i32
  %630 = or i32 %629, 24
  %631 = add i32 %mul.i.4, %630
  %632 = sext i32 %631 to i64
  %633 = getelementptr inbounds float, float* %2, i64 %632
  %634 = bitcast float* %633 to <8 x float>*
  %wide.load103.3 = load <8 x float>, <8 x float>* %634, align 4, !tbaa !12, !llvm.access.group !16
  %635 = fpext <8 x float> %wide.load103.3 to <8 x double>
  %636 = or i32 %631, 1
  %637 = sext i32 %636 to i64
  %638 = getelementptr inbounds float, float* %0, i64 %637
  %639 = bitcast float* %638 to <8 x float>*
  %wide.load104.3 = load <8 x float>, <8 x float>* %639, align 4, !tbaa !12, !llvm.access.group !16
  %640 = getelementptr inbounds float, float* %0, i64 %632
  %641 = bitcast float* %640 to <8 x float>*
  %wide.load105.3 = load <8 x float>, <8 x float>* %641, align 4, !tbaa !12, !llvm.access.group !16
  %642 = fsub <8 x float> %wide.load104.3, %wide.load105.3
  %643 = add nsw i32 %mul14.i.4, %630
  %644 = sext i32 %643 to i64
  %645 = getelementptr inbounds float, float* %1, i64 %644
  %646 = bitcast float* %645 to <8 x float>*
  %wide.load106.3 = load <8 x float>, <8 x float>* %646, align 4, !tbaa !12, !llvm.access.group !16
  %647 = fadd <8 x float> %642, %wide.load106.3
  %648 = getelementptr inbounds float, float* %1, i64 %632
  %649 = bitcast float* %648 to <8 x float>*
  %wide.load107.3 = load <8 x float>, <8 x float>* %649, align 4, !tbaa !12, !llvm.access.group !16
  %650 = fsub <8 x float> %647, %wide.load107.3
  %651 = fpext <8 x float> %650 to <8 x double>
  %652 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %651, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %635)
  %653 = fptrunc <8 x double> %652 to <8 x float>
  %654 = bitcast float* %633 to <8 x float>*
  store <8 x float> %653, <8 x float>* %654, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.4

pregion_for_entry.entry.i.4:                      ; preds = %pregion_for_entry.entry.i.4, %pregion_for_entry.entry.i.4.preheader
  %_local_id_x.0.4 = phi i64 [ %668, %pregion_for_entry.entry.i.4 ], [ 0, %pregion_for_entry.entry.i.4.preheader ]
  %add1.i.i.4 = add nuw nsw i64 %_local_id_x.0.4, %mul.i.i
  %conv.i.4 = trunc i64 %add1.i.i.4 to i32
  %add.i.4 = add i32 %mul.i.4, %conv.i.4
  %idxprom.i.4 = sext i32 %add.i.4 to i64
  %arrayidx.i.4 = getelementptr inbounds float, float* %2, i64 %idxprom.i.4
  %655 = load float, float* %arrayidx.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.4 = fpext float %655 to double
  %add6.i.4 = or i32 %add.i.4, 1
  %idxprom7.i.4 = sext i32 %add6.i.4 to i64
  %arrayidx8.i.4 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.4
  %656 = load float, float* %arrayidx8.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.4 = getelementptr inbounds float, float* %0, i64 %idxprom.i.4
  %657 = load float, float* %arrayidx12.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.4 = fsub float %656, %657
  %add15.i.4 = add nsw i32 %mul14.i.4, %conv.i.4
  %idxprom16.i.4 = sext i32 %add15.i.4 to i64
  %arrayidx17.i.4 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.4
  %658 = load float, float* %arrayidx17.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.4 = fadd float %sub.i.4, %658
  %arrayidx22.i.4 = getelementptr inbounds float, float* %1, i64 %idxprom.i.4
  %659 = load float, float* %arrayidx22.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.4 = fsub float %add18.i.4, %659
  %conv24.i.4 = fpext float %sub23.i.4 to double
  %660 = tail call double @llvm.fmuladd.f64(double %conv24.i.4, double 0xBFE6666666666666, double %conv3.i.4) #3
  %conv26.i.4 = fptrunc double %660 to float
  store float %conv26.i.4, float* %arrayidx.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %661 = or i64 %_local_id_x.0.4, 1
  %add1.i.i.4.1 = add nuw nsw i64 %661, %mul.i.i
  %conv.i.4.1 = trunc i64 %add1.i.i.4.1 to i32
  %add.i.4.1 = add i32 %mul.i.4, %conv.i.4.1
  %idxprom.i.4.1 = sext i32 %add.i.4.1 to i64
  %arrayidx.i.4.1 = getelementptr inbounds float, float* %2, i64 %idxprom.i.4.1
  %662 = load float, float* %arrayidx.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.4.1 = fpext float %662 to double
  %add6.i.4.1 = add i32 %add.i.4.1, 1
  %idxprom7.i.4.1 = sext i32 %add6.i.4.1 to i64
  %arrayidx8.i.4.1 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.4.1
  %663 = load float, float* %arrayidx8.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.4.1 = getelementptr inbounds float, float* %0, i64 %idxprom.i.4.1
  %664 = load float, float* %arrayidx12.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.4.1 = fsub float %663, %664
  %add15.i.4.1 = add nsw i32 %mul14.i.4, %conv.i.4.1
  %idxprom16.i.4.1 = sext i32 %add15.i.4.1 to i64
  %arrayidx17.i.4.1 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.4.1
  %665 = load float, float* %arrayidx17.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.4.1 = fadd float %sub.i.4.1, %665
  %arrayidx22.i.4.1 = getelementptr inbounds float, float* %1, i64 %idxprom.i.4.1
  %666 = load float, float* %arrayidx22.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.4.1 = fsub float %add18.i.4.1, %666
  %conv24.i.4.1 = fpext float %sub23.i.4.1 to double
  %667 = tail call double @llvm.fmuladd.f64(double %conv24.i.4.1, double 0xBFE6666666666666, double %conv3.i.4.1) #3
  %conv26.i.4.1 = fptrunc double %667 to float
  store float %conv26.i.4.1, float* %arrayidx.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %668 = add nuw nsw i64 %_local_id_x.0.4, 2
  %exitcond.4.not.1 = icmp eq i64 %668, 32
  br i1 %exitcond.4.not.1, label %pregion_for_end.i.4.loopexit, label %pregion_for_entry.entry.i.4, !llvm.loop !25

pregion_for_end.i.4.loopexit:                     ; preds = %pregion_for_entry.entry.i.4
  br label %pregion_for_end.i.4

pregion_for_end.i.4:                              ; preds = %pregion_for_end.i.4.loopexit, %vector.body86
  %669 = trunc i64 %mul3.i.i to i32
  %conv2.i.5 = or i32 %669, 5
  %mul.i.5 = mul nsw i32 %conv2.i.5, %4
  %add13.i.5 = add nuw nsw i32 %conv2.i.5, 1
  %mul14.i.5 = mul nsw i32 %add13.i.5, %4
  %670 = mul i32 %add13.i.4, %4
  %671 = trunc i64 %6 to i32
  %672 = shl i32 %671, 5
  %673 = add nsw i32 %670, %672
  %674 = icmp sgt i32 %673, 2147483616
  %675 = add i32 %670, %672
  %676 = add i32 %675, 1
  %677 = add i32 %675, 32
  %678 = icmp slt i32 %677, %676
  %679 = or i1 %674, %678
  %680 = or i32 %conv2.i, 6
  %681 = mul i32 %680, %4
  %682 = add nsw i32 %681, %672
  %683 = icmp sgt i32 %682, 2147483616
  %684 = or i1 %679, %683
  br i1 %684, label %pregion_for_entry.entry.i.5.preheader, label %vector.body110

pregion_for_entry.entry.i.5.preheader:            ; preds = %pregion_for_end.i.4
  br label %pregion_for_entry.entry.i.5

vector.body110:                                   ; preds = %pregion_for_end.i.4
  %685 = trunc i64 %mul.i.i to i32
  %686 = add i32 %mul.i.5, %685
  %687 = sext i32 %686 to i64
  %688 = getelementptr inbounds float, float* %2, i64 %687
  %689 = bitcast float* %688 to <8 x float>*
  %wide.load127 = load <8 x float>, <8 x float>* %689, align 4, !tbaa !12, !llvm.access.group !16
  %690 = fpext <8 x float> %wide.load127 to <8 x double>
  %691 = add i32 %686, 1
  %692 = sext i32 %691 to i64
  %693 = getelementptr inbounds float, float* %0, i64 %692
  %694 = bitcast float* %693 to <8 x float>*
  %wide.load128 = load <8 x float>, <8 x float>* %694, align 4, !tbaa !12, !llvm.access.group !16
  %695 = getelementptr inbounds float, float* %0, i64 %687
  %696 = bitcast float* %695 to <8 x float>*
  %wide.load129 = load <8 x float>, <8 x float>* %696, align 4, !tbaa !12, !llvm.access.group !16
  %697 = fsub <8 x float> %wide.load128, %wide.load129
  %698 = add nsw i32 %mul14.i.5, %685
  %699 = sext i32 %698 to i64
  %700 = getelementptr inbounds float, float* %1, i64 %699
  %701 = bitcast float* %700 to <8 x float>*
  %wide.load130 = load <8 x float>, <8 x float>* %701, align 4, !tbaa !12, !llvm.access.group !16
  %702 = fadd <8 x float> %697, %wide.load130
  %703 = getelementptr inbounds float, float* %1, i64 %687
  %704 = bitcast float* %703 to <8 x float>*
  %wide.load131 = load <8 x float>, <8 x float>* %704, align 4, !tbaa !12, !llvm.access.group !16
  %705 = fsub <8 x float> %702, %wide.load131
  %706 = fpext <8 x float> %705 to <8 x double>
  %707 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %706, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %690)
  %708 = fptrunc <8 x double> %707 to <8 x float>
  %709 = bitcast float* %688 to <8 x float>*
  store <8 x float> %708, <8 x float>* %709, align 4, !tbaa !12, !llvm.access.group !16
  %710 = trunc i64 %mul.i.i to i32
  %711 = or i32 %710, 8
  %712 = add i32 %mul.i.5, %711
  %713 = sext i32 %712 to i64
  %714 = getelementptr inbounds float, float* %2, i64 %713
  %715 = bitcast float* %714 to <8 x float>*
  %wide.load127.1 = load <8 x float>, <8 x float>* %715, align 4, !tbaa !12, !llvm.access.group !16
  %716 = fpext <8 x float> %wide.load127.1 to <8 x double>
  %717 = add i32 %712, 1
  %718 = sext i32 %717 to i64
  %719 = getelementptr inbounds float, float* %0, i64 %718
  %720 = bitcast float* %719 to <8 x float>*
  %wide.load128.1 = load <8 x float>, <8 x float>* %720, align 4, !tbaa !12, !llvm.access.group !16
  %721 = getelementptr inbounds float, float* %0, i64 %713
  %722 = bitcast float* %721 to <8 x float>*
  %wide.load129.1 = load <8 x float>, <8 x float>* %722, align 4, !tbaa !12, !llvm.access.group !16
  %723 = fsub <8 x float> %wide.load128.1, %wide.load129.1
  %724 = add nsw i32 %mul14.i.5, %711
  %725 = sext i32 %724 to i64
  %726 = getelementptr inbounds float, float* %1, i64 %725
  %727 = bitcast float* %726 to <8 x float>*
  %wide.load130.1 = load <8 x float>, <8 x float>* %727, align 4, !tbaa !12, !llvm.access.group !16
  %728 = fadd <8 x float> %723, %wide.load130.1
  %729 = getelementptr inbounds float, float* %1, i64 %713
  %730 = bitcast float* %729 to <8 x float>*
  %wide.load131.1 = load <8 x float>, <8 x float>* %730, align 4, !tbaa !12, !llvm.access.group !16
  %731 = fsub <8 x float> %728, %wide.load131.1
  %732 = fpext <8 x float> %731 to <8 x double>
  %733 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %732, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %716)
  %734 = fptrunc <8 x double> %733 to <8 x float>
  %735 = bitcast float* %714 to <8 x float>*
  store <8 x float> %734, <8 x float>* %735, align 4, !tbaa !12, !llvm.access.group !16
  %736 = trunc i64 %mul.i.i to i32
  %737 = or i32 %736, 16
  %738 = add i32 %mul.i.5, %737
  %739 = sext i32 %738 to i64
  %740 = getelementptr inbounds float, float* %2, i64 %739
  %741 = bitcast float* %740 to <8 x float>*
  %wide.load127.2 = load <8 x float>, <8 x float>* %741, align 4, !tbaa !12, !llvm.access.group !16
  %742 = fpext <8 x float> %wide.load127.2 to <8 x double>
  %743 = add i32 %738, 1
  %744 = sext i32 %743 to i64
  %745 = getelementptr inbounds float, float* %0, i64 %744
  %746 = bitcast float* %745 to <8 x float>*
  %wide.load128.2 = load <8 x float>, <8 x float>* %746, align 4, !tbaa !12, !llvm.access.group !16
  %747 = getelementptr inbounds float, float* %0, i64 %739
  %748 = bitcast float* %747 to <8 x float>*
  %wide.load129.2 = load <8 x float>, <8 x float>* %748, align 4, !tbaa !12, !llvm.access.group !16
  %749 = fsub <8 x float> %wide.load128.2, %wide.load129.2
  %750 = add nsw i32 %mul14.i.5, %737
  %751 = sext i32 %750 to i64
  %752 = getelementptr inbounds float, float* %1, i64 %751
  %753 = bitcast float* %752 to <8 x float>*
  %wide.load130.2 = load <8 x float>, <8 x float>* %753, align 4, !tbaa !12, !llvm.access.group !16
  %754 = fadd <8 x float> %749, %wide.load130.2
  %755 = getelementptr inbounds float, float* %1, i64 %739
  %756 = bitcast float* %755 to <8 x float>*
  %wide.load131.2 = load <8 x float>, <8 x float>* %756, align 4, !tbaa !12, !llvm.access.group !16
  %757 = fsub <8 x float> %754, %wide.load131.2
  %758 = fpext <8 x float> %757 to <8 x double>
  %759 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %758, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %742)
  %760 = fptrunc <8 x double> %759 to <8 x float>
  %761 = bitcast float* %740 to <8 x float>*
  store <8 x float> %760, <8 x float>* %761, align 4, !tbaa !12, !llvm.access.group !16
  %762 = trunc i64 %mul.i.i to i32
  %763 = or i32 %762, 24
  %764 = add i32 %mul.i.5, %763
  %765 = sext i32 %764 to i64
  %766 = getelementptr inbounds float, float* %2, i64 %765
  %767 = bitcast float* %766 to <8 x float>*
  %wide.load127.3 = load <8 x float>, <8 x float>* %767, align 4, !tbaa !12, !llvm.access.group !16
  %768 = fpext <8 x float> %wide.load127.3 to <8 x double>
  %769 = add i32 %764, 1
  %770 = sext i32 %769 to i64
  %771 = getelementptr inbounds float, float* %0, i64 %770
  %772 = bitcast float* %771 to <8 x float>*
  %wide.load128.3 = load <8 x float>, <8 x float>* %772, align 4, !tbaa !12, !llvm.access.group !16
  %773 = getelementptr inbounds float, float* %0, i64 %765
  %774 = bitcast float* %773 to <8 x float>*
  %wide.load129.3 = load <8 x float>, <8 x float>* %774, align 4, !tbaa !12, !llvm.access.group !16
  %775 = fsub <8 x float> %wide.load128.3, %wide.load129.3
  %776 = add nsw i32 %mul14.i.5, %763
  %777 = sext i32 %776 to i64
  %778 = getelementptr inbounds float, float* %1, i64 %777
  %779 = bitcast float* %778 to <8 x float>*
  %wide.load130.3 = load <8 x float>, <8 x float>* %779, align 4, !tbaa !12, !llvm.access.group !16
  %780 = fadd <8 x float> %775, %wide.load130.3
  %781 = getelementptr inbounds float, float* %1, i64 %765
  %782 = bitcast float* %781 to <8 x float>*
  %wide.load131.3 = load <8 x float>, <8 x float>* %782, align 4, !tbaa !12, !llvm.access.group !16
  %783 = fsub <8 x float> %780, %wide.load131.3
  %784 = fpext <8 x float> %783 to <8 x double>
  %785 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %784, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %768)
  %786 = fptrunc <8 x double> %785 to <8 x float>
  %787 = bitcast float* %766 to <8 x float>*
  store <8 x float> %786, <8 x float>* %787, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.5

pregion_for_entry.entry.i.5:                      ; preds = %pregion_for_entry.entry.i.5, %pregion_for_entry.entry.i.5.preheader
  %_local_id_x.0.5 = phi i64 [ %801, %pregion_for_entry.entry.i.5 ], [ 0, %pregion_for_entry.entry.i.5.preheader ]
  %add1.i.i.5 = add nuw nsw i64 %_local_id_x.0.5, %mul.i.i
  %conv.i.5 = trunc i64 %add1.i.i.5 to i32
  %add.i.5 = add i32 %mul.i.5, %conv.i.5
  %idxprom.i.5 = sext i32 %add.i.5 to i64
  %arrayidx.i.5 = getelementptr inbounds float, float* %2, i64 %idxprom.i.5
  %788 = load float, float* %arrayidx.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.5 = fpext float %788 to double
  %add6.i.5 = add i32 %add.i.5, 1
  %idxprom7.i.5 = sext i32 %add6.i.5 to i64
  %arrayidx8.i.5 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.5
  %789 = load float, float* %arrayidx8.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.5 = getelementptr inbounds float, float* %0, i64 %idxprom.i.5
  %790 = load float, float* %arrayidx12.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.5 = fsub float %789, %790
  %add15.i.5 = add nsw i32 %mul14.i.5, %conv.i.5
  %idxprom16.i.5 = sext i32 %add15.i.5 to i64
  %arrayidx17.i.5 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.5
  %791 = load float, float* %arrayidx17.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.5 = fadd float %sub.i.5, %791
  %arrayidx22.i.5 = getelementptr inbounds float, float* %1, i64 %idxprom.i.5
  %792 = load float, float* %arrayidx22.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.5 = fsub float %add18.i.5, %792
  %conv24.i.5 = fpext float %sub23.i.5 to double
  %793 = tail call double @llvm.fmuladd.f64(double %conv24.i.5, double 0xBFE6666666666666, double %conv3.i.5) #3
  %conv26.i.5 = fptrunc double %793 to float
  store float %conv26.i.5, float* %arrayidx.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %794 = or i64 %_local_id_x.0.5, 1
  %add1.i.i.5.1 = add nuw nsw i64 %794, %mul.i.i
  %conv.i.5.1 = trunc i64 %add1.i.i.5.1 to i32
  %add.i.5.1 = add i32 %mul.i.5, %conv.i.5.1
  %idxprom.i.5.1 = sext i32 %add.i.5.1 to i64
  %arrayidx.i.5.1 = getelementptr inbounds float, float* %2, i64 %idxprom.i.5.1
  %795 = load float, float* %arrayidx.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.5.1 = fpext float %795 to double
  %add6.i.5.1 = add i32 %add.i.5.1, 1
  %idxprom7.i.5.1 = sext i32 %add6.i.5.1 to i64
  %arrayidx8.i.5.1 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.5.1
  %796 = load float, float* %arrayidx8.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.5.1 = getelementptr inbounds float, float* %0, i64 %idxprom.i.5.1
  %797 = load float, float* %arrayidx12.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.5.1 = fsub float %796, %797
  %add15.i.5.1 = add nsw i32 %mul14.i.5, %conv.i.5.1
  %idxprom16.i.5.1 = sext i32 %add15.i.5.1 to i64
  %arrayidx17.i.5.1 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.5.1
  %798 = load float, float* %arrayidx17.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.5.1 = fadd float %sub.i.5.1, %798
  %arrayidx22.i.5.1 = getelementptr inbounds float, float* %1, i64 %idxprom.i.5.1
  %799 = load float, float* %arrayidx22.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.5.1 = fsub float %add18.i.5.1, %799
  %conv24.i.5.1 = fpext float %sub23.i.5.1 to double
  %800 = tail call double @llvm.fmuladd.f64(double %conv24.i.5.1, double 0xBFE6666666666666, double %conv3.i.5.1) #3
  %conv26.i.5.1 = fptrunc double %800 to float
  store float %conv26.i.5.1, float* %arrayidx.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %801 = add nuw nsw i64 %_local_id_x.0.5, 2
  %exitcond.5.not.1 = icmp eq i64 %801, 32
  br i1 %exitcond.5.not.1, label %pregion_for_end.i.5.loopexit, label %pregion_for_entry.entry.i.5, !llvm.loop !26

pregion_for_end.i.5.loopexit:                     ; preds = %pregion_for_entry.entry.i.5
  br label %pregion_for_end.i.5

pregion_for_end.i.5:                              ; preds = %pregion_for_end.i.5.loopexit, %vector.body110
  %802 = trunc i64 %mul3.i.i to i32
  %conv2.i.6 = or i32 %802, 6
  %mul.i.6 = mul nsw i32 %conv2.i.6, %4
  %add13.i.6 = or i32 %802, 7
  %mul14.i.6 = mul nsw i32 %add13.i.6, %4
  %803 = mul i32 %conv2.i.6, %4
  %804 = trunc i64 %6 to i32
  %805 = shl i32 %804, 5
  %806 = add nsw i32 %803, %805
  %807 = icmp sgt i32 %806, 2147483616
  %808 = add i32 %803, %805
  %809 = or i32 %808, 1
  %810 = icmp sgt i32 %809, 2147483616
  %811 = or i1 %807, %810
  %812 = mul i32 %add13.i.6, %4
  %813 = add nsw i32 %812, %805
  %814 = icmp sgt i32 %813, 2147483616
  %815 = or i1 %811, %814
  br i1 %815, label %pregion_for_entry.entry.i.6.preheader, label %vector.body134

pregion_for_entry.entry.i.6.preheader:            ; preds = %pregion_for_end.i.5
  br label %pregion_for_entry.entry.i.6

vector.body134:                                   ; preds = %pregion_for_end.i.5
  %816 = trunc i64 %mul.i.i to i32
  %817 = add i32 %mul.i.6, %816
  %818 = sext i32 %817 to i64
  %819 = getelementptr inbounds float, float* %2, i64 %818
  %820 = bitcast float* %819 to <8 x float>*
  %wide.load151 = load <8 x float>, <8 x float>* %820, align 4, !tbaa !12, !llvm.access.group !16
  %821 = fpext <8 x float> %wide.load151 to <8 x double>
  %822 = or i32 %817, 1
  %823 = sext i32 %822 to i64
  %824 = getelementptr inbounds float, float* %0, i64 %823
  %825 = bitcast float* %824 to <8 x float>*
  %wide.load152 = load <8 x float>, <8 x float>* %825, align 4, !tbaa !12, !llvm.access.group !16
  %826 = getelementptr inbounds float, float* %0, i64 %818
  %827 = bitcast float* %826 to <8 x float>*
  %wide.load153 = load <8 x float>, <8 x float>* %827, align 4, !tbaa !12, !llvm.access.group !16
  %828 = fsub <8 x float> %wide.load152, %wide.load153
  %829 = add nsw i32 %mul14.i.6, %816
  %830 = sext i32 %829 to i64
  %831 = getelementptr inbounds float, float* %1, i64 %830
  %832 = bitcast float* %831 to <8 x float>*
  %wide.load154 = load <8 x float>, <8 x float>* %832, align 4, !tbaa !12, !llvm.access.group !16
  %833 = fadd <8 x float> %828, %wide.load154
  %834 = getelementptr inbounds float, float* %1, i64 %818
  %835 = bitcast float* %834 to <8 x float>*
  %wide.load155 = load <8 x float>, <8 x float>* %835, align 4, !tbaa !12, !llvm.access.group !16
  %836 = fsub <8 x float> %833, %wide.load155
  %837 = fpext <8 x float> %836 to <8 x double>
  %838 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %837, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %821)
  %839 = fptrunc <8 x double> %838 to <8 x float>
  %840 = bitcast float* %819 to <8 x float>*
  store <8 x float> %839, <8 x float>* %840, align 4, !tbaa !12, !llvm.access.group !16
  %841 = trunc i64 %mul.i.i to i32
  %842 = or i32 %841, 8
  %843 = add i32 %mul.i.6, %842
  %844 = sext i32 %843 to i64
  %845 = getelementptr inbounds float, float* %2, i64 %844
  %846 = bitcast float* %845 to <8 x float>*
  %wide.load151.1 = load <8 x float>, <8 x float>* %846, align 4, !tbaa !12, !llvm.access.group !16
  %847 = fpext <8 x float> %wide.load151.1 to <8 x double>
  %848 = or i32 %843, 1
  %849 = sext i32 %848 to i64
  %850 = getelementptr inbounds float, float* %0, i64 %849
  %851 = bitcast float* %850 to <8 x float>*
  %wide.load152.1 = load <8 x float>, <8 x float>* %851, align 4, !tbaa !12, !llvm.access.group !16
  %852 = getelementptr inbounds float, float* %0, i64 %844
  %853 = bitcast float* %852 to <8 x float>*
  %wide.load153.1 = load <8 x float>, <8 x float>* %853, align 4, !tbaa !12, !llvm.access.group !16
  %854 = fsub <8 x float> %wide.load152.1, %wide.load153.1
  %855 = add nsw i32 %mul14.i.6, %842
  %856 = sext i32 %855 to i64
  %857 = getelementptr inbounds float, float* %1, i64 %856
  %858 = bitcast float* %857 to <8 x float>*
  %wide.load154.1 = load <8 x float>, <8 x float>* %858, align 4, !tbaa !12, !llvm.access.group !16
  %859 = fadd <8 x float> %854, %wide.load154.1
  %860 = getelementptr inbounds float, float* %1, i64 %844
  %861 = bitcast float* %860 to <8 x float>*
  %wide.load155.1 = load <8 x float>, <8 x float>* %861, align 4, !tbaa !12, !llvm.access.group !16
  %862 = fsub <8 x float> %859, %wide.load155.1
  %863 = fpext <8 x float> %862 to <8 x double>
  %864 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %863, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %847)
  %865 = fptrunc <8 x double> %864 to <8 x float>
  %866 = bitcast float* %845 to <8 x float>*
  store <8 x float> %865, <8 x float>* %866, align 4, !tbaa !12, !llvm.access.group !16
  %867 = trunc i64 %mul.i.i to i32
  %868 = or i32 %867, 16
  %869 = add i32 %mul.i.6, %868
  %870 = sext i32 %869 to i64
  %871 = getelementptr inbounds float, float* %2, i64 %870
  %872 = bitcast float* %871 to <8 x float>*
  %wide.load151.2 = load <8 x float>, <8 x float>* %872, align 4, !tbaa !12, !llvm.access.group !16
  %873 = fpext <8 x float> %wide.load151.2 to <8 x double>
  %874 = or i32 %869, 1
  %875 = sext i32 %874 to i64
  %876 = getelementptr inbounds float, float* %0, i64 %875
  %877 = bitcast float* %876 to <8 x float>*
  %wide.load152.2 = load <8 x float>, <8 x float>* %877, align 4, !tbaa !12, !llvm.access.group !16
  %878 = getelementptr inbounds float, float* %0, i64 %870
  %879 = bitcast float* %878 to <8 x float>*
  %wide.load153.2 = load <8 x float>, <8 x float>* %879, align 4, !tbaa !12, !llvm.access.group !16
  %880 = fsub <8 x float> %wide.load152.2, %wide.load153.2
  %881 = add nsw i32 %mul14.i.6, %868
  %882 = sext i32 %881 to i64
  %883 = getelementptr inbounds float, float* %1, i64 %882
  %884 = bitcast float* %883 to <8 x float>*
  %wide.load154.2 = load <8 x float>, <8 x float>* %884, align 4, !tbaa !12, !llvm.access.group !16
  %885 = fadd <8 x float> %880, %wide.load154.2
  %886 = getelementptr inbounds float, float* %1, i64 %870
  %887 = bitcast float* %886 to <8 x float>*
  %wide.load155.2 = load <8 x float>, <8 x float>* %887, align 4, !tbaa !12, !llvm.access.group !16
  %888 = fsub <8 x float> %885, %wide.load155.2
  %889 = fpext <8 x float> %888 to <8 x double>
  %890 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %889, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %873)
  %891 = fptrunc <8 x double> %890 to <8 x float>
  %892 = bitcast float* %871 to <8 x float>*
  store <8 x float> %891, <8 x float>* %892, align 4, !tbaa !12, !llvm.access.group !16
  %893 = trunc i64 %mul.i.i to i32
  %894 = or i32 %893, 24
  %895 = add i32 %mul.i.6, %894
  %896 = sext i32 %895 to i64
  %897 = getelementptr inbounds float, float* %2, i64 %896
  %898 = bitcast float* %897 to <8 x float>*
  %wide.load151.3 = load <8 x float>, <8 x float>* %898, align 4, !tbaa !12, !llvm.access.group !16
  %899 = fpext <8 x float> %wide.load151.3 to <8 x double>
  %900 = or i32 %895, 1
  %901 = sext i32 %900 to i64
  %902 = getelementptr inbounds float, float* %0, i64 %901
  %903 = bitcast float* %902 to <8 x float>*
  %wide.load152.3 = load <8 x float>, <8 x float>* %903, align 4, !tbaa !12, !llvm.access.group !16
  %904 = getelementptr inbounds float, float* %0, i64 %896
  %905 = bitcast float* %904 to <8 x float>*
  %wide.load153.3 = load <8 x float>, <8 x float>* %905, align 4, !tbaa !12, !llvm.access.group !16
  %906 = fsub <8 x float> %wide.load152.3, %wide.load153.3
  %907 = add nsw i32 %mul14.i.6, %894
  %908 = sext i32 %907 to i64
  %909 = getelementptr inbounds float, float* %1, i64 %908
  %910 = bitcast float* %909 to <8 x float>*
  %wide.load154.3 = load <8 x float>, <8 x float>* %910, align 4, !tbaa !12, !llvm.access.group !16
  %911 = fadd <8 x float> %906, %wide.load154.3
  %912 = getelementptr inbounds float, float* %1, i64 %896
  %913 = bitcast float* %912 to <8 x float>*
  %wide.load155.3 = load <8 x float>, <8 x float>* %913, align 4, !tbaa !12, !llvm.access.group !16
  %914 = fsub <8 x float> %911, %wide.load155.3
  %915 = fpext <8 x float> %914 to <8 x double>
  %916 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %915, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %899)
  %917 = fptrunc <8 x double> %916 to <8 x float>
  %918 = bitcast float* %897 to <8 x float>*
  store <8 x float> %917, <8 x float>* %918, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.6

pregion_for_entry.entry.i.6:                      ; preds = %pregion_for_entry.entry.i.6, %pregion_for_entry.entry.i.6.preheader
  %_local_id_x.0.6 = phi i64 [ %932, %pregion_for_entry.entry.i.6 ], [ 0, %pregion_for_entry.entry.i.6.preheader ]
  %add1.i.i.6 = add nuw nsw i64 %_local_id_x.0.6, %mul.i.i
  %conv.i.6 = trunc i64 %add1.i.i.6 to i32
  %add.i.6 = add i32 %mul.i.6, %conv.i.6
  %idxprom.i.6 = sext i32 %add.i.6 to i64
  %arrayidx.i.6 = getelementptr inbounds float, float* %2, i64 %idxprom.i.6
  %919 = load float, float* %arrayidx.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.6 = fpext float %919 to double
  %add6.i.6 = or i32 %add.i.6, 1
  %idxprom7.i.6 = sext i32 %add6.i.6 to i64
  %arrayidx8.i.6 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.6
  %920 = load float, float* %arrayidx8.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.6 = getelementptr inbounds float, float* %0, i64 %idxprom.i.6
  %921 = load float, float* %arrayidx12.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.6 = fsub float %920, %921
  %add15.i.6 = add nsw i32 %mul14.i.6, %conv.i.6
  %idxprom16.i.6 = sext i32 %add15.i.6 to i64
  %arrayidx17.i.6 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.6
  %922 = load float, float* %arrayidx17.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.6 = fadd float %sub.i.6, %922
  %arrayidx22.i.6 = getelementptr inbounds float, float* %1, i64 %idxprom.i.6
  %923 = load float, float* %arrayidx22.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.6 = fsub float %add18.i.6, %923
  %conv24.i.6 = fpext float %sub23.i.6 to double
  %924 = tail call double @llvm.fmuladd.f64(double %conv24.i.6, double 0xBFE6666666666666, double %conv3.i.6) #3
  %conv26.i.6 = fptrunc double %924 to float
  store float %conv26.i.6, float* %arrayidx.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %925 = or i64 %_local_id_x.0.6, 1
  %add1.i.i.6.1 = add nuw nsw i64 %925, %mul.i.i
  %conv.i.6.1 = trunc i64 %add1.i.i.6.1 to i32
  %add.i.6.1 = add i32 %mul.i.6, %conv.i.6.1
  %idxprom.i.6.1 = sext i32 %add.i.6.1 to i64
  %arrayidx.i.6.1 = getelementptr inbounds float, float* %2, i64 %idxprom.i.6.1
  %926 = load float, float* %arrayidx.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.6.1 = fpext float %926 to double
  %add6.i.6.1 = add i32 %add.i.6.1, 1
  %idxprom7.i.6.1 = sext i32 %add6.i.6.1 to i64
  %arrayidx8.i.6.1 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.6.1
  %927 = load float, float* %arrayidx8.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.6.1 = getelementptr inbounds float, float* %0, i64 %idxprom.i.6.1
  %928 = load float, float* %arrayidx12.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.6.1 = fsub float %927, %928
  %add15.i.6.1 = add nsw i32 %mul14.i.6, %conv.i.6.1
  %idxprom16.i.6.1 = sext i32 %add15.i.6.1 to i64
  %arrayidx17.i.6.1 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.6.1
  %929 = load float, float* %arrayidx17.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.6.1 = fadd float %sub.i.6.1, %929
  %arrayidx22.i.6.1 = getelementptr inbounds float, float* %1, i64 %idxprom.i.6.1
  %930 = load float, float* %arrayidx22.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.6.1 = fsub float %add18.i.6.1, %930
  %conv24.i.6.1 = fpext float %sub23.i.6.1 to double
  %931 = tail call double @llvm.fmuladd.f64(double %conv24.i.6.1, double 0xBFE6666666666666, double %conv3.i.6.1) #3
  %conv26.i.6.1 = fptrunc double %931 to float
  store float %conv26.i.6.1, float* %arrayidx.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %932 = add nuw nsw i64 %_local_id_x.0.6, 2
  %exitcond.6.not.1 = icmp eq i64 %932, 32
  br i1 %exitcond.6.not.1, label %pregion_for_end.i.6.loopexit, label %pregion_for_entry.entry.i.6, !llvm.loop !27

pregion_for_end.i.6.loopexit:                     ; preds = %pregion_for_entry.entry.i.6
  br label %pregion_for_end.i.6

pregion_for_end.i.6:                              ; preds = %pregion_for_end.i.6.loopexit, %vector.body134
  %933 = trunc i64 %mul3.i.i to i32
  %conv2.i.7 = or i32 %933, 7
  %mul.i.7 = mul nsw i32 %conv2.i.7, %4
  %add13.i.7 = add nsw i32 %conv2.i.7, 1
  %mul14.i.7 = mul nsw i32 %add13.i.7, %4
  %934 = mul i32 %add13.i.6, %4
  %935 = trunc i64 %6 to i32
  %936 = shl i32 %935, 5
  %937 = add nsw i32 %934, %936
  %938 = icmp sgt i32 %937, 2147483616
  %939 = add i32 %934, %936
  %940 = add i32 %939, 1
  %941 = add i32 %939, 32
  %942 = icmp slt i32 %941, %940
  %943 = or i1 %938, %942
  %944 = add i32 %conv2.i, 8
  %945 = mul i32 %944, %4
  %946 = add nsw i32 %945, %936
  %947 = icmp sgt i32 %946, 2147483616
  %948 = or i1 %943, %947
  br i1 %948, label %pregion_for_entry.entry.i.7.preheader, label %vector.body158

pregion_for_entry.entry.i.7.preheader:            ; preds = %pregion_for_end.i.6
  br label %pregion_for_entry.entry.i.7

vector.body158:                                   ; preds = %pregion_for_end.i.6
  %949 = trunc i64 %mul.i.i to i32
  %950 = add i32 %mul.i.7, %949
  %951 = sext i32 %950 to i64
  %952 = getelementptr inbounds float, float* %2, i64 %951
  %953 = bitcast float* %952 to <8 x float>*
  %wide.load175 = load <8 x float>, <8 x float>* %953, align 4, !tbaa !12, !llvm.access.group !16
  %954 = fpext <8 x float> %wide.load175 to <8 x double>
  %955 = add i32 %950, 1
  %956 = sext i32 %955 to i64
  %957 = getelementptr inbounds float, float* %0, i64 %956
  %958 = bitcast float* %957 to <8 x float>*
  %wide.load176 = load <8 x float>, <8 x float>* %958, align 4, !tbaa !12, !llvm.access.group !16
  %959 = getelementptr inbounds float, float* %0, i64 %951
  %960 = bitcast float* %959 to <8 x float>*
  %wide.load177 = load <8 x float>, <8 x float>* %960, align 4, !tbaa !12, !llvm.access.group !16
  %961 = fsub <8 x float> %wide.load176, %wide.load177
  %962 = add nsw i32 %mul14.i.7, %949
  %963 = sext i32 %962 to i64
  %964 = getelementptr inbounds float, float* %1, i64 %963
  %965 = bitcast float* %964 to <8 x float>*
  %wide.load178 = load <8 x float>, <8 x float>* %965, align 4, !tbaa !12, !llvm.access.group !16
  %966 = fadd <8 x float> %961, %wide.load178
  %967 = getelementptr inbounds float, float* %1, i64 %951
  %968 = bitcast float* %967 to <8 x float>*
  %wide.load179 = load <8 x float>, <8 x float>* %968, align 4, !tbaa !12, !llvm.access.group !16
  %969 = fsub <8 x float> %966, %wide.load179
  %970 = fpext <8 x float> %969 to <8 x double>
  %971 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %970, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %954)
  %972 = fptrunc <8 x double> %971 to <8 x float>
  %973 = bitcast float* %952 to <8 x float>*
  store <8 x float> %972, <8 x float>* %973, align 4, !tbaa !12, !llvm.access.group !16
  %974 = trunc i64 %mul.i.i to i32
  %975 = or i32 %974, 8
  %976 = add i32 %mul.i.7, %975
  %977 = sext i32 %976 to i64
  %978 = getelementptr inbounds float, float* %2, i64 %977
  %979 = bitcast float* %978 to <8 x float>*
  %wide.load175.1 = load <8 x float>, <8 x float>* %979, align 4, !tbaa !12, !llvm.access.group !16
  %980 = fpext <8 x float> %wide.load175.1 to <8 x double>
  %981 = add i32 %976, 1
  %982 = sext i32 %981 to i64
  %983 = getelementptr inbounds float, float* %0, i64 %982
  %984 = bitcast float* %983 to <8 x float>*
  %wide.load176.1 = load <8 x float>, <8 x float>* %984, align 4, !tbaa !12, !llvm.access.group !16
  %985 = getelementptr inbounds float, float* %0, i64 %977
  %986 = bitcast float* %985 to <8 x float>*
  %wide.load177.1 = load <8 x float>, <8 x float>* %986, align 4, !tbaa !12, !llvm.access.group !16
  %987 = fsub <8 x float> %wide.load176.1, %wide.load177.1
  %988 = add nsw i32 %mul14.i.7, %975
  %989 = sext i32 %988 to i64
  %990 = getelementptr inbounds float, float* %1, i64 %989
  %991 = bitcast float* %990 to <8 x float>*
  %wide.load178.1 = load <8 x float>, <8 x float>* %991, align 4, !tbaa !12, !llvm.access.group !16
  %992 = fadd <8 x float> %987, %wide.load178.1
  %993 = getelementptr inbounds float, float* %1, i64 %977
  %994 = bitcast float* %993 to <8 x float>*
  %wide.load179.1 = load <8 x float>, <8 x float>* %994, align 4, !tbaa !12, !llvm.access.group !16
  %995 = fsub <8 x float> %992, %wide.load179.1
  %996 = fpext <8 x float> %995 to <8 x double>
  %997 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %996, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %980)
  %998 = fptrunc <8 x double> %997 to <8 x float>
  %999 = bitcast float* %978 to <8 x float>*
  store <8 x float> %998, <8 x float>* %999, align 4, !tbaa !12, !llvm.access.group !16
  %1000 = trunc i64 %mul.i.i to i32
  %1001 = or i32 %1000, 16
  %1002 = add i32 %mul.i.7, %1001
  %1003 = sext i32 %1002 to i64
  %1004 = getelementptr inbounds float, float* %2, i64 %1003
  %1005 = bitcast float* %1004 to <8 x float>*
  %wide.load175.2 = load <8 x float>, <8 x float>* %1005, align 4, !tbaa !12, !llvm.access.group !16
  %1006 = fpext <8 x float> %wide.load175.2 to <8 x double>
  %1007 = add i32 %1002, 1
  %1008 = sext i32 %1007 to i64
  %1009 = getelementptr inbounds float, float* %0, i64 %1008
  %1010 = bitcast float* %1009 to <8 x float>*
  %wide.load176.2 = load <8 x float>, <8 x float>* %1010, align 4, !tbaa !12, !llvm.access.group !16
  %1011 = getelementptr inbounds float, float* %0, i64 %1003
  %1012 = bitcast float* %1011 to <8 x float>*
  %wide.load177.2 = load <8 x float>, <8 x float>* %1012, align 4, !tbaa !12, !llvm.access.group !16
  %1013 = fsub <8 x float> %wide.load176.2, %wide.load177.2
  %1014 = add nsw i32 %mul14.i.7, %1001
  %1015 = sext i32 %1014 to i64
  %1016 = getelementptr inbounds float, float* %1, i64 %1015
  %1017 = bitcast float* %1016 to <8 x float>*
  %wide.load178.2 = load <8 x float>, <8 x float>* %1017, align 4, !tbaa !12, !llvm.access.group !16
  %1018 = fadd <8 x float> %1013, %wide.load178.2
  %1019 = getelementptr inbounds float, float* %1, i64 %1003
  %1020 = bitcast float* %1019 to <8 x float>*
  %wide.load179.2 = load <8 x float>, <8 x float>* %1020, align 4, !tbaa !12, !llvm.access.group !16
  %1021 = fsub <8 x float> %1018, %wide.load179.2
  %1022 = fpext <8 x float> %1021 to <8 x double>
  %1023 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %1022, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %1006)
  %1024 = fptrunc <8 x double> %1023 to <8 x float>
  %1025 = bitcast float* %1004 to <8 x float>*
  store <8 x float> %1024, <8 x float>* %1025, align 4, !tbaa !12, !llvm.access.group !16
  %1026 = trunc i64 %mul.i.i to i32
  %1027 = or i32 %1026, 24
  %1028 = add i32 %mul.i.7, %1027
  %1029 = sext i32 %1028 to i64
  %1030 = getelementptr inbounds float, float* %2, i64 %1029
  %1031 = bitcast float* %1030 to <8 x float>*
  %wide.load175.3 = load <8 x float>, <8 x float>* %1031, align 4, !tbaa !12, !llvm.access.group !16
  %1032 = fpext <8 x float> %wide.load175.3 to <8 x double>
  %1033 = add i32 %1028, 1
  %1034 = sext i32 %1033 to i64
  %1035 = getelementptr inbounds float, float* %0, i64 %1034
  %1036 = bitcast float* %1035 to <8 x float>*
  %wide.load176.3 = load <8 x float>, <8 x float>* %1036, align 4, !tbaa !12, !llvm.access.group !16
  %1037 = getelementptr inbounds float, float* %0, i64 %1029
  %1038 = bitcast float* %1037 to <8 x float>*
  %wide.load177.3 = load <8 x float>, <8 x float>* %1038, align 4, !tbaa !12, !llvm.access.group !16
  %1039 = fsub <8 x float> %wide.load176.3, %wide.load177.3
  %1040 = add nsw i32 %mul14.i.7, %1027
  %1041 = sext i32 %1040 to i64
  %1042 = getelementptr inbounds float, float* %1, i64 %1041
  %1043 = bitcast float* %1042 to <8 x float>*
  %wide.load178.3 = load <8 x float>, <8 x float>* %1043, align 4, !tbaa !12, !llvm.access.group !16
  %1044 = fadd <8 x float> %1039, %wide.load178.3
  %1045 = getelementptr inbounds float, float* %1, i64 %1029
  %1046 = bitcast float* %1045 to <8 x float>*
  %wide.load179.3 = load <8 x float>, <8 x float>* %1046, align 4, !tbaa !12, !llvm.access.group !16
  %1047 = fsub <8 x float> %1044, %wide.load179.3
  %1048 = fpext <8 x float> %1047 to <8 x double>
  %1049 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %1048, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %1032)
  %1050 = fptrunc <8 x double> %1049 to <8 x float>
  %1051 = bitcast float* %1030 to <8 x float>*
  store <8 x float> %1050, <8 x float>* %1051, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.7

pregion_for_entry.entry.i.7:                      ; preds = %pregion_for_entry.entry.i.7, %pregion_for_entry.entry.i.7.preheader
  %_local_id_x.0.7 = phi i64 [ %1065, %pregion_for_entry.entry.i.7 ], [ 0, %pregion_for_entry.entry.i.7.preheader ]
  %add1.i.i.7 = add nuw nsw i64 %_local_id_x.0.7, %mul.i.i
  %conv.i.7 = trunc i64 %add1.i.i.7 to i32
  %add.i.7 = add i32 %mul.i.7, %conv.i.7
  %idxprom.i.7 = sext i32 %add.i.7 to i64
  %arrayidx.i.7 = getelementptr inbounds float, float* %2, i64 %idxprom.i.7
  %1052 = load float, float* %arrayidx.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.7 = fpext float %1052 to double
  %add6.i.7 = add i32 %add.i.7, 1
  %idxprom7.i.7 = sext i32 %add6.i.7 to i64
  %arrayidx8.i.7 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.7
  %1053 = load float, float* %arrayidx8.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.7 = getelementptr inbounds float, float* %0, i64 %idxprom.i.7
  %1054 = load float, float* %arrayidx12.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.7 = fsub float %1053, %1054
  %add15.i.7 = add nsw i32 %mul14.i.7, %conv.i.7
  %idxprom16.i.7 = sext i32 %add15.i.7 to i64
  %arrayidx17.i.7 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.7
  %1055 = load float, float* %arrayidx17.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.7 = fadd float %sub.i.7, %1055
  %arrayidx22.i.7 = getelementptr inbounds float, float* %1, i64 %idxprom.i.7
  %1056 = load float, float* %arrayidx22.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.7 = fsub float %add18.i.7, %1056
  %conv24.i.7 = fpext float %sub23.i.7 to double
  %1057 = tail call double @llvm.fmuladd.f64(double %conv24.i.7, double 0xBFE6666666666666, double %conv3.i.7) #3
  %conv26.i.7 = fptrunc double %1057 to float
  store float %conv26.i.7, float* %arrayidx.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %1058 = or i64 %_local_id_x.0.7, 1
  %add1.i.i.7.1 = add nuw nsw i64 %1058, %mul.i.i
  %conv.i.7.1 = trunc i64 %add1.i.i.7.1 to i32
  %add.i.7.1 = add i32 %mul.i.7, %conv.i.7.1
  %idxprom.i.7.1 = sext i32 %add.i.7.1 to i64
  %arrayidx.i.7.1 = getelementptr inbounds float, float* %2, i64 %idxprom.i.7.1
  %1059 = load float, float* %arrayidx.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.7.1 = fpext float %1059 to double
  %add6.i.7.1 = add i32 %add.i.7.1, 1
  %idxprom7.i.7.1 = sext i32 %add6.i.7.1 to i64
  %arrayidx8.i.7.1 = getelementptr inbounds float, float* %0, i64 %idxprom7.i.7.1
  %1060 = load float, float* %arrayidx8.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.7.1 = getelementptr inbounds float, float* %0, i64 %idxprom.i.7.1
  %1061 = load float, float* %arrayidx12.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.7.1 = fsub float %1060, %1061
  %add15.i.7.1 = add nsw i32 %mul14.i.7, %conv.i.7.1
  %idxprom16.i.7.1 = sext i32 %add15.i.7.1 to i64
  %arrayidx17.i.7.1 = getelementptr inbounds float, float* %1, i64 %idxprom16.i.7.1
  %1062 = load float, float* %arrayidx17.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.7.1 = fadd float %sub.i.7.1, %1062
  %arrayidx22.i.7.1 = getelementptr inbounds float, float* %1, i64 %idxprom.i.7.1
  %1063 = load float, float* %arrayidx22.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.7.1 = fsub float %add18.i.7.1, %1063
  %conv24.i.7.1 = fpext float %sub23.i.7.1 to double
  %1064 = tail call double @llvm.fmuladd.f64(double %conv24.i.7.1, double 0xBFE6666666666666, double %conv3.i.7.1) #3
  %conv26.i.7.1 = fptrunc double %1064 to float
  store float %conv26.i.7.1, float* %arrayidx.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %1065 = add nuw nsw i64 %_local_id_x.0.7, 2
  %exitcond.7.not.1 = icmp eq i64 %1065, 32
  br i1 %exitcond.7.not.1, label %pregion_for_end.i.7.loopexit, label %pregion_for_entry.entry.i.7, !llvm.loop !28

pregion_for_end.i.7.loopexit:                     ; preds = %pregion_for_entry.entry.i.7
  br label %pregion_for_end.i.7

pregion_for_end.i.7:                              ; preds = %pregion_for_end.i.7.loopexit, %vector.body158
  ret void
}

; Function Attrs: nofree nounwind
define void @_pocl_kernel_fdtd_kernel3_workgroup(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #2 {
pregion_for_entry.pregion_for_init.i.i:
  %5 = bitcast i8** %0 to float***
  %6 = load float**, float*** %5, align 8
  %7 = load float*, float** %6, align 8
  %8 = getelementptr i8*, i8** %0, i64 1
  %9 = bitcast i8** %8 to float***
  %10 = load float**, float*** %9, align 8
  %11 = load float*, float** %10, align 8
  %12 = getelementptr i8*, i8** %0, i64 2
  %13 = bitcast i8** %12 to float***
  %14 = load float**, float*** %13, align 8
  %15 = load float*, float** %14, align 8
  %16 = getelementptr i8*, i8** %0, i64 4
  %17 = bitcast i8** %16 to i32**
  %18 = load i32*, i32** %17, align 8
  %19 = load i32, i32* %18, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %conv2.i.i = trunc i64 %mul3.i.i.i to i32
  %mul.i.i = mul nsw i32 %19, %conv2.i.i
  %add13.i.i = or i32 %conv2.i.i, 1
  %mul14.i.i = mul nsw i32 %add13.i.i, %19
  %20 = trunc i64 %3 to i32
  %21 = mul i32 %19, %20
  %22 = shl i32 %21, 3
  %23 = trunc i64 %2 to i32
  %24 = shl i32 %23, 5
  %25 = add i32 %22, %24
  %26 = icmp sgt i32 %25, 2147483616
  %27 = add i32 %22, %24
  %28 = or i32 %27, 1
  %29 = icmp sgt i32 %28, 2147483616
  %30 = or i1 %26, %29
  %31 = mul i32 %19, %add13.i.i
  %32 = add nsw i32 %31, %24
  %33 = icmp sgt i32 %32, 2147483616
  %34 = or i1 %30, %33
  br i1 %34, label %pregion_for_entry.entry.i.i.preheader, label %vector.body

pregion_for_entry.entry.i.i.preheader:            ; preds = %pregion_for_entry.pregion_for_init.i.i
  br label %pregion_for_entry.entry.i.i

vector.body:                                      ; preds = %pregion_for_entry.pregion_for_init.i.i
  %35 = trunc i64 %mul.i.i.i to i32
  %36 = add i32 %mul.i.i, %35
  %37 = sext i32 %36 to i64
  %38 = getelementptr inbounds float, float* %15, i64 %37
  %39 = bitcast float* %38 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %39, align 4, !tbaa !12, !llvm.access.group !16
  %40 = fpext <8 x float> %wide.load to <8 x double>
  %41 = or i32 %36, 1
  %42 = sext i32 %41 to i64
  %43 = getelementptr inbounds float, float* %7, i64 %42
  %44 = bitcast float* %43 to <8 x float>*
  %wide.load8 = load <8 x float>, <8 x float>* %44, align 4, !tbaa !12, !llvm.access.group !16
  %45 = getelementptr inbounds float, float* %7, i64 %37
  %46 = bitcast float* %45 to <8 x float>*
  %wide.load9 = load <8 x float>, <8 x float>* %46, align 4, !tbaa !12, !llvm.access.group !16
  %47 = fsub <8 x float> %wide.load8, %wide.load9
  %48 = add nsw i32 %mul14.i.i, %35
  %49 = sext i32 %48 to i64
  %50 = getelementptr inbounds float, float* %11, i64 %49
  %51 = bitcast float* %50 to <8 x float>*
  %wide.load10 = load <8 x float>, <8 x float>* %51, align 4, !tbaa !12, !llvm.access.group !16
  %52 = fadd <8 x float> %47, %wide.load10
  %53 = getelementptr inbounds float, float* %11, i64 %37
  %54 = bitcast float* %53 to <8 x float>*
  %wide.load11 = load <8 x float>, <8 x float>* %54, align 4, !tbaa !12, !llvm.access.group !16
  %55 = fsub <8 x float> %52, %wide.load11
  %56 = fpext <8 x float> %55 to <8 x double>
  %57 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %56, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %40)
  %58 = fptrunc <8 x double> %57 to <8 x float>
  %59 = bitcast float* %38 to <8 x float>*
  store <8 x float> %58, <8 x float>* %59, align 4, !tbaa !12, !llvm.access.group !16
  %60 = trunc i64 %mul.i.i.i to i32
  %61 = or i32 %60, 8
  %62 = add i32 %mul.i.i, %61
  %63 = sext i32 %62 to i64
  %64 = getelementptr inbounds float, float* %15, i64 %63
  %65 = bitcast float* %64 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %65, align 4, !tbaa !12, !llvm.access.group !16
  %66 = fpext <8 x float> %wide.load.1 to <8 x double>
  %67 = or i32 %62, 1
  %68 = sext i32 %67 to i64
  %69 = getelementptr inbounds float, float* %7, i64 %68
  %70 = bitcast float* %69 to <8 x float>*
  %wide.load8.1 = load <8 x float>, <8 x float>* %70, align 4, !tbaa !12, !llvm.access.group !16
  %71 = getelementptr inbounds float, float* %7, i64 %63
  %72 = bitcast float* %71 to <8 x float>*
  %wide.load9.1 = load <8 x float>, <8 x float>* %72, align 4, !tbaa !12, !llvm.access.group !16
  %73 = fsub <8 x float> %wide.load8.1, %wide.load9.1
  %74 = add nsw i32 %mul14.i.i, %61
  %75 = sext i32 %74 to i64
  %76 = getelementptr inbounds float, float* %11, i64 %75
  %77 = bitcast float* %76 to <8 x float>*
  %wide.load10.1 = load <8 x float>, <8 x float>* %77, align 4, !tbaa !12, !llvm.access.group !16
  %78 = fadd <8 x float> %73, %wide.load10.1
  %79 = getelementptr inbounds float, float* %11, i64 %63
  %80 = bitcast float* %79 to <8 x float>*
  %wide.load11.1 = load <8 x float>, <8 x float>* %80, align 4, !tbaa !12, !llvm.access.group !16
  %81 = fsub <8 x float> %78, %wide.load11.1
  %82 = fpext <8 x float> %81 to <8 x double>
  %83 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %82, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %66)
  %84 = fptrunc <8 x double> %83 to <8 x float>
  %85 = bitcast float* %64 to <8 x float>*
  store <8 x float> %84, <8 x float>* %85, align 4, !tbaa !12, !llvm.access.group !16
  %86 = trunc i64 %mul.i.i.i to i32
  %87 = or i32 %86, 16
  %88 = add i32 %mul.i.i, %87
  %89 = sext i32 %88 to i64
  %90 = getelementptr inbounds float, float* %15, i64 %89
  %91 = bitcast float* %90 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %91, align 4, !tbaa !12, !llvm.access.group !16
  %92 = fpext <8 x float> %wide.load.2 to <8 x double>
  %93 = or i32 %88, 1
  %94 = sext i32 %93 to i64
  %95 = getelementptr inbounds float, float* %7, i64 %94
  %96 = bitcast float* %95 to <8 x float>*
  %wide.load8.2 = load <8 x float>, <8 x float>* %96, align 4, !tbaa !12, !llvm.access.group !16
  %97 = getelementptr inbounds float, float* %7, i64 %89
  %98 = bitcast float* %97 to <8 x float>*
  %wide.load9.2 = load <8 x float>, <8 x float>* %98, align 4, !tbaa !12, !llvm.access.group !16
  %99 = fsub <8 x float> %wide.load8.2, %wide.load9.2
  %100 = add nsw i32 %mul14.i.i, %87
  %101 = sext i32 %100 to i64
  %102 = getelementptr inbounds float, float* %11, i64 %101
  %103 = bitcast float* %102 to <8 x float>*
  %wide.load10.2 = load <8 x float>, <8 x float>* %103, align 4, !tbaa !12, !llvm.access.group !16
  %104 = fadd <8 x float> %99, %wide.load10.2
  %105 = getelementptr inbounds float, float* %11, i64 %89
  %106 = bitcast float* %105 to <8 x float>*
  %wide.load11.2 = load <8 x float>, <8 x float>* %106, align 4, !tbaa !12, !llvm.access.group !16
  %107 = fsub <8 x float> %104, %wide.load11.2
  %108 = fpext <8 x float> %107 to <8 x double>
  %109 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %108, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %92)
  %110 = fptrunc <8 x double> %109 to <8 x float>
  %111 = bitcast float* %90 to <8 x float>*
  store <8 x float> %110, <8 x float>* %111, align 4, !tbaa !12, !llvm.access.group !16
  %112 = trunc i64 %mul.i.i.i to i32
  %113 = or i32 %112, 24
  %114 = add i32 %mul.i.i, %113
  %115 = sext i32 %114 to i64
  %116 = getelementptr inbounds float, float* %15, i64 %115
  %117 = bitcast float* %116 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %117, align 4, !tbaa !12, !llvm.access.group !16
  %118 = fpext <8 x float> %wide.load.3 to <8 x double>
  %119 = or i32 %114, 1
  %120 = sext i32 %119 to i64
  %121 = getelementptr inbounds float, float* %7, i64 %120
  %122 = bitcast float* %121 to <8 x float>*
  %wide.load8.3 = load <8 x float>, <8 x float>* %122, align 4, !tbaa !12, !llvm.access.group !16
  %123 = getelementptr inbounds float, float* %7, i64 %115
  %124 = bitcast float* %123 to <8 x float>*
  %wide.load9.3 = load <8 x float>, <8 x float>* %124, align 4, !tbaa !12, !llvm.access.group !16
  %125 = fsub <8 x float> %wide.load8.3, %wide.load9.3
  %126 = add nsw i32 %mul14.i.i, %113
  %127 = sext i32 %126 to i64
  %128 = getelementptr inbounds float, float* %11, i64 %127
  %129 = bitcast float* %128 to <8 x float>*
  %wide.load10.3 = load <8 x float>, <8 x float>* %129, align 4, !tbaa !12, !llvm.access.group !16
  %130 = fadd <8 x float> %125, %wide.load10.3
  %131 = getelementptr inbounds float, float* %11, i64 %115
  %132 = bitcast float* %131 to <8 x float>*
  %wide.load11.3 = load <8 x float>, <8 x float>* %132, align 4, !tbaa !12, !llvm.access.group !16
  %133 = fsub <8 x float> %130, %wide.load11.3
  %134 = fpext <8 x float> %133 to <8 x double>
  %135 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %134, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %118)
  %136 = fptrunc <8 x double> %135 to <8 x float>
  %137 = bitcast float* %116 to <8 x float>*
  store <8 x float> %136, <8 x float>* %137, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i

pregion_for_end.i.i.loopexit:                     ; preds = %pregion_for_entry.entry.i.i
  br label %pregion_for_end.i.i

pregion_for_end.i.i:                              ; preds = %pregion_for_end.i.i.loopexit, %vector.body
  %138 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.1 = or i32 %138, 1
  %mul.i.i.1 = mul nsw i32 %19, %conv2.i.i.1
  %add13.i.i.1 = add nuw nsw i32 %conv2.i.i.1, 1
  %mul14.i.i.1 = mul nsw i32 %add13.i.i.1, %19
  %139 = mul i32 %19, %add13.i.i
  %140 = trunc i64 %2 to i32
  %141 = shl i32 %140, 5
  %142 = add nsw i32 %139, %141
  %143 = icmp sgt i32 %142, 2147483616
  %144 = add i32 %139, %141
  %145 = add i32 %144, 1
  %146 = add i32 %144, 32
  %147 = icmp slt i32 %146, %145
  %148 = or i1 %143, %147
  %149 = or i32 %conv2.i.i, 2
  %150 = mul i32 %19, %149
  %151 = add nsw i32 %150, %141
  %152 = icmp sgt i32 %151, 2147483616
  %153 = or i1 %148, %152
  br i1 %153, label %pregion_for_entry.entry.i.i.1.preheader, label %vector.body14

pregion_for_entry.entry.i.i.1.preheader:          ; preds = %pregion_for_end.i.i
  br label %pregion_for_entry.entry.i.i.1

vector.body14:                                    ; preds = %pregion_for_end.i.i
  %154 = trunc i64 %mul.i.i.i to i32
  %155 = add i32 %mul.i.i.1, %154
  %156 = sext i32 %155 to i64
  %157 = getelementptr inbounds float, float* %15, i64 %156
  %158 = bitcast float* %157 to <8 x float>*
  %wide.load31 = load <8 x float>, <8 x float>* %158, align 4, !tbaa !12, !llvm.access.group !16
  %159 = fpext <8 x float> %wide.load31 to <8 x double>
  %160 = add i32 %155, 1
  %161 = sext i32 %160 to i64
  %162 = getelementptr inbounds float, float* %7, i64 %161
  %163 = bitcast float* %162 to <8 x float>*
  %wide.load32 = load <8 x float>, <8 x float>* %163, align 4, !tbaa !12, !llvm.access.group !16
  %164 = getelementptr inbounds float, float* %7, i64 %156
  %165 = bitcast float* %164 to <8 x float>*
  %wide.load33 = load <8 x float>, <8 x float>* %165, align 4, !tbaa !12, !llvm.access.group !16
  %166 = fsub <8 x float> %wide.load32, %wide.load33
  %167 = add nsw i32 %mul14.i.i.1, %154
  %168 = sext i32 %167 to i64
  %169 = getelementptr inbounds float, float* %11, i64 %168
  %170 = bitcast float* %169 to <8 x float>*
  %wide.load34 = load <8 x float>, <8 x float>* %170, align 4, !tbaa !12, !llvm.access.group !16
  %171 = fadd <8 x float> %166, %wide.load34
  %172 = getelementptr inbounds float, float* %11, i64 %156
  %173 = bitcast float* %172 to <8 x float>*
  %wide.load35 = load <8 x float>, <8 x float>* %173, align 4, !tbaa !12, !llvm.access.group !16
  %174 = fsub <8 x float> %171, %wide.load35
  %175 = fpext <8 x float> %174 to <8 x double>
  %176 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %175, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %159)
  %177 = fptrunc <8 x double> %176 to <8 x float>
  %178 = bitcast float* %157 to <8 x float>*
  store <8 x float> %177, <8 x float>* %178, align 4, !tbaa !12, !llvm.access.group !16
  %179 = trunc i64 %mul.i.i.i to i32
  %180 = or i32 %179, 8
  %181 = add i32 %mul.i.i.1, %180
  %182 = sext i32 %181 to i64
  %183 = getelementptr inbounds float, float* %15, i64 %182
  %184 = bitcast float* %183 to <8 x float>*
  %wide.load31.1 = load <8 x float>, <8 x float>* %184, align 4, !tbaa !12, !llvm.access.group !16
  %185 = fpext <8 x float> %wide.load31.1 to <8 x double>
  %186 = add i32 %181, 1
  %187 = sext i32 %186 to i64
  %188 = getelementptr inbounds float, float* %7, i64 %187
  %189 = bitcast float* %188 to <8 x float>*
  %wide.load32.1 = load <8 x float>, <8 x float>* %189, align 4, !tbaa !12, !llvm.access.group !16
  %190 = getelementptr inbounds float, float* %7, i64 %182
  %191 = bitcast float* %190 to <8 x float>*
  %wide.load33.1 = load <8 x float>, <8 x float>* %191, align 4, !tbaa !12, !llvm.access.group !16
  %192 = fsub <8 x float> %wide.load32.1, %wide.load33.1
  %193 = add nsw i32 %mul14.i.i.1, %180
  %194 = sext i32 %193 to i64
  %195 = getelementptr inbounds float, float* %11, i64 %194
  %196 = bitcast float* %195 to <8 x float>*
  %wide.load34.1 = load <8 x float>, <8 x float>* %196, align 4, !tbaa !12, !llvm.access.group !16
  %197 = fadd <8 x float> %192, %wide.load34.1
  %198 = getelementptr inbounds float, float* %11, i64 %182
  %199 = bitcast float* %198 to <8 x float>*
  %wide.load35.1 = load <8 x float>, <8 x float>* %199, align 4, !tbaa !12, !llvm.access.group !16
  %200 = fsub <8 x float> %197, %wide.load35.1
  %201 = fpext <8 x float> %200 to <8 x double>
  %202 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %201, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %185)
  %203 = fptrunc <8 x double> %202 to <8 x float>
  %204 = bitcast float* %183 to <8 x float>*
  store <8 x float> %203, <8 x float>* %204, align 4, !tbaa !12, !llvm.access.group !16
  %205 = trunc i64 %mul.i.i.i to i32
  %206 = or i32 %205, 16
  %207 = add i32 %mul.i.i.1, %206
  %208 = sext i32 %207 to i64
  %209 = getelementptr inbounds float, float* %15, i64 %208
  %210 = bitcast float* %209 to <8 x float>*
  %wide.load31.2 = load <8 x float>, <8 x float>* %210, align 4, !tbaa !12, !llvm.access.group !16
  %211 = fpext <8 x float> %wide.load31.2 to <8 x double>
  %212 = add i32 %207, 1
  %213 = sext i32 %212 to i64
  %214 = getelementptr inbounds float, float* %7, i64 %213
  %215 = bitcast float* %214 to <8 x float>*
  %wide.load32.2 = load <8 x float>, <8 x float>* %215, align 4, !tbaa !12, !llvm.access.group !16
  %216 = getelementptr inbounds float, float* %7, i64 %208
  %217 = bitcast float* %216 to <8 x float>*
  %wide.load33.2 = load <8 x float>, <8 x float>* %217, align 4, !tbaa !12, !llvm.access.group !16
  %218 = fsub <8 x float> %wide.load32.2, %wide.load33.2
  %219 = add nsw i32 %mul14.i.i.1, %206
  %220 = sext i32 %219 to i64
  %221 = getelementptr inbounds float, float* %11, i64 %220
  %222 = bitcast float* %221 to <8 x float>*
  %wide.load34.2 = load <8 x float>, <8 x float>* %222, align 4, !tbaa !12, !llvm.access.group !16
  %223 = fadd <8 x float> %218, %wide.load34.2
  %224 = getelementptr inbounds float, float* %11, i64 %208
  %225 = bitcast float* %224 to <8 x float>*
  %wide.load35.2 = load <8 x float>, <8 x float>* %225, align 4, !tbaa !12, !llvm.access.group !16
  %226 = fsub <8 x float> %223, %wide.load35.2
  %227 = fpext <8 x float> %226 to <8 x double>
  %228 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %227, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %211)
  %229 = fptrunc <8 x double> %228 to <8 x float>
  %230 = bitcast float* %209 to <8 x float>*
  store <8 x float> %229, <8 x float>* %230, align 4, !tbaa !12, !llvm.access.group !16
  %231 = trunc i64 %mul.i.i.i to i32
  %232 = or i32 %231, 24
  %233 = add i32 %mul.i.i.1, %232
  %234 = sext i32 %233 to i64
  %235 = getelementptr inbounds float, float* %15, i64 %234
  %236 = bitcast float* %235 to <8 x float>*
  %wide.load31.3 = load <8 x float>, <8 x float>* %236, align 4, !tbaa !12, !llvm.access.group !16
  %237 = fpext <8 x float> %wide.load31.3 to <8 x double>
  %238 = add i32 %233, 1
  %239 = sext i32 %238 to i64
  %240 = getelementptr inbounds float, float* %7, i64 %239
  %241 = bitcast float* %240 to <8 x float>*
  %wide.load32.3 = load <8 x float>, <8 x float>* %241, align 4, !tbaa !12, !llvm.access.group !16
  %242 = getelementptr inbounds float, float* %7, i64 %234
  %243 = bitcast float* %242 to <8 x float>*
  %wide.load33.3 = load <8 x float>, <8 x float>* %243, align 4, !tbaa !12, !llvm.access.group !16
  %244 = fsub <8 x float> %wide.load32.3, %wide.load33.3
  %245 = add nsw i32 %mul14.i.i.1, %232
  %246 = sext i32 %245 to i64
  %247 = getelementptr inbounds float, float* %11, i64 %246
  %248 = bitcast float* %247 to <8 x float>*
  %wide.load34.3 = load <8 x float>, <8 x float>* %248, align 4, !tbaa !12, !llvm.access.group !16
  %249 = fadd <8 x float> %244, %wide.load34.3
  %250 = getelementptr inbounds float, float* %11, i64 %234
  %251 = bitcast float* %250 to <8 x float>*
  %wide.load35.3 = load <8 x float>, <8 x float>* %251, align 4, !tbaa !12, !llvm.access.group !16
  %252 = fsub <8 x float> %249, %wide.load35.3
  %253 = fpext <8 x float> %252 to <8 x double>
  %254 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %253, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %237)
  %255 = fptrunc <8 x double> %254 to <8 x float>
  %256 = bitcast float* %235 to <8 x float>*
  store <8 x float> %255, <8 x float>* %256, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.1

pregion_for_entry.entry.i.i:                      ; preds = %pregion_for_entry.entry.i.i, %pregion_for_entry.entry.i.i.preheader
  %_local_id_x.i.0 = phi i64 [ %270, %pregion_for_entry.entry.i.i ], [ 0, %pregion_for_entry.entry.i.i.preheader ]
  %add1.i.i.i = add nuw nsw i64 %_local_id_x.i.0, %mul.i.i.i
  %conv.i.i = trunc i64 %add1.i.i.i to i32
  %add.i.i = add i32 %mul.i.i, %conv.i.i
  %idxprom.i.i = sext i32 %add.i.i to i64
  %arrayidx.i.i = getelementptr inbounds float, float* %15, i64 %idxprom.i.i
  %257 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i = fpext float %257 to double
  %add6.i.i = or i32 %add.i.i, 1
  %idxprom7.i.i = sext i32 %add6.i.i to i64
  %arrayidx8.i.i = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i
  %258 = load float, float* %arrayidx8.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i = getelementptr inbounds float, float* %7, i64 %idxprom.i.i
  %259 = load float, float* %arrayidx12.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i = fsub float %258, %259
  %add15.i.i = add nsw i32 %mul14.i.i, %conv.i.i
  %idxprom16.i.i = sext i32 %add15.i.i to i64
  %arrayidx17.i.i = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i
  %260 = load float, float* %arrayidx17.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i = fadd float %sub.i.i, %260
  %arrayidx22.i.i = getelementptr inbounds float, float* %11, i64 %idxprom.i.i
  %261 = load float, float* %arrayidx22.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i = fsub float %add18.i.i, %261
  %conv24.i.i = fpext float %sub23.i.i to double
  %262 = tail call double @llvm.fmuladd.f64(double %conv24.i.i, double 0xBFE6666666666666, double %conv3.i.i) #3
  %conv26.i.i = fptrunc double %262 to float
  store float %conv26.i.i, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %263 = or i64 %_local_id_x.i.0, 1
  %add1.i.i.i.1189 = add nuw nsw i64 %263, %mul.i.i.i
  %conv.i.i.1190 = trunc i64 %add1.i.i.i.1189 to i32
  %add.i.i.1191 = add i32 %mul.i.i, %conv.i.i.1190
  %idxprom.i.i.1192 = sext i32 %add.i.i.1191 to i64
  %arrayidx.i.i.1193 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.1192
  %264 = load float, float* %arrayidx.i.i.1193, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.1194 = fpext float %264 to double
  %add6.i.i.1195 = add i32 %add.i.i.1191, 1
  %idxprom7.i.i.1196 = sext i32 %add6.i.i.1195 to i64
  %arrayidx8.i.i.1197 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.1196
  %265 = load float, float* %arrayidx8.i.i.1197, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.1198 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.1192
  %266 = load float, float* %arrayidx12.i.i.1198, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.1199 = fsub float %265, %266
  %add15.i.i.1200 = add nsw i32 %mul14.i.i, %conv.i.i.1190
  %idxprom16.i.i.1201 = sext i32 %add15.i.i.1200 to i64
  %arrayidx17.i.i.1202 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.1201
  %267 = load float, float* %arrayidx17.i.i.1202, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.1203 = fadd float %sub.i.i.1199, %267
  %arrayidx22.i.i.1204 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.1192
  %268 = load float, float* %arrayidx22.i.i.1204, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.1205 = fsub float %add18.i.i.1203, %268
  %conv24.i.i.1206 = fpext float %sub23.i.i.1205 to double
  %269 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.1206, double 0xBFE6666666666666, double %conv3.i.i.1194) #3
  %conv26.i.i.1207 = fptrunc double %269 to float
  store float %conv26.i.i.1207, float* %arrayidx.i.i.1193, align 4, !tbaa !12, !llvm.access.group !16
  %270 = add nuw nsw i64 %_local_id_x.i.0, 2
  %exitcond.not.1 = icmp eq i64 %270, 32
  br i1 %exitcond.not.1, label %pregion_for_end.i.i.loopexit, label %pregion_for_entry.entry.i.i, !llvm.loop !29

pregion_for_entry.entry.i.i.1:                    ; preds = %pregion_for_entry.entry.i.i.1, %pregion_for_entry.entry.i.i.1.preheader
  %_local_id_x.i.0.1 = phi i64 [ %284, %pregion_for_entry.entry.i.i.1 ], [ 0, %pregion_for_entry.entry.i.i.1.preheader ]
  %add1.i.i.i.1 = add nuw nsw i64 %_local_id_x.i.0.1, %mul.i.i.i
  %conv.i.i.1 = trunc i64 %add1.i.i.i.1 to i32
  %add.i.i.1 = add i32 %mul.i.i.1, %conv.i.i.1
  %idxprom.i.i.1 = sext i32 %add.i.i.1 to i64
  %arrayidx.i.i.1 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.1
  %271 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.1 = fpext float %271 to double
  %add6.i.i.1 = add i32 %add.i.i.1, 1
  %idxprom7.i.i.1 = sext i32 %add6.i.i.1 to i64
  %arrayidx8.i.i.1 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.1
  %272 = load float, float* %arrayidx8.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.1 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.1
  %273 = load float, float* %arrayidx12.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.1 = fsub float %272, %273
  %add15.i.i.1 = add nsw i32 %mul14.i.i.1, %conv.i.i.1
  %idxprom16.i.i.1 = sext i32 %add15.i.i.1 to i64
  %arrayidx17.i.i.1 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.1
  %274 = load float, float* %arrayidx17.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.1 = fadd float %sub.i.i.1, %274
  %arrayidx22.i.i.1 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.1
  %275 = load float, float* %arrayidx22.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.1 = fsub float %add18.i.i.1, %275
  %conv24.i.i.1 = fpext float %sub23.i.i.1 to double
  %276 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.1, double 0xBFE6666666666666, double %conv3.i.i.1) #3
  %conv26.i.i.1 = fptrunc double %276 to float
  store float %conv26.i.i.1, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %277 = or i64 %_local_id_x.i.0.1, 1
  %add1.i.i.i.1.1 = add nuw nsw i64 %277, %mul.i.i.i
  %conv.i.i.1.1 = trunc i64 %add1.i.i.i.1.1 to i32
  %add.i.i.1.1 = add i32 %mul.i.i.1, %conv.i.i.1.1
  %idxprom.i.i.1.1 = sext i32 %add.i.i.1.1 to i64
  %arrayidx.i.i.1.1 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.1.1
  %278 = load float, float* %arrayidx.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.1.1 = fpext float %278 to double
  %add6.i.i.1.1 = add i32 %add.i.i.1.1, 1
  %idxprom7.i.i.1.1 = sext i32 %add6.i.i.1.1 to i64
  %arrayidx8.i.i.1.1 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.1.1
  %279 = load float, float* %arrayidx8.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.1.1 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.1.1
  %280 = load float, float* %arrayidx12.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.1.1 = fsub float %279, %280
  %add15.i.i.1.1 = add nsw i32 %mul14.i.i.1, %conv.i.i.1.1
  %idxprom16.i.i.1.1 = sext i32 %add15.i.i.1.1 to i64
  %arrayidx17.i.i.1.1 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.1.1
  %281 = load float, float* %arrayidx17.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.1.1 = fadd float %sub.i.i.1.1, %281
  %arrayidx22.i.i.1.1 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.1.1
  %282 = load float, float* %arrayidx22.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.1.1 = fsub float %add18.i.i.1.1, %282
  %conv24.i.i.1.1 = fpext float %sub23.i.i.1.1 to double
  %283 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.1.1, double 0xBFE6666666666666, double %conv3.i.i.1.1) #3
  %conv26.i.i.1.1 = fptrunc double %283 to float
  store float %conv26.i.i.1.1, float* %arrayidx.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %284 = add nuw nsw i64 %_local_id_x.i.0.1, 2
  %exitcond.1.not.1 = icmp eq i64 %284, 32
  br i1 %exitcond.1.not.1, label %pregion_for_end.i.i.1.loopexit, label %pregion_for_entry.entry.i.i.1, !llvm.loop !30

pregion_for_end.i.i.1.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.1
  br label %pregion_for_end.i.i.1

pregion_for_end.i.i.1:                            ; preds = %pregion_for_end.i.i.1.loopexit, %vector.body14
  %285 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.2 = or i32 %285, 2
  %mul.i.i.2 = mul nsw i32 %19, %conv2.i.i.2
  %add13.i.i.2 = or i32 %285, 3
  %mul14.i.i.2 = mul nsw i32 %add13.i.i.2, %19
  %286 = mul i32 %19, %conv2.i.i.2
  %287 = trunc i64 %2 to i32
  %288 = shl i32 %287, 5
  %289 = add nsw i32 %286, %288
  %290 = icmp sgt i32 %289, 2147483616
  %291 = add i32 %286, %288
  %292 = or i32 %291, 1
  %293 = icmp sgt i32 %292, 2147483616
  %294 = or i1 %290, %293
  %295 = mul i32 %19, %add13.i.i.2
  %296 = add nsw i32 %295, %288
  %297 = icmp sgt i32 %296, 2147483616
  %298 = or i1 %294, %297
  br i1 %298, label %pregion_for_entry.entry.i.i.2.preheader, label %vector.body38

pregion_for_entry.entry.i.i.2.preheader:          ; preds = %pregion_for_end.i.i.1
  br label %pregion_for_entry.entry.i.i.2

vector.body38:                                    ; preds = %pregion_for_end.i.i.1
  %299 = trunc i64 %mul.i.i.i to i32
  %300 = add i32 %mul.i.i.2, %299
  %301 = sext i32 %300 to i64
  %302 = getelementptr inbounds float, float* %15, i64 %301
  %303 = bitcast float* %302 to <8 x float>*
  %wide.load55 = load <8 x float>, <8 x float>* %303, align 4, !tbaa !12, !llvm.access.group !16
  %304 = fpext <8 x float> %wide.load55 to <8 x double>
  %305 = or i32 %300, 1
  %306 = sext i32 %305 to i64
  %307 = getelementptr inbounds float, float* %7, i64 %306
  %308 = bitcast float* %307 to <8 x float>*
  %wide.load56 = load <8 x float>, <8 x float>* %308, align 4, !tbaa !12, !llvm.access.group !16
  %309 = getelementptr inbounds float, float* %7, i64 %301
  %310 = bitcast float* %309 to <8 x float>*
  %wide.load57 = load <8 x float>, <8 x float>* %310, align 4, !tbaa !12, !llvm.access.group !16
  %311 = fsub <8 x float> %wide.load56, %wide.load57
  %312 = add nsw i32 %mul14.i.i.2, %299
  %313 = sext i32 %312 to i64
  %314 = getelementptr inbounds float, float* %11, i64 %313
  %315 = bitcast float* %314 to <8 x float>*
  %wide.load58 = load <8 x float>, <8 x float>* %315, align 4, !tbaa !12, !llvm.access.group !16
  %316 = fadd <8 x float> %311, %wide.load58
  %317 = getelementptr inbounds float, float* %11, i64 %301
  %318 = bitcast float* %317 to <8 x float>*
  %wide.load59 = load <8 x float>, <8 x float>* %318, align 4, !tbaa !12, !llvm.access.group !16
  %319 = fsub <8 x float> %316, %wide.load59
  %320 = fpext <8 x float> %319 to <8 x double>
  %321 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %320, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %304)
  %322 = fptrunc <8 x double> %321 to <8 x float>
  %323 = bitcast float* %302 to <8 x float>*
  store <8 x float> %322, <8 x float>* %323, align 4, !tbaa !12, !llvm.access.group !16
  %324 = trunc i64 %mul.i.i.i to i32
  %325 = or i32 %324, 8
  %326 = add i32 %mul.i.i.2, %325
  %327 = sext i32 %326 to i64
  %328 = getelementptr inbounds float, float* %15, i64 %327
  %329 = bitcast float* %328 to <8 x float>*
  %wide.load55.1 = load <8 x float>, <8 x float>* %329, align 4, !tbaa !12, !llvm.access.group !16
  %330 = fpext <8 x float> %wide.load55.1 to <8 x double>
  %331 = or i32 %326, 1
  %332 = sext i32 %331 to i64
  %333 = getelementptr inbounds float, float* %7, i64 %332
  %334 = bitcast float* %333 to <8 x float>*
  %wide.load56.1 = load <8 x float>, <8 x float>* %334, align 4, !tbaa !12, !llvm.access.group !16
  %335 = getelementptr inbounds float, float* %7, i64 %327
  %336 = bitcast float* %335 to <8 x float>*
  %wide.load57.1 = load <8 x float>, <8 x float>* %336, align 4, !tbaa !12, !llvm.access.group !16
  %337 = fsub <8 x float> %wide.load56.1, %wide.load57.1
  %338 = add nsw i32 %mul14.i.i.2, %325
  %339 = sext i32 %338 to i64
  %340 = getelementptr inbounds float, float* %11, i64 %339
  %341 = bitcast float* %340 to <8 x float>*
  %wide.load58.1 = load <8 x float>, <8 x float>* %341, align 4, !tbaa !12, !llvm.access.group !16
  %342 = fadd <8 x float> %337, %wide.load58.1
  %343 = getelementptr inbounds float, float* %11, i64 %327
  %344 = bitcast float* %343 to <8 x float>*
  %wide.load59.1 = load <8 x float>, <8 x float>* %344, align 4, !tbaa !12, !llvm.access.group !16
  %345 = fsub <8 x float> %342, %wide.load59.1
  %346 = fpext <8 x float> %345 to <8 x double>
  %347 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %346, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %330)
  %348 = fptrunc <8 x double> %347 to <8 x float>
  %349 = bitcast float* %328 to <8 x float>*
  store <8 x float> %348, <8 x float>* %349, align 4, !tbaa !12, !llvm.access.group !16
  %350 = trunc i64 %mul.i.i.i to i32
  %351 = or i32 %350, 16
  %352 = add i32 %mul.i.i.2, %351
  %353 = sext i32 %352 to i64
  %354 = getelementptr inbounds float, float* %15, i64 %353
  %355 = bitcast float* %354 to <8 x float>*
  %wide.load55.2 = load <8 x float>, <8 x float>* %355, align 4, !tbaa !12, !llvm.access.group !16
  %356 = fpext <8 x float> %wide.load55.2 to <8 x double>
  %357 = or i32 %352, 1
  %358 = sext i32 %357 to i64
  %359 = getelementptr inbounds float, float* %7, i64 %358
  %360 = bitcast float* %359 to <8 x float>*
  %wide.load56.2 = load <8 x float>, <8 x float>* %360, align 4, !tbaa !12, !llvm.access.group !16
  %361 = getelementptr inbounds float, float* %7, i64 %353
  %362 = bitcast float* %361 to <8 x float>*
  %wide.load57.2 = load <8 x float>, <8 x float>* %362, align 4, !tbaa !12, !llvm.access.group !16
  %363 = fsub <8 x float> %wide.load56.2, %wide.load57.2
  %364 = add nsw i32 %mul14.i.i.2, %351
  %365 = sext i32 %364 to i64
  %366 = getelementptr inbounds float, float* %11, i64 %365
  %367 = bitcast float* %366 to <8 x float>*
  %wide.load58.2 = load <8 x float>, <8 x float>* %367, align 4, !tbaa !12, !llvm.access.group !16
  %368 = fadd <8 x float> %363, %wide.load58.2
  %369 = getelementptr inbounds float, float* %11, i64 %353
  %370 = bitcast float* %369 to <8 x float>*
  %wide.load59.2 = load <8 x float>, <8 x float>* %370, align 4, !tbaa !12, !llvm.access.group !16
  %371 = fsub <8 x float> %368, %wide.load59.2
  %372 = fpext <8 x float> %371 to <8 x double>
  %373 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %372, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %356)
  %374 = fptrunc <8 x double> %373 to <8 x float>
  %375 = bitcast float* %354 to <8 x float>*
  store <8 x float> %374, <8 x float>* %375, align 4, !tbaa !12, !llvm.access.group !16
  %376 = trunc i64 %mul.i.i.i to i32
  %377 = or i32 %376, 24
  %378 = add i32 %mul.i.i.2, %377
  %379 = sext i32 %378 to i64
  %380 = getelementptr inbounds float, float* %15, i64 %379
  %381 = bitcast float* %380 to <8 x float>*
  %wide.load55.3 = load <8 x float>, <8 x float>* %381, align 4, !tbaa !12, !llvm.access.group !16
  %382 = fpext <8 x float> %wide.load55.3 to <8 x double>
  %383 = or i32 %378, 1
  %384 = sext i32 %383 to i64
  %385 = getelementptr inbounds float, float* %7, i64 %384
  %386 = bitcast float* %385 to <8 x float>*
  %wide.load56.3 = load <8 x float>, <8 x float>* %386, align 4, !tbaa !12, !llvm.access.group !16
  %387 = getelementptr inbounds float, float* %7, i64 %379
  %388 = bitcast float* %387 to <8 x float>*
  %wide.load57.3 = load <8 x float>, <8 x float>* %388, align 4, !tbaa !12, !llvm.access.group !16
  %389 = fsub <8 x float> %wide.load56.3, %wide.load57.3
  %390 = add nsw i32 %mul14.i.i.2, %377
  %391 = sext i32 %390 to i64
  %392 = getelementptr inbounds float, float* %11, i64 %391
  %393 = bitcast float* %392 to <8 x float>*
  %wide.load58.3 = load <8 x float>, <8 x float>* %393, align 4, !tbaa !12, !llvm.access.group !16
  %394 = fadd <8 x float> %389, %wide.load58.3
  %395 = getelementptr inbounds float, float* %11, i64 %379
  %396 = bitcast float* %395 to <8 x float>*
  %wide.load59.3 = load <8 x float>, <8 x float>* %396, align 4, !tbaa !12, !llvm.access.group !16
  %397 = fsub <8 x float> %394, %wide.load59.3
  %398 = fpext <8 x float> %397 to <8 x double>
  %399 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %398, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %382)
  %400 = fptrunc <8 x double> %399 to <8 x float>
  %401 = bitcast float* %380 to <8 x float>*
  store <8 x float> %400, <8 x float>* %401, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.2

pregion_for_entry.entry.i.i.2:                    ; preds = %pregion_for_entry.entry.i.i.2, %pregion_for_entry.entry.i.i.2.preheader
  %_local_id_x.i.0.2 = phi i64 [ %415, %pregion_for_entry.entry.i.i.2 ], [ 0, %pregion_for_entry.entry.i.i.2.preheader ]
  %add1.i.i.i.2 = add nuw nsw i64 %_local_id_x.i.0.2, %mul.i.i.i
  %conv.i.i.2 = trunc i64 %add1.i.i.i.2 to i32
  %add.i.i.2 = add i32 %mul.i.i.2, %conv.i.i.2
  %idxprom.i.i.2 = sext i32 %add.i.i.2 to i64
  %arrayidx.i.i.2 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.2
  %402 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.2 = fpext float %402 to double
  %add6.i.i.2 = or i32 %add.i.i.2, 1
  %idxprom7.i.i.2 = sext i32 %add6.i.i.2 to i64
  %arrayidx8.i.i.2 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.2
  %403 = load float, float* %arrayidx8.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.2 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.2
  %404 = load float, float* %arrayidx12.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.2 = fsub float %403, %404
  %add15.i.i.2 = add nsw i32 %mul14.i.i.2, %conv.i.i.2
  %idxprom16.i.i.2 = sext i32 %add15.i.i.2 to i64
  %arrayidx17.i.i.2 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.2
  %405 = load float, float* %arrayidx17.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.2 = fadd float %sub.i.i.2, %405
  %arrayidx22.i.i.2 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.2
  %406 = load float, float* %arrayidx22.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.2 = fsub float %add18.i.i.2, %406
  %conv24.i.i.2 = fpext float %sub23.i.i.2 to double
  %407 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.2, double 0xBFE6666666666666, double %conv3.i.i.2) #3
  %conv26.i.i.2 = fptrunc double %407 to float
  store float %conv26.i.i.2, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %408 = or i64 %_local_id_x.i.0.2, 1
  %add1.i.i.i.2.1 = add nuw nsw i64 %408, %mul.i.i.i
  %conv.i.i.2.1 = trunc i64 %add1.i.i.i.2.1 to i32
  %add.i.i.2.1 = add i32 %mul.i.i.2, %conv.i.i.2.1
  %idxprom.i.i.2.1 = sext i32 %add.i.i.2.1 to i64
  %arrayidx.i.i.2.1 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.2.1
  %409 = load float, float* %arrayidx.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.2.1 = fpext float %409 to double
  %add6.i.i.2.1 = add i32 %add.i.i.2.1, 1
  %idxprom7.i.i.2.1 = sext i32 %add6.i.i.2.1 to i64
  %arrayidx8.i.i.2.1 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.2.1
  %410 = load float, float* %arrayidx8.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.2.1 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.2.1
  %411 = load float, float* %arrayidx12.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.2.1 = fsub float %410, %411
  %add15.i.i.2.1 = add nsw i32 %mul14.i.i.2, %conv.i.i.2.1
  %idxprom16.i.i.2.1 = sext i32 %add15.i.i.2.1 to i64
  %arrayidx17.i.i.2.1 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.2.1
  %412 = load float, float* %arrayidx17.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.2.1 = fadd float %sub.i.i.2.1, %412
  %arrayidx22.i.i.2.1 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.2.1
  %413 = load float, float* %arrayidx22.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.2.1 = fsub float %add18.i.i.2.1, %413
  %conv24.i.i.2.1 = fpext float %sub23.i.i.2.1 to double
  %414 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.2.1, double 0xBFE6666666666666, double %conv3.i.i.2.1) #3
  %conv26.i.i.2.1 = fptrunc double %414 to float
  store float %conv26.i.i.2.1, float* %arrayidx.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %415 = add nuw nsw i64 %_local_id_x.i.0.2, 2
  %exitcond.2.not.1 = icmp eq i64 %415, 32
  br i1 %exitcond.2.not.1, label %pregion_for_end.i.i.2.loopexit, label %pregion_for_entry.entry.i.i.2, !llvm.loop !31

pregion_for_end.i.i.2.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.2
  br label %pregion_for_end.i.i.2

pregion_for_end.i.i.2:                            ; preds = %pregion_for_end.i.i.2.loopexit, %vector.body38
  %416 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.3 = or i32 %416, 3
  %mul.i.i.3 = mul nsw i32 %19, %conv2.i.i.3
  %add13.i.i.3 = add nuw nsw i32 %conv2.i.i.3, 1
  %mul14.i.i.3 = mul nsw i32 %add13.i.i.3, %19
  %417 = mul i32 %19, %add13.i.i.2
  %418 = trunc i64 %2 to i32
  %419 = shl i32 %418, 5
  %420 = add nsw i32 %417, %419
  %421 = icmp sgt i32 %420, 2147483616
  %422 = add i32 %417, %419
  %423 = add i32 %422, 1
  %424 = add i32 %422, 32
  %425 = icmp slt i32 %424, %423
  %426 = or i1 %421, %425
  %427 = or i32 %conv2.i.i, 4
  %428 = mul i32 %19, %427
  %429 = add nsw i32 %428, %419
  %430 = icmp sgt i32 %429, 2147483616
  %431 = or i1 %426, %430
  br i1 %431, label %pregion_for_entry.entry.i.i.3.preheader, label %vector.body62

pregion_for_entry.entry.i.i.3.preheader:          ; preds = %pregion_for_end.i.i.2
  br label %pregion_for_entry.entry.i.i.3

vector.body62:                                    ; preds = %pregion_for_end.i.i.2
  %432 = trunc i64 %mul.i.i.i to i32
  %433 = add i32 %mul.i.i.3, %432
  %434 = sext i32 %433 to i64
  %435 = getelementptr inbounds float, float* %15, i64 %434
  %436 = bitcast float* %435 to <8 x float>*
  %wide.load79 = load <8 x float>, <8 x float>* %436, align 4, !tbaa !12, !llvm.access.group !16
  %437 = fpext <8 x float> %wide.load79 to <8 x double>
  %438 = add i32 %433, 1
  %439 = sext i32 %438 to i64
  %440 = getelementptr inbounds float, float* %7, i64 %439
  %441 = bitcast float* %440 to <8 x float>*
  %wide.load80 = load <8 x float>, <8 x float>* %441, align 4, !tbaa !12, !llvm.access.group !16
  %442 = getelementptr inbounds float, float* %7, i64 %434
  %443 = bitcast float* %442 to <8 x float>*
  %wide.load81 = load <8 x float>, <8 x float>* %443, align 4, !tbaa !12, !llvm.access.group !16
  %444 = fsub <8 x float> %wide.load80, %wide.load81
  %445 = add nsw i32 %mul14.i.i.3, %432
  %446 = sext i32 %445 to i64
  %447 = getelementptr inbounds float, float* %11, i64 %446
  %448 = bitcast float* %447 to <8 x float>*
  %wide.load82 = load <8 x float>, <8 x float>* %448, align 4, !tbaa !12, !llvm.access.group !16
  %449 = fadd <8 x float> %444, %wide.load82
  %450 = getelementptr inbounds float, float* %11, i64 %434
  %451 = bitcast float* %450 to <8 x float>*
  %wide.load83 = load <8 x float>, <8 x float>* %451, align 4, !tbaa !12, !llvm.access.group !16
  %452 = fsub <8 x float> %449, %wide.load83
  %453 = fpext <8 x float> %452 to <8 x double>
  %454 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %453, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %437)
  %455 = fptrunc <8 x double> %454 to <8 x float>
  %456 = bitcast float* %435 to <8 x float>*
  store <8 x float> %455, <8 x float>* %456, align 4, !tbaa !12, !llvm.access.group !16
  %457 = trunc i64 %mul.i.i.i to i32
  %458 = or i32 %457, 8
  %459 = add i32 %mul.i.i.3, %458
  %460 = sext i32 %459 to i64
  %461 = getelementptr inbounds float, float* %15, i64 %460
  %462 = bitcast float* %461 to <8 x float>*
  %wide.load79.1 = load <8 x float>, <8 x float>* %462, align 4, !tbaa !12, !llvm.access.group !16
  %463 = fpext <8 x float> %wide.load79.1 to <8 x double>
  %464 = add i32 %459, 1
  %465 = sext i32 %464 to i64
  %466 = getelementptr inbounds float, float* %7, i64 %465
  %467 = bitcast float* %466 to <8 x float>*
  %wide.load80.1 = load <8 x float>, <8 x float>* %467, align 4, !tbaa !12, !llvm.access.group !16
  %468 = getelementptr inbounds float, float* %7, i64 %460
  %469 = bitcast float* %468 to <8 x float>*
  %wide.load81.1 = load <8 x float>, <8 x float>* %469, align 4, !tbaa !12, !llvm.access.group !16
  %470 = fsub <8 x float> %wide.load80.1, %wide.load81.1
  %471 = add nsw i32 %mul14.i.i.3, %458
  %472 = sext i32 %471 to i64
  %473 = getelementptr inbounds float, float* %11, i64 %472
  %474 = bitcast float* %473 to <8 x float>*
  %wide.load82.1 = load <8 x float>, <8 x float>* %474, align 4, !tbaa !12, !llvm.access.group !16
  %475 = fadd <8 x float> %470, %wide.load82.1
  %476 = getelementptr inbounds float, float* %11, i64 %460
  %477 = bitcast float* %476 to <8 x float>*
  %wide.load83.1 = load <8 x float>, <8 x float>* %477, align 4, !tbaa !12, !llvm.access.group !16
  %478 = fsub <8 x float> %475, %wide.load83.1
  %479 = fpext <8 x float> %478 to <8 x double>
  %480 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %479, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %463)
  %481 = fptrunc <8 x double> %480 to <8 x float>
  %482 = bitcast float* %461 to <8 x float>*
  store <8 x float> %481, <8 x float>* %482, align 4, !tbaa !12, !llvm.access.group !16
  %483 = trunc i64 %mul.i.i.i to i32
  %484 = or i32 %483, 16
  %485 = add i32 %mul.i.i.3, %484
  %486 = sext i32 %485 to i64
  %487 = getelementptr inbounds float, float* %15, i64 %486
  %488 = bitcast float* %487 to <8 x float>*
  %wide.load79.2 = load <8 x float>, <8 x float>* %488, align 4, !tbaa !12, !llvm.access.group !16
  %489 = fpext <8 x float> %wide.load79.2 to <8 x double>
  %490 = add i32 %485, 1
  %491 = sext i32 %490 to i64
  %492 = getelementptr inbounds float, float* %7, i64 %491
  %493 = bitcast float* %492 to <8 x float>*
  %wide.load80.2 = load <8 x float>, <8 x float>* %493, align 4, !tbaa !12, !llvm.access.group !16
  %494 = getelementptr inbounds float, float* %7, i64 %486
  %495 = bitcast float* %494 to <8 x float>*
  %wide.load81.2 = load <8 x float>, <8 x float>* %495, align 4, !tbaa !12, !llvm.access.group !16
  %496 = fsub <8 x float> %wide.load80.2, %wide.load81.2
  %497 = add nsw i32 %mul14.i.i.3, %484
  %498 = sext i32 %497 to i64
  %499 = getelementptr inbounds float, float* %11, i64 %498
  %500 = bitcast float* %499 to <8 x float>*
  %wide.load82.2 = load <8 x float>, <8 x float>* %500, align 4, !tbaa !12, !llvm.access.group !16
  %501 = fadd <8 x float> %496, %wide.load82.2
  %502 = getelementptr inbounds float, float* %11, i64 %486
  %503 = bitcast float* %502 to <8 x float>*
  %wide.load83.2 = load <8 x float>, <8 x float>* %503, align 4, !tbaa !12, !llvm.access.group !16
  %504 = fsub <8 x float> %501, %wide.load83.2
  %505 = fpext <8 x float> %504 to <8 x double>
  %506 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %505, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %489)
  %507 = fptrunc <8 x double> %506 to <8 x float>
  %508 = bitcast float* %487 to <8 x float>*
  store <8 x float> %507, <8 x float>* %508, align 4, !tbaa !12, !llvm.access.group !16
  %509 = trunc i64 %mul.i.i.i to i32
  %510 = or i32 %509, 24
  %511 = add i32 %mul.i.i.3, %510
  %512 = sext i32 %511 to i64
  %513 = getelementptr inbounds float, float* %15, i64 %512
  %514 = bitcast float* %513 to <8 x float>*
  %wide.load79.3 = load <8 x float>, <8 x float>* %514, align 4, !tbaa !12, !llvm.access.group !16
  %515 = fpext <8 x float> %wide.load79.3 to <8 x double>
  %516 = add i32 %511, 1
  %517 = sext i32 %516 to i64
  %518 = getelementptr inbounds float, float* %7, i64 %517
  %519 = bitcast float* %518 to <8 x float>*
  %wide.load80.3 = load <8 x float>, <8 x float>* %519, align 4, !tbaa !12, !llvm.access.group !16
  %520 = getelementptr inbounds float, float* %7, i64 %512
  %521 = bitcast float* %520 to <8 x float>*
  %wide.load81.3 = load <8 x float>, <8 x float>* %521, align 4, !tbaa !12, !llvm.access.group !16
  %522 = fsub <8 x float> %wide.load80.3, %wide.load81.3
  %523 = add nsw i32 %mul14.i.i.3, %510
  %524 = sext i32 %523 to i64
  %525 = getelementptr inbounds float, float* %11, i64 %524
  %526 = bitcast float* %525 to <8 x float>*
  %wide.load82.3 = load <8 x float>, <8 x float>* %526, align 4, !tbaa !12, !llvm.access.group !16
  %527 = fadd <8 x float> %522, %wide.load82.3
  %528 = getelementptr inbounds float, float* %11, i64 %512
  %529 = bitcast float* %528 to <8 x float>*
  %wide.load83.3 = load <8 x float>, <8 x float>* %529, align 4, !tbaa !12, !llvm.access.group !16
  %530 = fsub <8 x float> %527, %wide.load83.3
  %531 = fpext <8 x float> %530 to <8 x double>
  %532 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %531, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %515)
  %533 = fptrunc <8 x double> %532 to <8 x float>
  %534 = bitcast float* %513 to <8 x float>*
  store <8 x float> %533, <8 x float>* %534, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.3

pregion_for_entry.entry.i.i.3:                    ; preds = %pregion_for_entry.entry.i.i.3, %pregion_for_entry.entry.i.i.3.preheader
  %_local_id_x.i.0.3 = phi i64 [ %548, %pregion_for_entry.entry.i.i.3 ], [ 0, %pregion_for_entry.entry.i.i.3.preheader ]
  %add1.i.i.i.3 = add nuw nsw i64 %_local_id_x.i.0.3, %mul.i.i.i
  %conv.i.i.3 = trunc i64 %add1.i.i.i.3 to i32
  %add.i.i.3 = add i32 %mul.i.i.3, %conv.i.i.3
  %idxprom.i.i.3 = sext i32 %add.i.i.3 to i64
  %arrayidx.i.i.3 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.3
  %535 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.3 = fpext float %535 to double
  %add6.i.i.3 = add i32 %add.i.i.3, 1
  %idxprom7.i.i.3 = sext i32 %add6.i.i.3 to i64
  %arrayidx8.i.i.3 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.3
  %536 = load float, float* %arrayidx8.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.3 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.3
  %537 = load float, float* %arrayidx12.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.3 = fsub float %536, %537
  %add15.i.i.3 = add nsw i32 %mul14.i.i.3, %conv.i.i.3
  %idxprom16.i.i.3 = sext i32 %add15.i.i.3 to i64
  %arrayidx17.i.i.3 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.3
  %538 = load float, float* %arrayidx17.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.3 = fadd float %sub.i.i.3, %538
  %arrayidx22.i.i.3 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.3
  %539 = load float, float* %arrayidx22.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.3 = fsub float %add18.i.i.3, %539
  %conv24.i.i.3 = fpext float %sub23.i.i.3 to double
  %540 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.3, double 0xBFE6666666666666, double %conv3.i.i.3) #3
  %conv26.i.i.3 = fptrunc double %540 to float
  store float %conv26.i.i.3, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %541 = or i64 %_local_id_x.i.0.3, 1
  %add1.i.i.i.3.1 = add nuw nsw i64 %541, %mul.i.i.i
  %conv.i.i.3.1 = trunc i64 %add1.i.i.i.3.1 to i32
  %add.i.i.3.1 = add i32 %mul.i.i.3, %conv.i.i.3.1
  %idxprom.i.i.3.1 = sext i32 %add.i.i.3.1 to i64
  %arrayidx.i.i.3.1 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.3.1
  %542 = load float, float* %arrayidx.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.3.1 = fpext float %542 to double
  %add6.i.i.3.1 = add i32 %add.i.i.3.1, 1
  %idxprom7.i.i.3.1 = sext i32 %add6.i.i.3.1 to i64
  %arrayidx8.i.i.3.1 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.3.1
  %543 = load float, float* %arrayidx8.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.3.1 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.3.1
  %544 = load float, float* %arrayidx12.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.3.1 = fsub float %543, %544
  %add15.i.i.3.1 = add nsw i32 %mul14.i.i.3, %conv.i.i.3.1
  %idxprom16.i.i.3.1 = sext i32 %add15.i.i.3.1 to i64
  %arrayidx17.i.i.3.1 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.3.1
  %545 = load float, float* %arrayidx17.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.3.1 = fadd float %sub.i.i.3.1, %545
  %arrayidx22.i.i.3.1 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.3.1
  %546 = load float, float* %arrayidx22.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.3.1 = fsub float %add18.i.i.3.1, %546
  %conv24.i.i.3.1 = fpext float %sub23.i.i.3.1 to double
  %547 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.3.1, double 0xBFE6666666666666, double %conv3.i.i.3.1) #3
  %conv26.i.i.3.1 = fptrunc double %547 to float
  store float %conv26.i.i.3.1, float* %arrayidx.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %548 = add nuw nsw i64 %_local_id_x.i.0.3, 2
  %exitcond.3.not.1 = icmp eq i64 %548, 32
  br i1 %exitcond.3.not.1, label %pregion_for_end.i.i.3.loopexit, label %pregion_for_entry.entry.i.i.3, !llvm.loop !32

pregion_for_end.i.i.3.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.3
  br label %pregion_for_end.i.i.3

pregion_for_end.i.i.3:                            ; preds = %pregion_for_end.i.i.3.loopexit, %vector.body62
  %549 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.4 = or i32 %549, 4
  %mul.i.i.4 = mul nsw i32 %19, %conv2.i.i.4
  %add13.i.i.4 = or i32 %549, 5
  %mul14.i.i.4 = mul nsw i32 %add13.i.i.4, %19
  %550 = mul i32 %19, %conv2.i.i.4
  %551 = trunc i64 %2 to i32
  %552 = shl i32 %551, 5
  %553 = add nsw i32 %550, %552
  %554 = icmp sgt i32 %553, 2147483616
  %555 = add i32 %550, %552
  %556 = or i32 %555, 1
  %557 = icmp sgt i32 %556, 2147483616
  %558 = or i1 %554, %557
  %559 = mul i32 %19, %add13.i.i.4
  %560 = add nsw i32 %559, %552
  %561 = icmp sgt i32 %560, 2147483616
  %562 = or i1 %558, %561
  br i1 %562, label %pregion_for_entry.entry.i.i.4.preheader, label %vector.body86

pregion_for_entry.entry.i.i.4.preheader:          ; preds = %pregion_for_end.i.i.3
  br label %pregion_for_entry.entry.i.i.4

vector.body86:                                    ; preds = %pregion_for_end.i.i.3
  %563 = trunc i64 %mul.i.i.i to i32
  %564 = add i32 %mul.i.i.4, %563
  %565 = sext i32 %564 to i64
  %566 = getelementptr inbounds float, float* %15, i64 %565
  %567 = bitcast float* %566 to <8 x float>*
  %wide.load103 = load <8 x float>, <8 x float>* %567, align 4, !tbaa !12, !llvm.access.group !16
  %568 = fpext <8 x float> %wide.load103 to <8 x double>
  %569 = or i32 %564, 1
  %570 = sext i32 %569 to i64
  %571 = getelementptr inbounds float, float* %7, i64 %570
  %572 = bitcast float* %571 to <8 x float>*
  %wide.load104 = load <8 x float>, <8 x float>* %572, align 4, !tbaa !12, !llvm.access.group !16
  %573 = getelementptr inbounds float, float* %7, i64 %565
  %574 = bitcast float* %573 to <8 x float>*
  %wide.load105 = load <8 x float>, <8 x float>* %574, align 4, !tbaa !12, !llvm.access.group !16
  %575 = fsub <8 x float> %wide.load104, %wide.load105
  %576 = add nsw i32 %mul14.i.i.4, %563
  %577 = sext i32 %576 to i64
  %578 = getelementptr inbounds float, float* %11, i64 %577
  %579 = bitcast float* %578 to <8 x float>*
  %wide.load106 = load <8 x float>, <8 x float>* %579, align 4, !tbaa !12, !llvm.access.group !16
  %580 = fadd <8 x float> %575, %wide.load106
  %581 = getelementptr inbounds float, float* %11, i64 %565
  %582 = bitcast float* %581 to <8 x float>*
  %wide.load107 = load <8 x float>, <8 x float>* %582, align 4, !tbaa !12, !llvm.access.group !16
  %583 = fsub <8 x float> %580, %wide.load107
  %584 = fpext <8 x float> %583 to <8 x double>
  %585 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %584, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %568)
  %586 = fptrunc <8 x double> %585 to <8 x float>
  %587 = bitcast float* %566 to <8 x float>*
  store <8 x float> %586, <8 x float>* %587, align 4, !tbaa !12, !llvm.access.group !16
  %588 = trunc i64 %mul.i.i.i to i32
  %589 = or i32 %588, 8
  %590 = add i32 %mul.i.i.4, %589
  %591 = sext i32 %590 to i64
  %592 = getelementptr inbounds float, float* %15, i64 %591
  %593 = bitcast float* %592 to <8 x float>*
  %wide.load103.1 = load <8 x float>, <8 x float>* %593, align 4, !tbaa !12, !llvm.access.group !16
  %594 = fpext <8 x float> %wide.load103.1 to <8 x double>
  %595 = or i32 %590, 1
  %596 = sext i32 %595 to i64
  %597 = getelementptr inbounds float, float* %7, i64 %596
  %598 = bitcast float* %597 to <8 x float>*
  %wide.load104.1 = load <8 x float>, <8 x float>* %598, align 4, !tbaa !12, !llvm.access.group !16
  %599 = getelementptr inbounds float, float* %7, i64 %591
  %600 = bitcast float* %599 to <8 x float>*
  %wide.load105.1 = load <8 x float>, <8 x float>* %600, align 4, !tbaa !12, !llvm.access.group !16
  %601 = fsub <8 x float> %wide.load104.1, %wide.load105.1
  %602 = add nsw i32 %mul14.i.i.4, %589
  %603 = sext i32 %602 to i64
  %604 = getelementptr inbounds float, float* %11, i64 %603
  %605 = bitcast float* %604 to <8 x float>*
  %wide.load106.1 = load <8 x float>, <8 x float>* %605, align 4, !tbaa !12, !llvm.access.group !16
  %606 = fadd <8 x float> %601, %wide.load106.1
  %607 = getelementptr inbounds float, float* %11, i64 %591
  %608 = bitcast float* %607 to <8 x float>*
  %wide.load107.1 = load <8 x float>, <8 x float>* %608, align 4, !tbaa !12, !llvm.access.group !16
  %609 = fsub <8 x float> %606, %wide.load107.1
  %610 = fpext <8 x float> %609 to <8 x double>
  %611 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %610, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %594)
  %612 = fptrunc <8 x double> %611 to <8 x float>
  %613 = bitcast float* %592 to <8 x float>*
  store <8 x float> %612, <8 x float>* %613, align 4, !tbaa !12, !llvm.access.group !16
  %614 = trunc i64 %mul.i.i.i to i32
  %615 = or i32 %614, 16
  %616 = add i32 %mul.i.i.4, %615
  %617 = sext i32 %616 to i64
  %618 = getelementptr inbounds float, float* %15, i64 %617
  %619 = bitcast float* %618 to <8 x float>*
  %wide.load103.2 = load <8 x float>, <8 x float>* %619, align 4, !tbaa !12, !llvm.access.group !16
  %620 = fpext <8 x float> %wide.load103.2 to <8 x double>
  %621 = or i32 %616, 1
  %622 = sext i32 %621 to i64
  %623 = getelementptr inbounds float, float* %7, i64 %622
  %624 = bitcast float* %623 to <8 x float>*
  %wide.load104.2 = load <8 x float>, <8 x float>* %624, align 4, !tbaa !12, !llvm.access.group !16
  %625 = getelementptr inbounds float, float* %7, i64 %617
  %626 = bitcast float* %625 to <8 x float>*
  %wide.load105.2 = load <8 x float>, <8 x float>* %626, align 4, !tbaa !12, !llvm.access.group !16
  %627 = fsub <8 x float> %wide.load104.2, %wide.load105.2
  %628 = add nsw i32 %mul14.i.i.4, %615
  %629 = sext i32 %628 to i64
  %630 = getelementptr inbounds float, float* %11, i64 %629
  %631 = bitcast float* %630 to <8 x float>*
  %wide.load106.2 = load <8 x float>, <8 x float>* %631, align 4, !tbaa !12, !llvm.access.group !16
  %632 = fadd <8 x float> %627, %wide.load106.2
  %633 = getelementptr inbounds float, float* %11, i64 %617
  %634 = bitcast float* %633 to <8 x float>*
  %wide.load107.2 = load <8 x float>, <8 x float>* %634, align 4, !tbaa !12, !llvm.access.group !16
  %635 = fsub <8 x float> %632, %wide.load107.2
  %636 = fpext <8 x float> %635 to <8 x double>
  %637 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %636, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %620)
  %638 = fptrunc <8 x double> %637 to <8 x float>
  %639 = bitcast float* %618 to <8 x float>*
  store <8 x float> %638, <8 x float>* %639, align 4, !tbaa !12, !llvm.access.group !16
  %640 = trunc i64 %mul.i.i.i to i32
  %641 = or i32 %640, 24
  %642 = add i32 %mul.i.i.4, %641
  %643 = sext i32 %642 to i64
  %644 = getelementptr inbounds float, float* %15, i64 %643
  %645 = bitcast float* %644 to <8 x float>*
  %wide.load103.3 = load <8 x float>, <8 x float>* %645, align 4, !tbaa !12, !llvm.access.group !16
  %646 = fpext <8 x float> %wide.load103.3 to <8 x double>
  %647 = or i32 %642, 1
  %648 = sext i32 %647 to i64
  %649 = getelementptr inbounds float, float* %7, i64 %648
  %650 = bitcast float* %649 to <8 x float>*
  %wide.load104.3 = load <8 x float>, <8 x float>* %650, align 4, !tbaa !12, !llvm.access.group !16
  %651 = getelementptr inbounds float, float* %7, i64 %643
  %652 = bitcast float* %651 to <8 x float>*
  %wide.load105.3 = load <8 x float>, <8 x float>* %652, align 4, !tbaa !12, !llvm.access.group !16
  %653 = fsub <8 x float> %wide.load104.3, %wide.load105.3
  %654 = add nsw i32 %mul14.i.i.4, %641
  %655 = sext i32 %654 to i64
  %656 = getelementptr inbounds float, float* %11, i64 %655
  %657 = bitcast float* %656 to <8 x float>*
  %wide.load106.3 = load <8 x float>, <8 x float>* %657, align 4, !tbaa !12, !llvm.access.group !16
  %658 = fadd <8 x float> %653, %wide.load106.3
  %659 = getelementptr inbounds float, float* %11, i64 %643
  %660 = bitcast float* %659 to <8 x float>*
  %wide.load107.3 = load <8 x float>, <8 x float>* %660, align 4, !tbaa !12, !llvm.access.group !16
  %661 = fsub <8 x float> %658, %wide.load107.3
  %662 = fpext <8 x float> %661 to <8 x double>
  %663 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %662, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %646)
  %664 = fptrunc <8 x double> %663 to <8 x float>
  %665 = bitcast float* %644 to <8 x float>*
  store <8 x float> %664, <8 x float>* %665, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.4

pregion_for_entry.entry.i.i.4:                    ; preds = %pregion_for_entry.entry.i.i.4, %pregion_for_entry.entry.i.i.4.preheader
  %_local_id_x.i.0.4 = phi i64 [ %679, %pregion_for_entry.entry.i.i.4 ], [ 0, %pregion_for_entry.entry.i.i.4.preheader ]
  %add1.i.i.i.4 = add nuw nsw i64 %_local_id_x.i.0.4, %mul.i.i.i
  %conv.i.i.4 = trunc i64 %add1.i.i.i.4 to i32
  %add.i.i.4 = add i32 %mul.i.i.4, %conv.i.i.4
  %idxprom.i.i.4 = sext i32 %add.i.i.4 to i64
  %arrayidx.i.i.4 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.4
  %666 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.4 = fpext float %666 to double
  %add6.i.i.4 = or i32 %add.i.i.4, 1
  %idxprom7.i.i.4 = sext i32 %add6.i.i.4 to i64
  %arrayidx8.i.i.4 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.4
  %667 = load float, float* %arrayidx8.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.4 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.4
  %668 = load float, float* %arrayidx12.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.4 = fsub float %667, %668
  %add15.i.i.4 = add nsw i32 %mul14.i.i.4, %conv.i.i.4
  %idxprom16.i.i.4 = sext i32 %add15.i.i.4 to i64
  %arrayidx17.i.i.4 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.4
  %669 = load float, float* %arrayidx17.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.4 = fadd float %sub.i.i.4, %669
  %arrayidx22.i.i.4 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.4
  %670 = load float, float* %arrayidx22.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.4 = fsub float %add18.i.i.4, %670
  %conv24.i.i.4 = fpext float %sub23.i.i.4 to double
  %671 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.4, double 0xBFE6666666666666, double %conv3.i.i.4) #3
  %conv26.i.i.4 = fptrunc double %671 to float
  store float %conv26.i.i.4, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %672 = or i64 %_local_id_x.i.0.4, 1
  %add1.i.i.i.4.1 = add nuw nsw i64 %672, %mul.i.i.i
  %conv.i.i.4.1 = trunc i64 %add1.i.i.i.4.1 to i32
  %add.i.i.4.1 = add i32 %mul.i.i.4, %conv.i.i.4.1
  %idxprom.i.i.4.1 = sext i32 %add.i.i.4.1 to i64
  %arrayidx.i.i.4.1 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.4.1
  %673 = load float, float* %arrayidx.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.4.1 = fpext float %673 to double
  %add6.i.i.4.1 = add i32 %add.i.i.4.1, 1
  %idxprom7.i.i.4.1 = sext i32 %add6.i.i.4.1 to i64
  %arrayidx8.i.i.4.1 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.4.1
  %674 = load float, float* %arrayidx8.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.4.1 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.4.1
  %675 = load float, float* %arrayidx12.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.4.1 = fsub float %674, %675
  %add15.i.i.4.1 = add nsw i32 %mul14.i.i.4, %conv.i.i.4.1
  %idxprom16.i.i.4.1 = sext i32 %add15.i.i.4.1 to i64
  %arrayidx17.i.i.4.1 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.4.1
  %676 = load float, float* %arrayidx17.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.4.1 = fadd float %sub.i.i.4.1, %676
  %arrayidx22.i.i.4.1 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.4.1
  %677 = load float, float* %arrayidx22.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.4.1 = fsub float %add18.i.i.4.1, %677
  %conv24.i.i.4.1 = fpext float %sub23.i.i.4.1 to double
  %678 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.4.1, double 0xBFE6666666666666, double %conv3.i.i.4.1) #3
  %conv26.i.i.4.1 = fptrunc double %678 to float
  store float %conv26.i.i.4.1, float* %arrayidx.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %679 = add nuw nsw i64 %_local_id_x.i.0.4, 2
  %exitcond.4.not.1 = icmp eq i64 %679, 32
  br i1 %exitcond.4.not.1, label %pregion_for_end.i.i.4.loopexit, label %pregion_for_entry.entry.i.i.4, !llvm.loop !33

pregion_for_end.i.i.4.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.4
  br label %pregion_for_end.i.i.4

pregion_for_end.i.i.4:                            ; preds = %pregion_for_end.i.i.4.loopexit, %vector.body86
  %680 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.5 = or i32 %680, 5
  %mul.i.i.5 = mul nsw i32 %19, %conv2.i.i.5
  %add13.i.i.5 = add nuw nsw i32 %conv2.i.i.5, 1
  %mul14.i.i.5 = mul nsw i32 %add13.i.i.5, %19
  %681 = mul i32 %19, %add13.i.i.4
  %682 = trunc i64 %2 to i32
  %683 = shl i32 %682, 5
  %684 = add nsw i32 %681, %683
  %685 = icmp sgt i32 %684, 2147483616
  %686 = add i32 %681, %683
  %687 = add i32 %686, 1
  %688 = add i32 %686, 32
  %689 = icmp slt i32 %688, %687
  %690 = or i1 %685, %689
  %691 = or i32 %conv2.i.i, 6
  %692 = mul i32 %19, %691
  %693 = add nsw i32 %692, %683
  %694 = icmp sgt i32 %693, 2147483616
  %695 = or i1 %690, %694
  br i1 %695, label %pregion_for_entry.entry.i.i.5.preheader, label %vector.body110

pregion_for_entry.entry.i.i.5.preheader:          ; preds = %pregion_for_end.i.i.4
  br label %pregion_for_entry.entry.i.i.5

vector.body110:                                   ; preds = %pregion_for_end.i.i.4
  %696 = trunc i64 %mul.i.i.i to i32
  %697 = add i32 %mul.i.i.5, %696
  %698 = sext i32 %697 to i64
  %699 = getelementptr inbounds float, float* %15, i64 %698
  %700 = bitcast float* %699 to <8 x float>*
  %wide.load127 = load <8 x float>, <8 x float>* %700, align 4, !tbaa !12, !llvm.access.group !16
  %701 = fpext <8 x float> %wide.load127 to <8 x double>
  %702 = add i32 %697, 1
  %703 = sext i32 %702 to i64
  %704 = getelementptr inbounds float, float* %7, i64 %703
  %705 = bitcast float* %704 to <8 x float>*
  %wide.load128 = load <8 x float>, <8 x float>* %705, align 4, !tbaa !12, !llvm.access.group !16
  %706 = getelementptr inbounds float, float* %7, i64 %698
  %707 = bitcast float* %706 to <8 x float>*
  %wide.load129 = load <8 x float>, <8 x float>* %707, align 4, !tbaa !12, !llvm.access.group !16
  %708 = fsub <8 x float> %wide.load128, %wide.load129
  %709 = add nsw i32 %mul14.i.i.5, %696
  %710 = sext i32 %709 to i64
  %711 = getelementptr inbounds float, float* %11, i64 %710
  %712 = bitcast float* %711 to <8 x float>*
  %wide.load130 = load <8 x float>, <8 x float>* %712, align 4, !tbaa !12, !llvm.access.group !16
  %713 = fadd <8 x float> %708, %wide.load130
  %714 = getelementptr inbounds float, float* %11, i64 %698
  %715 = bitcast float* %714 to <8 x float>*
  %wide.load131 = load <8 x float>, <8 x float>* %715, align 4, !tbaa !12, !llvm.access.group !16
  %716 = fsub <8 x float> %713, %wide.load131
  %717 = fpext <8 x float> %716 to <8 x double>
  %718 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %717, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %701)
  %719 = fptrunc <8 x double> %718 to <8 x float>
  %720 = bitcast float* %699 to <8 x float>*
  store <8 x float> %719, <8 x float>* %720, align 4, !tbaa !12, !llvm.access.group !16
  %721 = trunc i64 %mul.i.i.i to i32
  %722 = or i32 %721, 8
  %723 = add i32 %mul.i.i.5, %722
  %724 = sext i32 %723 to i64
  %725 = getelementptr inbounds float, float* %15, i64 %724
  %726 = bitcast float* %725 to <8 x float>*
  %wide.load127.1 = load <8 x float>, <8 x float>* %726, align 4, !tbaa !12, !llvm.access.group !16
  %727 = fpext <8 x float> %wide.load127.1 to <8 x double>
  %728 = add i32 %723, 1
  %729 = sext i32 %728 to i64
  %730 = getelementptr inbounds float, float* %7, i64 %729
  %731 = bitcast float* %730 to <8 x float>*
  %wide.load128.1 = load <8 x float>, <8 x float>* %731, align 4, !tbaa !12, !llvm.access.group !16
  %732 = getelementptr inbounds float, float* %7, i64 %724
  %733 = bitcast float* %732 to <8 x float>*
  %wide.load129.1 = load <8 x float>, <8 x float>* %733, align 4, !tbaa !12, !llvm.access.group !16
  %734 = fsub <8 x float> %wide.load128.1, %wide.load129.1
  %735 = add nsw i32 %mul14.i.i.5, %722
  %736 = sext i32 %735 to i64
  %737 = getelementptr inbounds float, float* %11, i64 %736
  %738 = bitcast float* %737 to <8 x float>*
  %wide.load130.1 = load <8 x float>, <8 x float>* %738, align 4, !tbaa !12, !llvm.access.group !16
  %739 = fadd <8 x float> %734, %wide.load130.1
  %740 = getelementptr inbounds float, float* %11, i64 %724
  %741 = bitcast float* %740 to <8 x float>*
  %wide.load131.1 = load <8 x float>, <8 x float>* %741, align 4, !tbaa !12, !llvm.access.group !16
  %742 = fsub <8 x float> %739, %wide.load131.1
  %743 = fpext <8 x float> %742 to <8 x double>
  %744 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %743, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %727)
  %745 = fptrunc <8 x double> %744 to <8 x float>
  %746 = bitcast float* %725 to <8 x float>*
  store <8 x float> %745, <8 x float>* %746, align 4, !tbaa !12, !llvm.access.group !16
  %747 = trunc i64 %mul.i.i.i to i32
  %748 = or i32 %747, 16
  %749 = add i32 %mul.i.i.5, %748
  %750 = sext i32 %749 to i64
  %751 = getelementptr inbounds float, float* %15, i64 %750
  %752 = bitcast float* %751 to <8 x float>*
  %wide.load127.2 = load <8 x float>, <8 x float>* %752, align 4, !tbaa !12, !llvm.access.group !16
  %753 = fpext <8 x float> %wide.load127.2 to <8 x double>
  %754 = add i32 %749, 1
  %755 = sext i32 %754 to i64
  %756 = getelementptr inbounds float, float* %7, i64 %755
  %757 = bitcast float* %756 to <8 x float>*
  %wide.load128.2 = load <8 x float>, <8 x float>* %757, align 4, !tbaa !12, !llvm.access.group !16
  %758 = getelementptr inbounds float, float* %7, i64 %750
  %759 = bitcast float* %758 to <8 x float>*
  %wide.load129.2 = load <8 x float>, <8 x float>* %759, align 4, !tbaa !12, !llvm.access.group !16
  %760 = fsub <8 x float> %wide.load128.2, %wide.load129.2
  %761 = add nsw i32 %mul14.i.i.5, %748
  %762 = sext i32 %761 to i64
  %763 = getelementptr inbounds float, float* %11, i64 %762
  %764 = bitcast float* %763 to <8 x float>*
  %wide.load130.2 = load <8 x float>, <8 x float>* %764, align 4, !tbaa !12, !llvm.access.group !16
  %765 = fadd <8 x float> %760, %wide.load130.2
  %766 = getelementptr inbounds float, float* %11, i64 %750
  %767 = bitcast float* %766 to <8 x float>*
  %wide.load131.2 = load <8 x float>, <8 x float>* %767, align 4, !tbaa !12, !llvm.access.group !16
  %768 = fsub <8 x float> %765, %wide.load131.2
  %769 = fpext <8 x float> %768 to <8 x double>
  %770 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %769, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %753)
  %771 = fptrunc <8 x double> %770 to <8 x float>
  %772 = bitcast float* %751 to <8 x float>*
  store <8 x float> %771, <8 x float>* %772, align 4, !tbaa !12, !llvm.access.group !16
  %773 = trunc i64 %mul.i.i.i to i32
  %774 = or i32 %773, 24
  %775 = add i32 %mul.i.i.5, %774
  %776 = sext i32 %775 to i64
  %777 = getelementptr inbounds float, float* %15, i64 %776
  %778 = bitcast float* %777 to <8 x float>*
  %wide.load127.3 = load <8 x float>, <8 x float>* %778, align 4, !tbaa !12, !llvm.access.group !16
  %779 = fpext <8 x float> %wide.load127.3 to <8 x double>
  %780 = add i32 %775, 1
  %781 = sext i32 %780 to i64
  %782 = getelementptr inbounds float, float* %7, i64 %781
  %783 = bitcast float* %782 to <8 x float>*
  %wide.load128.3 = load <8 x float>, <8 x float>* %783, align 4, !tbaa !12, !llvm.access.group !16
  %784 = getelementptr inbounds float, float* %7, i64 %776
  %785 = bitcast float* %784 to <8 x float>*
  %wide.load129.3 = load <8 x float>, <8 x float>* %785, align 4, !tbaa !12, !llvm.access.group !16
  %786 = fsub <8 x float> %wide.load128.3, %wide.load129.3
  %787 = add nsw i32 %mul14.i.i.5, %774
  %788 = sext i32 %787 to i64
  %789 = getelementptr inbounds float, float* %11, i64 %788
  %790 = bitcast float* %789 to <8 x float>*
  %wide.load130.3 = load <8 x float>, <8 x float>* %790, align 4, !tbaa !12, !llvm.access.group !16
  %791 = fadd <8 x float> %786, %wide.load130.3
  %792 = getelementptr inbounds float, float* %11, i64 %776
  %793 = bitcast float* %792 to <8 x float>*
  %wide.load131.3 = load <8 x float>, <8 x float>* %793, align 4, !tbaa !12, !llvm.access.group !16
  %794 = fsub <8 x float> %791, %wide.load131.3
  %795 = fpext <8 x float> %794 to <8 x double>
  %796 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %795, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %779)
  %797 = fptrunc <8 x double> %796 to <8 x float>
  %798 = bitcast float* %777 to <8 x float>*
  store <8 x float> %797, <8 x float>* %798, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.5

pregion_for_entry.entry.i.i.5:                    ; preds = %pregion_for_entry.entry.i.i.5, %pregion_for_entry.entry.i.i.5.preheader
  %_local_id_x.i.0.5 = phi i64 [ %812, %pregion_for_entry.entry.i.i.5 ], [ 0, %pregion_for_entry.entry.i.i.5.preheader ]
  %add1.i.i.i.5 = add nuw nsw i64 %_local_id_x.i.0.5, %mul.i.i.i
  %conv.i.i.5 = trunc i64 %add1.i.i.i.5 to i32
  %add.i.i.5 = add i32 %mul.i.i.5, %conv.i.i.5
  %idxprom.i.i.5 = sext i32 %add.i.i.5 to i64
  %arrayidx.i.i.5 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.5
  %799 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.5 = fpext float %799 to double
  %add6.i.i.5 = add i32 %add.i.i.5, 1
  %idxprom7.i.i.5 = sext i32 %add6.i.i.5 to i64
  %arrayidx8.i.i.5 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.5
  %800 = load float, float* %arrayidx8.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.5 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.5
  %801 = load float, float* %arrayidx12.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.5 = fsub float %800, %801
  %add15.i.i.5 = add nsw i32 %mul14.i.i.5, %conv.i.i.5
  %idxprom16.i.i.5 = sext i32 %add15.i.i.5 to i64
  %arrayidx17.i.i.5 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.5
  %802 = load float, float* %arrayidx17.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.5 = fadd float %sub.i.i.5, %802
  %arrayidx22.i.i.5 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.5
  %803 = load float, float* %arrayidx22.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.5 = fsub float %add18.i.i.5, %803
  %conv24.i.i.5 = fpext float %sub23.i.i.5 to double
  %804 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.5, double 0xBFE6666666666666, double %conv3.i.i.5) #3
  %conv26.i.i.5 = fptrunc double %804 to float
  store float %conv26.i.i.5, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %805 = or i64 %_local_id_x.i.0.5, 1
  %add1.i.i.i.5.1 = add nuw nsw i64 %805, %mul.i.i.i
  %conv.i.i.5.1 = trunc i64 %add1.i.i.i.5.1 to i32
  %add.i.i.5.1 = add i32 %mul.i.i.5, %conv.i.i.5.1
  %idxprom.i.i.5.1 = sext i32 %add.i.i.5.1 to i64
  %arrayidx.i.i.5.1 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.5.1
  %806 = load float, float* %arrayidx.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.5.1 = fpext float %806 to double
  %add6.i.i.5.1 = add i32 %add.i.i.5.1, 1
  %idxprom7.i.i.5.1 = sext i32 %add6.i.i.5.1 to i64
  %arrayidx8.i.i.5.1 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.5.1
  %807 = load float, float* %arrayidx8.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.5.1 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.5.1
  %808 = load float, float* %arrayidx12.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.5.1 = fsub float %807, %808
  %add15.i.i.5.1 = add nsw i32 %mul14.i.i.5, %conv.i.i.5.1
  %idxprom16.i.i.5.1 = sext i32 %add15.i.i.5.1 to i64
  %arrayidx17.i.i.5.1 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.5.1
  %809 = load float, float* %arrayidx17.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.5.1 = fadd float %sub.i.i.5.1, %809
  %arrayidx22.i.i.5.1 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.5.1
  %810 = load float, float* %arrayidx22.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.5.1 = fsub float %add18.i.i.5.1, %810
  %conv24.i.i.5.1 = fpext float %sub23.i.i.5.1 to double
  %811 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.5.1, double 0xBFE6666666666666, double %conv3.i.i.5.1) #3
  %conv26.i.i.5.1 = fptrunc double %811 to float
  store float %conv26.i.i.5.1, float* %arrayidx.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %812 = add nuw nsw i64 %_local_id_x.i.0.5, 2
  %exitcond.5.not.1 = icmp eq i64 %812, 32
  br i1 %exitcond.5.not.1, label %pregion_for_end.i.i.5.loopexit, label %pregion_for_entry.entry.i.i.5, !llvm.loop !34

pregion_for_end.i.i.5.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.5
  br label %pregion_for_end.i.i.5

pregion_for_end.i.i.5:                            ; preds = %pregion_for_end.i.i.5.loopexit, %vector.body110
  %813 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.6 = or i32 %813, 6
  %mul.i.i.6 = mul nsw i32 %19, %conv2.i.i.6
  %add13.i.i.6 = or i32 %813, 7
  %mul14.i.i.6 = mul nsw i32 %add13.i.i.6, %19
  %814 = mul i32 %19, %conv2.i.i.6
  %815 = trunc i64 %2 to i32
  %816 = shl i32 %815, 5
  %817 = add nsw i32 %814, %816
  %818 = icmp sgt i32 %817, 2147483616
  %819 = add i32 %814, %816
  %820 = or i32 %819, 1
  %821 = icmp sgt i32 %820, 2147483616
  %822 = or i1 %818, %821
  %823 = mul i32 %19, %add13.i.i.6
  %824 = add nsw i32 %823, %816
  %825 = icmp sgt i32 %824, 2147483616
  %826 = or i1 %822, %825
  br i1 %826, label %pregion_for_entry.entry.i.i.6.preheader, label %vector.body134

pregion_for_entry.entry.i.i.6.preheader:          ; preds = %pregion_for_end.i.i.5
  br label %pregion_for_entry.entry.i.i.6

vector.body134:                                   ; preds = %pregion_for_end.i.i.5
  %827 = trunc i64 %mul.i.i.i to i32
  %828 = add i32 %mul.i.i.6, %827
  %829 = sext i32 %828 to i64
  %830 = getelementptr inbounds float, float* %15, i64 %829
  %831 = bitcast float* %830 to <8 x float>*
  %wide.load151 = load <8 x float>, <8 x float>* %831, align 4, !tbaa !12, !llvm.access.group !16
  %832 = fpext <8 x float> %wide.load151 to <8 x double>
  %833 = or i32 %828, 1
  %834 = sext i32 %833 to i64
  %835 = getelementptr inbounds float, float* %7, i64 %834
  %836 = bitcast float* %835 to <8 x float>*
  %wide.load152 = load <8 x float>, <8 x float>* %836, align 4, !tbaa !12, !llvm.access.group !16
  %837 = getelementptr inbounds float, float* %7, i64 %829
  %838 = bitcast float* %837 to <8 x float>*
  %wide.load153 = load <8 x float>, <8 x float>* %838, align 4, !tbaa !12, !llvm.access.group !16
  %839 = fsub <8 x float> %wide.load152, %wide.load153
  %840 = add nsw i32 %mul14.i.i.6, %827
  %841 = sext i32 %840 to i64
  %842 = getelementptr inbounds float, float* %11, i64 %841
  %843 = bitcast float* %842 to <8 x float>*
  %wide.load154 = load <8 x float>, <8 x float>* %843, align 4, !tbaa !12, !llvm.access.group !16
  %844 = fadd <8 x float> %839, %wide.load154
  %845 = getelementptr inbounds float, float* %11, i64 %829
  %846 = bitcast float* %845 to <8 x float>*
  %wide.load155 = load <8 x float>, <8 x float>* %846, align 4, !tbaa !12, !llvm.access.group !16
  %847 = fsub <8 x float> %844, %wide.load155
  %848 = fpext <8 x float> %847 to <8 x double>
  %849 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %848, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %832)
  %850 = fptrunc <8 x double> %849 to <8 x float>
  %851 = bitcast float* %830 to <8 x float>*
  store <8 x float> %850, <8 x float>* %851, align 4, !tbaa !12, !llvm.access.group !16
  %852 = trunc i64 %mul.i.i.i to i32
  %853 = or i32 %852, 8
  %854 = add i32 %mul.i.i.6, %853
  %855 = sext i32 %854 to i64
  %856 = getelementptr inbounds float, float* %15, i64 %855
  %857 = bitcast float* %856 to <8 x float>*
  %wide.load151.1 = load <8 x float>, <8 x float>* %857, align 4, !tbaa !12, !llvm.access.group !16
  %858 = fpext <8 x float> %wide.load151.1 to <8 x double>
  %859 = or i32 %854, 1
  %860 = sext i32 %859 to i64
  %861 = getelementptr inbounds float, float* %7, i64 %860
  %862 = bitcast float* %861 to <8 x float>*
  %wide.load152.1 = load <8 x float>, <8 x float>* %862, align 4, !tbaa !12, !llvm.access.group !16
  %863 = getelementptr inbounds float, float* %7, i64 %855
  %864 = bitcast float* %863 to <8 x float>*
  %wide.load153.1 = load <8 x float>, <8 x float>* %864, align 4, !tbaa !12, !llvm.access.group !16
  %865 = fsub <8 x float> %wide.load152.1, %wide.load153.1
  %866 = add nsw i32 %mul14.i.i.6, %853
  %867 = sext i32 %866 to i64
  %868 = getelementptr inbounds float, float* %11, i64 %867
  %869 = bitcast float* %868 to <8 x float>*
  %wide.load154.1 = load <8 x float>, <8 x float>* %869, align 4, !tbaa !12, !llvm.access.group !16
  %870 = fadd <8 x float> %865, %wide.load154.1
  %871 = getelementptr inbounds float, float* %11, i64 %855
  %872 = bitcast float* %871 to <8 x float>*
  %wide.load155.1 = load <8 x float>, <8 x float>* %872, align 4, !tbaa !12, !llvm.access.group !16
  %873 = fsub <8 x float> %870, %wide.load155.1
  %874 = fpext <8 x float> %873 to <8 x double>
  %875 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %874, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %858)
  %876 = fptrunc <8 x double> %875 to <8 x float>
  %877 = bitcast float* %856 to <8 x float>*
  store <8 x float> %876, <8 x float>* %877, align 4, !tbaa !12, !llvm.access.group !16
  %878 = trunc i64 %mul.i.i.i to i32
  %879 = or i32 %878, 16
  %880 = add i32 %mul.i.i.6, %879
  %881 = sext i32 %880 to i64
  %882 = getelementptr inbounds float, float* %15, i64 %881
  %883 = bitcast float* %882 to <8 x float>*
  %wide.load151.2 = load <8 x float>, <8 x float>* %883, align 4, !tbaa !12, !llvm.access.group !16
  %884 = fpext <8 x float> %wide.load151.2 to <8 x double>
  %885 = or i32 %880, 1
  %886 = sext i32 %885 to i64
  %887 = getelementptr inbounds float, float* %7, i64 %886
  %888 = bitcast float* %887 to <8 x float>*
  %wide.load152.2 = load <8 x float>, <8 x float>* %888, align 4, !tbaa !12, !llvm.access.group !16
  %889 = getelementptr inbounds float, float* %7, i64 %881
  %890 = bitcast float* %889 to <8 x float>*
  %wide.load153.2 = load <8 x float>, <8 x float>* %890, align 4, !tbaa !12, !llvm.access.group !16
  %891 = fsub <8 x float> %wide.load152.2, %wide.load153.2
  %892 = add nsw i32 %mul14.i.i.6, %879
  %893 = sext i32 %892 to i64
  %894 = getelementptr inbounds float, float* %11, i64 %893
  %895 = bitcast float* %894 to <8 x float>*
  %wide.load154.2 = load <8 x float>, <8 x float>* %895, align 4, !tbaa !12, !llvm.access.group !16
  %896 = fadd <8 x float> %891, %wide.load154.2
  %897 = getelementptr inbounds float, float* %11, i64 %881
  %898 = bitcast float* %897 to <8 x float>*
  %wide.load155.2 = load <8 x float>, <8 x float>* %898, align 4, !tbaa !12, !llvm.access.group !16
  %899 = fsub <8 x float> %896, %wide.load155.2
  %900 = fpext <8 x float> %899 to <8 x double>
  %901 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %900, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %884)
  %902 = fptrunc <8 x double> %901 to <8 x float>
  %903 = bitcast float* %882 to <8 x float>*
  store <8 x float> %902, <8 x float>* %903, align 4, !tbaa !12, !llvm.access.group !16
  %904 = trunc i64 %mul.i.i.i to i32
  %905 = or i32 %904, 24
  %906 = add i32 %mul.i.i.6, %905
  %907 = sext i32 %906 to i64
  %908 = getelementptr inbounds float, float* %15, i64 %907
  %909 = bitcast float* %908 to <8 x float>*
  %wide.load151.3 = load <8 x float>, <8 x float>* %909, align 4, !tbaa !12, !llvm.access.group !16
  %910 = fpext <8 x float> %wide.load151.3 to <8 x double>
  %911 = or i32 %906, 1
  %912 = sext i32 %911 to i64
  %913 = getelementptr inbounds float, float* %7, i64 %912
  %914 = bitcast float* %913 to <8 x float>*
  %wide.load152.3 = load <8 x float>, <8 x float>* %914, align 4, !tbaa !12, !llvm.access.group !16
  %915 = getelementptr inbounds float, float* %7, i64 %907
  %916 = bitcast float* %915 to <8 x float>*
  %wide.load153.3 = load <8 x float>, <8 x float>* %916, align 4, !tbaa !12, !llvm.access.group !16
  %917 = fsub <8 x float> %wide.load152.3, %wide.load153.3
  %918 = add nsw i32 %mul14.i.i.6, %905
  %919 = sext i32 %918 to i64
  %920 = getelementptr inbounds float, float* %11, i64 %919
  %921 = bitcast float* %920 to <8 x float>*
  %wide.load154.3 = load <8 x float>, <8 x float>* %921, align 4, !tbaa !12, !llvm.access.group !16
  %922 = fadd <8 x float> %917, %wide.load154.3
  %923 = getelementptr inbounds float, float* %11, i64 %907
  %924 = bitcast float* %923 to <8 x float>*
  %wide.load155.3 = load <8 x float>, <8 x float>* %924, align 4, !tbaa !12, !llvm.access.group !16
  %925 = fsub <8 x float> %922, %wide.load155.3
  %926 = fpext <8 x float> %925 to <8 x double>
  %927 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %926, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %910)
  %928 = fptrunc <8 x double> %927 to <8 x float>
  %929 = bitcast float* %908 to <8 x float>*
  store <8 x float> %928, <8 x float>* %929, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.6

pregion_for_entry.entry.i.i.6:                    ; preds = %pregion_for_entry.entry.i.i.6, %pregion_for_entry.entry.i.i.6.preheader
  %_local_id_x.i.0.6 = phi i64 [ %943, %pregion_for_entry.entry.i.i.6 ], [ 0, %pregion_for_entry.entry.i.i.6.preheader ]
  %add1.i.i.i.6 = add nuw nsw i64 %_local_id_x.i.0.6, %mul.i.i.i
  %conv.i.i.6 = trunc i64 %add1.i.i.i.6 to i32
  %add.i.i.6 = add i32 %mul.i.i.6, %conv.i.i.6
  %idxprom.i.i.6 = sext i32 %add.i.i.6 to i64
  %arrayidx.i.i.6 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.6
  %930 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.6 = fpext float %930 to double
  %add6.i.i.6 = or i32 %add.i.i.6, 1
  %idxprom7.i.i.6 = sext i32 %add6.i.i.6 to i64
  %arrayidx8.i.i.6 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.6
  %931 = load float, float* %arrayidx8.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.6 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.6
  %932 = load float, float* %arrayidx12.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.6 = fsub float %931, %932
  %add15.i.i.6 = add nsw i32 %mul14.i.i.6, %conv.i.i.6
  %idxprom16.i.i.6 = sext i32 %add15.i.i.6 to i64
  %arrayidx17.i.i.6 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.6
  %933 = load float, float* %arrayidx17.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.6 = fadd float %sub.i.i.6, %933
  %arrayidx22.i.i.6 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.6
  %934 = load float, float* %arrayidx22.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.6 = fsub float %add18.i.i.6, %934
  %conv24.i.i.6 = fpext float %sub23.i.i.6 to double
  %935 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.6, double 0xBFE6666666666666, double %conv3.i.i.6) #3
  %conv26.i.i.6 = fptrunc double %935 to float
  store float %conv26.i.i.6, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %936 = or i64 %_local_id_x.i.0.6, 1
  %add1.i.i.i.6.1 = add nuw nsw i64 %936, %mul.i.i.i
  %conv.i.i.6.1 = trunc i64 %add1.i.i.i.6.1 to i32
  %add.i.i.6.1 = add i32 %mul.i.i.6, %conv.i.i.6.1
  %idxprom.i.i.6.1 = sext i32 %add.i.i.6.1 to i64
  %arrayidx.i.i.6.1 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.6.1
  %937 = load float, float* %arrayidx.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.6.1 = fpext float %937 to double
  %add6.i.i.6.1 = add i32 %add.i.i.6.1, 1
  %idxprom7.i.i.6.1 = sext i32 %add6.i.i.6.1 to i64
  %arrayidx8.i.i.6.1 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.6.1
  %938 = load float, float* %arrayidx8.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.6.1 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.6.1
  %939 = load float, float* %arrayidx12.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.6.1 = fsub float %938, %939
  %add15.i.i.6.1 = add nsw i32 %mul14.i.i.6, %conv.i.i.6.1
  %idxprom16.i.i.6.1 = sext i32 %add15.i.i.6.1 to i64
  %arrayidx17.i.i.6.1 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.6.1
  %940 = load float, float* %arrayidx17.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.6.1 = fadd float %sub.i.i.6.1, %940
  %arrayidx22.i.i.6.1 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.6.1
  %941 = load float, float* %arrayidx22.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.6.1 = fsub float %add18.i.i.6.1, %941
  %conv24.i.i.6.1 = fpext float %sub23.i.i.6.1 to double
  %942 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.6.1, double 0xBFE6666666666666, double %conv3.i.i.6.1) #3
  %conv26.i.i.6.1 = fptrunc double %942 to float
  store float %conv26.i.i.6.1, float* %arrayidx.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %943 = add nuw nsw i64 %_local_id_x.i.0.6, 2
  %exitcond.6.not.1 = icmp eq i64 %943, 32
  br i1 %exitcond.6.not.1, label %pregion_for_end.i.i.6.loopexit, label %pregion_for_entry.entry.i.i.6, !llvm.loop !35

pregion_for_end.i.i.6.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.6
  br label %pregion_for_end.i.i.6

pregion_for_end.i.i.6:                            ; preds = %pregion_for_end.i.i.6.loopexit, %vector.body134
  %944 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.7 = or i32 %944, 7
  %mul.i.i.7 = mul nsw i32 %19, %conv2.i.i.7
  %add13.i.i.7 = add nsw i32 %conv2.i.i.7, 1
  %mul14.i.i.7 = mul nsw i32 %add13.i.i.7, %19
  %945 = mul i32 %19, %add13.i.i.6
  %946 = trunc i64 %2 to i32
  %947 = shl i32 %946, 5
  %948 = add nsw i32 %945, %947
  %949 = icmp sgt i32 %948, 2147483616
  %950 = add i32 %945, %947
  %951 = add i32 %950, 1
  %952 = add i32 %950, 32
  %953 = icmp slt i32 %952, %951
  %954 = or i1 %949, %953
  %955 = add i32 %conv2.i.i, 8
  %956 = mul i32 %19, %955
  %957 = add nsw i32 %956, %947
  %958 = icmp sgt i32 %957, 2147483616
  %959 = or i1 %954, %958
  br i1 %959, label %pregion_for_entry.entry.i.i.7.preheader, label %vector.body158

pregion_for_entry.entry.i.i.7.preheader:          ; preds = %pregion_for_end.i.i.6
  br label %pregion_for_entry.entry.i.i.7

vector.body158:                                   ; preds = %pregion_for_end.i.i.6
  %960 = trunc i64 %mul.i.i.i to i32
  %961 = add i32 %mul.i.i.7, %960
  %962 = sext i32 %961 to i64
  %963 = getelementptr inbounds float, float* %15, i64 %962
  %964 = bitcast float* %963 to <8 x float>*
  %wide.load175 = load <8 x float>, <8 x float>* %964, align 4, !tbaa !12, !llvm.access.group !16
  %965 = fpext <8 x float> %wide.load175 to <8 x double>
  %966 = add i32 %961, 1
  %967 = sext i32 %966 to i64
  %968 = getelementptr inbounds float, float* %7, i64 %967
  %969 = bitcast float* %968 to <8 x float>*
  %wide.load176 = load <8 x float>, <8 x float>* %969, align 4, !tbaa !12, !llvm.access.group !16
  %970 = getelementptr inbounds float, float* %7, i64 %962
  %971 = bitcast float* %970 to <8 x float>*
  %wide.load177 = load <8 x float>, <8 x float>* %971, align 4, !tbaa !12, !llvm.access.group !16
  %972 = fsub <8 x float> %wide.load176, %wide.load177
  %973 = add nsw i32 %mul14.i.i.7, %960
  %974 = sext i32 %973 to i64
  %975 = getelementptr inbounds float, float* %11, i64 %974
  %976 = bitcast float* %975 to <8 x float>*
  %wide.load178 = load <8 x float>, <8 x float>* %976, align 4, !tbaa !12, !llvm.access.group !16
  %977 = fadd <8 x float> %972, %wide.load178
  %978 = getelementptr inbounds float, float* %11, i64 %962
  %979 = bitcast float* %978 to <8 x float>*
  %wide.load179 = load <8 x float>, <8 x float>* %979, align 4, !tbaa !12, !llvm.access.group !16
  %980 = fsub <8 x float> %977, %wide.load179
  %981 = fpext <8 x float> %980 to <8 x double>
  %982 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %981, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %965)
  %983 = fptrunc <8 x double> %982 to <8 x float>
  %984 = bitcast float* %963 to <8 x float>*
  store <8 x float> %983, <8 x float>* %984, align 4, !tbaa !12, !llvm.access.group !16
  %985 = trunc i64 %mul.i.i.i to i32
  %986 = or i32 %985, 8
  %987 = add i32 %mul.i.i.7, %986
  %988 = sext i32 %987 to i64
  %989 = getelementptr inbounds float, float* %15, i64 %988
  %990 = bitcast float* %989 to <8 x float>*
  %wide.load175.1 = load <8 x float>, <8 x float>* %990, align 4, !tbaa !12, !llvm.access.group !16
  %991 = fpext <8 x float> %wide.load175.1 to <8 x double>
  %992 = add i32 %987, 1
  %993 = sext i32 %992 to i64
  %994 = getelementptr inbounds float, float* %7, i64 %993
  %995 = bitcast float* %994 to <8 x float>*
  %wide.load176.1 = load <8 x float>, <8 x float>* %995, align 4, !tbaa !12, !llvm.access.group !16
  %996 = getelementptr inbounds float, float* %7, i64 %988
  %997 = bitcast float* %996 to <8 x float>*
  %wide.load177.1 = load <8 x float>, <8 x float>* %997, align 4, !tbaa !12, !llvm.access.group !16
  %998 = fsub <8 x float> %wide.load176.1, %wide.load177.1
  %999 = add nsw i32 %mul14.i.i.7, %986
  %1000 = sext i32 %999 to i64
  %1001 = getelementptr inbounds float, float* %11, i64 %1000
  %1002 = bitcast float* %1001 to <8 x float>*
  %wide.load178.1 = load <8 x float>, <8 x float>* %1002, align 4, !tbaa !12, !llvm.access.group !16
  %1003 = fadd <8 x float> %998, %wide.load178.1
  %1004 = getelementptr inbounds float, float* %11, i64 %988
  %1005 = bitcast float* %1004 to <8 x float>*
  %wide.load179.1 = load <8 x float>, <8 x float>* %1005, align 4, !tbaa !12, !llvm.access.group !16
  %1006 = fsub <8 x float> %1003, %wide.load179.1
  %1007 = fpext <8 x float> %1006 to <8 x double>
  %1008 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %1007, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %991)
  %1009 = fptrunc <8 x double> %1008 to <8 x float>
  %1010 = bitcast float* %989 to <8 x float>*
  store <8 x float> %1009, <8 x float>* %1010, align 4, !tbaa !12, !llvm.access.group !16
  %1011 = trunc i64 %mul.i.i.i to i32
  %1012 = or i32 %1011, 16
  %1013 = add i32 %mul.i.i.7, %1012
  %1014 = sext i32 %1013 to i64
  %1015 = getelementptr inbounds float, float* %15, i64 %1014
  %1016 = bitcast float* %1015 to <8 x float>*
  %wide.load175.2 = load <8 x float>, <8 x float>* %1016, align 4, !tbaa !12, !llvm.access.group !16
  %1017 = fpext <8 x float> %wide.load175.2 to <8 x double>
  %1018 = add i32 %1013, 1
  %1019 = sext i32 %1018 to i64
  %1020 = getelementptr inbounds float, float* %7, i64 %1019
  %1021 = bitcast float* %1020 to <8 x float>*
  %wide.load176.2 = load <8 x float>, <8 x float>* %1021, align 4, !tbaa !12, !llvm.access.group !16
  %1022 = getelementptr inbounds float, float* %7, i64 %1014
  %1023 = bitcast float* %1022 to <8 x float>*
  %wide.load177.2 = load <8 x float>, <8 x float>* %1023, align 4, !tbaa !12, !llvm.access.group !16
  %1024 = fsub <8 x float> %wide.load176.2, %wide.load177.2
  %1025 = add nsw i32 %mul14.i.i.7, %1012
  %1026 = sext i32 %1025 to i64
  %1027 = getelementptr inbounds float, float* %11, i64 %1026
  %1028 = bitcast float* %1027 to <8 x float>*
  %wide.load178.2 = load <8 x float>, <8 x float>* %1028, align 4, !tbaa !12, !llvm.access.group !16
  %1029 = fadd <8 x float> %1024, %wide.load178.2
  %1030 = getelementptr inbounds float, float* %11, i64 %1014
  %1031 = bitcast float* %1030 to <8 x float>*
  %wide.load179.2 = load <8 x float>, <8 x float>* %1031, align 4, !tbaa !12, !llvm.access.group !16
  %1032 = fsub <8 x float> %1029, %wide.load179.2
  %1033 = fpext <8 x float> %1032 to <8 x double>
  %1034 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %1033, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %1017)
  %1035 = fptrunc <8 x double> %1034 to <8 x float>
  %1036 = bitcast float* %1015 to <8 x float>*
  store <8 x float> %1035, <8 x float>* %1036, align 4, !tbaa !12, !llvm.access.group !16
  %1037 = trunc i64 %mul.i.i.i to i32
  %1038 = or i32 %1037, 24
  %1039 = add i32 %mul.i.i.7, %1038
  %1040 = sext i32 %1039 to i64
  %1041 = getelementptr inbounds float, float* %15, i64 %1040
  %1042 = bitcast float* %1041 to <8 x float>*
  %wide.load175.3 = load <8 x float>, <8 x float>* %1042, align 4, !tbaa !12, !llvm.access.group !16
  %1043 = fpext <8 x float> %wide.load175.3 to <8 x double>
  %1044 = add i32 %1039, 1
  %1045 = sext i32 %1044 to i64
  %1046 = getelementptr inbounds float, float* %7, i64 %1045
  %1047 = bitcast float* %1046 to <8 x float>*
  %wide.load176.3 = load <8 x float>, <8 x float>* %1047, align 4, !tbaa !12, !llvm.access.group !16
  %1048 = getelementptr inbounds float, float* %7, i64 %1040
  %1049 = bitcast float* %1048 to <8 x float>*
  %wide.load177.3 = load <8 x float>, <8 x float>* %1049, align 4, !tbaa !12, !llvm.access.group !16
  %1050 = fsub <8 x float> %wide.load176.3, %wide.load177.3
  %1051 = add nsw i32 %mul14.i.i.7, %1038
  %1052 = sext i32 %1051 to i64
  %1053 = getelementptr inbounds float, float* %11, i64 %1052
  %1054 = bitcast float* %1053 to <8 x float>*
  %wide.load178.3 = load <8 x float>, <8 x float>* %1054, align 4, !tbaa !12, !llvm.access.group !16
  %1055 = fadd <8 x float> %1050, %wide.load178.3
  %1056 = getelementptr inbounds float, float* %11, i64 %1040
  %1057 = bitcast float* %1056 to <8 x float>*
  %wide.load179.3 = load <8 x float>, <8 x float>* %1057, align 4, !tbaa !12, !llvm.access.group !16
  %1058 = fsub <8 x float> %1055, %wide.load179.3
  %1059 = fpext <8 x float> %1058 to <8 x double>
  %1060 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %1059, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %1043)
  %1061 = fptrunc <8 x double> %1060 to <8 x float>
  %1062 = bitcast float* %1041 to <8 x float>*
  store <8 x float> %1061, <8 x float>* %1062, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.7

pregion_for_entry.entry.i.i.7:                    ; preds = %pregion_for_entry.entry.i.i.7, %pregion_for_entry.entry.i.i.7.preheader
  %_local_id_x.i.0.7 = phi i64 [ %1076, %pregion_for_entry.entry.i.i.7 ], [ 0, %pregion_for_entry.entry.i.i.7.preheader ]
  %add1.i.i.i.7 = add nuw nsw i64 %_local_id_x.i.0.7, %mul.i.i.i
  %conv.i.i.7 = trunc i64 %add1.i.i.i.7 to i32
  %add.i.i.7 = add i32 %mul.i.i.7, %conv.i.i.7
  %idxprom.i.i.7 = sext i32 %add.i.i.7 to i64
  %arrayidx.i.i.7 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.7
  %1063 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.7 = fpext float %1063 to double
  %add6.i.i.7 = add i32 %add.i.i.7, 1
  %idxprom7.i.i.7 = sext i32 %add6.i.i.7 to i64
  %arrayidx8.i.i.7 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.7
  %1064 = load float, float* %arrayidx8.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.7 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.7
  %1065 = load float, float* %arrayidx12.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.7 = fsub float %1064, %1065
  %add15.i.i.7 = add nsw i32 %mul14.i.i.7, %conv.i.i.7
  %idxprom16.i.i.7 = sext i32 %add15.i.i.7 to i64
  %arrayidx17.i.i.7 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.7
  %1066 = load float, float* %arrayidx17.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.7 = fadd float %sub.i.i.7, %1066
  %arrayidx22.i.i.7 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.7
  %1067 = load float, float* %arrayidx22.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.7 = fsub float %add18.i.i.7, %1067
  %conv24.i.i.7 = fpext float %sub23.i.i.7 to double
  %1068 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.7, double 0xBFE6666666666666, double %conv3.i.i.7) #3
  %conv26.i.i.7 = fptrunc double %1068 to float
  store float %conv26.i.i.7, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %1069 = or i64 %_local_id_x.i.0.7, 1
  %add1.i.i.i.7.1 = add nuw nsw i64 %1069, %mul.i.i.i
  %conv.i.i.7.1 = trunc i64 %add1.i.i.i.7.1 to i32
  %add.i.i.7.1 = add i32 %mul.i.i.7, %conv.i.i.7.1
  %idxprom.i.i.7.1 = sext i32 %add.i.i.7.1 to i64
  %arrayidx.i.i.7.1 = getelementptr inbounds float, float* %15, i64 %idxprom.i.i.7.1
  %1070 = load float, float* %arrayidx.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.7.1 = fpext float %1070 to double
  %add6.i.i.7.1 = add i32 %add.i.i.7.1, 1
  %idxprom7.i.i.7.1 = sext i32 %add6.i.i.7.1 to i64
  %arrayidx8.i.i.7.1 = getelementptr inbounds float, float* %7, i64 %idxprom7.i.i.7.1
  %1071 = load float, float* %arrayidx8.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.7.1 = getelementptr inbounds float, float* %7, i64 %idxprom.i.i.7.1
  %1072 = load float, float* %arrayidx12.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.7.1 = fsub float %1071, %1072
  %add15.i.i.7.1 = add nsw i32 %mul14.i.i.7, %conv.i.i.7.1
  %idxprom16.i.i.7.1 = sext i32 %add15.i.i.7.1 to i64
  %arrayidx17.i.i.7.1 = getelementptr inbounds float, float* %11, i64 %idxprom16.i.i.7.1
  %1073 = load float, float* %arrayidx17.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.7.1 = fadd float %sub.i.i.7.1, %1073
  %arrayidx22.i.i.7.1 = getelementptr inbounds float, float* %11, i64 %idxprom.i.i.7.1
  %1074 = load float, float* %arrayidx22.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.7.1 = fsub float %add18.i.i.7.1, %1074
  %conv24.i.i.7.1 = fpext float %sub23.i.i.7.1 to double
  %1075 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.7.1, double 0xBFE6666666666666, double %conv3.i.i.7.1) #3
  %conv26.i.i.7.1 = fptrunc double %1075 to float
  store float %conv26.i.i.7.1, float* %arrayidx.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %1076 = add nuw nsw i64 %_local_id_x.i.0.7, 2
  %exitcond.7.not.1 = icmp eq i64 %1076, 32
  br i1 %exitcond.7.not.1, label %pregion_for_end.i.i.7.loopexit, label %pregion_for_entry.entry.i.i.7, !llvm.loop !36

pregion_for_end.i.i.7.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.7
  br label %pregion_for_end.i.i.7

pregion_for_end.i.i.7:                            ; preds = %pregion_for_end.i.i.7.loopexit, %vector.body158
  ret void
}

; Function Attrs: nofree nounwind
define void @_pocl_kernel_fdtd_kernel3_workgroup_fast(i8** nocapture readonly %0, { [3 x i64], [3 x i64], [3 x i64], i8*, i32*, i32, i32 }* nocapture readnone %1, i64 %2, i64 %3, i64 %4) local_unnamed_addr #2 {
pregion_for_entry.pregion_for_init.i.i:
  %5 = bitcast i8** %0 to float**
  %6 = load float*, float** %5, align 8
  %7 = getelementptr i8*, i8** %0, i64 1
  %8 = bitcast i8** %7 to float**
  %9 = load float*, float** %8, align 8
  %10 = getelementptr i8*, i8** %0, i64 2
  %11 = bitcast i8** %10 to float**
  %12 = load float*, float** %11, align 8
  %13 = getelementptr i8*, i8** %0, i64 4
  %14 = bitcast i8** %13 to i32**
  %15 = load i32*, i32** %14, align 8
  %16 = load i32, i32* %15, align 4
  %mul.i.i.i = shl i64 %2, 5
  %mul3.i.i.i = shl i64 %3, 3
  %conv2.i.i = trunc i64 %mul3.i.i.i to i32
  %mul.i.i = mul nsw i32 %16, %conv2.i.i
  %add13.i.i = or i32 %conv2.i.i, 1
  %mul14.i.i = mul nsw i32 %add13.i.i, %16
  %17 = trunc i64 %3 to i32
  %18 = mul i32 %16, %17
  %19 = shl i32 %18, 3
  %20 = trunc i64 %2 to i32
  %21 = shl i32 %20, 5
  %22 = add i32 %19, %21
  %23 = icmp sgt i32 %22, 2147483616
  %24 = add i32 %19, %21
  %25 = or i32 %24, 1
  %26 = icmp sgt i32 %25, 2147483616
  %27 = or i1 %23, %26
  %28 = mul i32 %16, %add13.i.i
  %29 = add nsw i32 %28, %21
  %30 = icmp sgt i32 %29, 2147483616
  %31 = or i1 %27, %30
  br i1 %31, label %pregion_for_entry.entry.i.i.preheader, label %vector.body

pregion_for_entry.entry.i.i.preheader:            ; preds = %pregion_for_entry.pregion_for_init.i.i
  br label %pregion_for_entry.entry.i.i

vector.body:                                      ; preds = %pregion_for_entry.pregion_for_init.i.i
  %32 = trunc i64 %mul.i.i.i to i32
  %33 = add i32 %mul.i.i, %32
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds float, float* %12, i64 %34
  %36 = bitcast float* %35 to <8 x float>*
  %wide.load = load <8 x float>, <8 x float>* %36, align 4, !tbaa !12, !llvm.access.group !16
  %37 = fpext <8 x float> %wide.load to <8 x double>
  %38 = or i32 %33, 1
  %39 = sext i32 %38 to i64
  %40 = getelementptr inbounds float, float* %6, i64 %39
  %41 = bitcast float* %40 to <8 x float>*
  %wide.load8 = load <8 x float>, <8 x float>* %41, align 4, !tbaa !12, !llvm.access.group !16
  %42 = getelementptr inbounds float, float* %6, i64 %34
  %43 = bitcast float* %42 to <8 x float>*
  %wide.load9 = load <8 x float>, <8 x float>* %43, align 4, !tbaa !12, !llvm.access.group !16
  %44 = fsub <8 x float> %wide.load8, %wide.load9
  %45 = add nsw i32 %mul14.i.i, %32
  %46 = sext i32 %45 to i64
  %47 = getelementptr inbounds float, float* %9, i64 %46
  %48 = bitcast float* %47 to <8 x float>*
  %wide.load10 = load <8 x float>, <8 x float>* %48, align 4, !tbaa !12, !llvm.access.group !16
  %49 = fadd <8 x float> %44, %wide.load10
  %50 = getelementptr inbounds float, float* %9, i64 %34
  %51 = bitcast float* %50 to <8 x float>*
  %wide.load11 = load <8 x float>, <8 x float>* %51, align 4, !tbaa !12, !llvm.access.group !16
  %52 = fsub <8 x float> %49, %wide.load11
  %53 = fpext <8 x float> %52 to <8 x double>
  %54 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %53, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %37)
  %55 = fptrunc <8 x double> %54 to <8 x float>
  %56 = bitcast float* %35 to <8 x float>*
  store <8 x float> %55, <8 x float>* %56, align 4, !tbaa !12, !llvm.access.group !16
  %57 = trunc i64 %mul.i.i.i to i32
  %58 = or i32 %57, 8
  %59 = add i32 %mul.i.i, %58
  %60 = sext i32 %59 to i64
  %61 = getelementptr inbounds float, float* %12, i64 %60
  %62 = bitcast float* %61 to <8 x float>*
  %wide.load.1 = load <8 x float>, <8 x float>* %62, align 4, !tbaa !12, !llvm.access.group !16
  %63 = fpext <8 x float> %wide.load.1 to <8 x double>
  %64 = or i32 %59, 1
  %65 = sext i32 %64 to i64
  %66 = getelementptr inbounds float, float* %6, i64 %65
  %67 = bitcast float* %66 to <8 x float>*
  %wide.load8.1 = load <8 x float>, <8 x float>* %67, align 4, !tbaa !12, !llvm.access.group !16
  %68 = getelementptr inbounds float, float* %6, i64 %60
  %69 = bitcast float* %68 to <8 x float>*
  %wide.load9.1 = load <8 x float>, <8 x float>* %69, align 4, !tbaa !12, !llvm.access.group !16
  %70 = fsub <8 x float> %wide.load8.1, %wide.load9.1
  %71 = add nsw i32 %mul14.i.i, %58
  %72 = sext i32 %71 to i64
  %73 = getelementptr inbounds float, float* %9, i64 %72
  %74 = bitcast float* %73 to <8 x float>*
  %wide.load10.1 = load <8 x float>, <8 x float>* %74, align 4, !tbaa !12, !llvm.access.group !16
  %75 = fadd <8 x float> %70, %wide.load10.1
  %76 = getelementptr inbounds float, float* %9, i64 %60
  %77 = bitcast float* %76 to <8 x float>*
  %wide.load11.1 = load <8 x float>, <8 x float>* %77, align 4, !tbaa !12, !llvm.access.group !16
  %78 = fsub <8 x float> %75, %wide.load11.1
  %79 = fpext <8 x float> %78 to <8 x double>
  %80 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %79, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %63)
  %81 = fptrunc <8 x double> %80 to <8 x float>
  %82 = bitcast float* %61 to <8 x float>*
  store <8 x float> %81, <8 x float>* %82, align 4, !tbaa !12, !llvm.access.group !16
  %83 = trunc i64 %mul.i.i.i to i32
  %84 = or i32 %83, 16
  %85 = add i32 %mul.i.i, %84
  %86 = sext i32 %85 to i64
  %87 = getelementptr inbounds float, float* %12, i64 %86
  %88 = bitcast float* %87 to <8 x float>*
  %wide.load.2 = load <8 x float>, <8 x float>* %88, align 4, !tbaa !12, !llvm.access.group !16
  %89 = fpext <8 x float> %wide.load.2 to <8 x double>
  %90 = or i32 %85, 1
  %91 = sext i32 %90 to i64
  %92 = getelementptr inbounds float, float* %6, i64 %91
  %93 = bitcast float* %92 to <8 x float>*
  %wide.load8.2 = load <8 x float>, <8 x float>* %93, align 4, !tbaa !12, !llvm.access.group !16
  %94 = getelementptr inbounds float, float* %6, i64 %86
  %95 = bitcast float* %94 to <8 x float>*
  %wide.load9.2 = load <8 x float>, <8 x float>* %95, align 4, !tbaa !12, !llvm.access.group !16
  %96 = fsub <8 x float> %wide.load8.2, %wide.load9.2
  %97 = add nsw i32 %mul14.i.i, %84
  %98 = sext i32 %97 to i64
  %99 = getelementptr inbounds float, float* %9, i64 %98
  %100 = bitcast float* %99 to <8 x float>*
  %wide.load10.2 = load <8 x float>, <8 x float>* %100, align 4, !tbaa !12, !llvm.access.group !16
  %101 = fadd <8 x float> %96, %wide.load10.2
  %102 = getelementptr inbounds float, float* %9, i64 %86
  %103 = bitcast float* %102 to <8 x float>*
  %wide.load11.2 = load <8 x float>, <8 x float>* %103, align 4, !tbaa !12, !llvm.access.group !16
  %104 = fsub <8 x float> %101, %wide.load11.2
  %105 = fpext <8 x float> %104 to <8 x double>
  %106 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %105, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %89)
  %107 = fptrunc <8 x double> %106 to <8 x float>
  %108 = bitcast float* %87 to <8 x float>*
  store <8 x float> %107, <8 x float>* %108, align 4, !tbaa !12, !llvm.access.group !16
  %109 = trunc i64 %mul.i.i.i to i32
  %110 = or i32 %109, 24
  %111 = add i32 %mul.i.i, %110
  %112 = sext i32 %111 to i64
  %113 = getelementptr inbounds float, float* %12, i64 %112
  %114 = bitcast float* %113 to <8 x float>*
  %wide.load.3 = load <8 x float>, <8 x float>* %114, align 4, !tbaa !12, !llvm.access.group !16
  %115 = fpext <8 x float> %wide.load.3 to <8 x double>
  %116 = or i32 %111, 1
  %117 = sext i32 %116 to i64
  %118 = getelementptr inbounds float, float* %6, i64 %117
  %119 = bitcast float* %118 to <8 x float>*
  %wide.load8.3 = load <8 x float>, <8 x float>* %119, align 4, !tbaa !12, !llvm.access.group !16
  %120 = getelementptr inbounds float, float* %6, i64 %112
  %121 = bitcast float* %120 to <8 x float>*
  %wide.load9.3 = load <8 x float>, <8 x float>* %121, align 4, !tbaa !12, !llvm.access.group !16
  %122 = fsub <8 x float> %wide.load8.3, %wide.load9.3
  %123 = add nsw i32 %mul14.i.i, %110
  %124 = sext i32 %123 to i64
  %125 = getelementptr inbounds float, float* %9, i64 %124
  %126 = bitcast float* %125 to <8 x float>*
  %wide.load10.3 = load <8 x float>, <8 x float>* %126, align 4, !tbaa !12, !llvm.access.group !16
  %127 = fadd <8 x float> %122, %wide.load10.3
  %128 = getelementptr inbounds float, float* %9, i64 %112
  %129 = bitcast float* %128 to <8 x float>*
  %wide.load11.3 = load <8 x float>, <8 x float>* %129, align 4, !tbaa !12, !llvm.access.group !16
  %130 = fsub <8 x float> %127, %wide.load11.3
  %131 = fpext <8 x float> %130 to <8 x double>
  %132 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %131, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %115)
  %133 = fptrunc <8 x double> %132 to <8 x float>
  %134 = bitcast float* %113 to <8 x float>*
  store <8 x float> %133, <8 x float>* %134, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i

pregion_for_end.i.i.loopexit:                     ; preds = %pregion_for_entry.entry.i.i
  br label %pregion_for_end.i.i

pregion_for_end.i.i:                              ; preds = %pregion_for_end.i.i.loopexit, %vector.body
  %135 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.1 = or i32 %135, 1
  %mul.i.i.1 = mul nsw i32 %16, %conv2.i.i.1
  %add13.i.i.1 = add nuw nsw i32 %conv2.i.i.1, 1
  %mul14.i.i.1 = mul nsw i32 %add13.i.i.1, %16
  %136 = mul i32 %16, %add13.i.i
  %137 = trunc i64 %2 to i32
  %138 = shl i32 %137, 5
  %139 = add nsw i32 %136, %138
  %140 = icmp sgt i32 %139, 2147483616
  %141 = add i32 %136, %138
  %142 = add i32 %141, 1
  %143 = add i32 %141, 32
  %144 = icmp slt i32 %143, %142
  %145 = or i1 %140, %144
  %146 = or i32 %conv2.i.i, 2
  %147 = mul i32 %16, %146
  %148 = add nsw i32 %147, %138
  %149 = icmp sgt i32 %148, 2147483616
  %150 = or i1 %145, %149
  br i1 %150, label %pregion_for_entry.entry.i.i.1.preheader, label %vector.body14

pregion_for_entry.entry.i.i.1.preheader:          ; preds = %pregion_for_end.i.i
  br label %pregion_for_entry.entry.i.i.1

vector.body14:                                    ; preds = %pregion_for_end.i.i
  %151 = trunc i64 %mul.i.i.i to i32
  %152 = add i32 %mul.i.i.1, %151
  %153 = sext i32 %152 to i64
  %154 = getelementptr inbounds float, float* %12, i64 %153
  %155 = bitcast float* %154 to <8 x float>*
  %wide.load31 = load <8 x float>, <8 x float>* %155, align 4, !tbaa !12, !llvm.access.group !16
  %156 = fpext <8 x float> %wide.load31 to <8 x double>
  %157 = add i32 %152, 1
  %158 = sext i32 %157 to i64
  %159 = getelementptr inbounds float, float* %6, i64 %158
  %160 = bitcast float* %159 to <8 x float>*
  %wide.load32 = load <8 x float>, <8 x float>* %160, align 4, !tbaa !12, !llvm.access.group !16
  %161 = getelementptr inbounds float, float* %6, i64 %153
  %162 = bitcast float* %161 to <8 x float>*
  %wide.load33 = load <8 x float>, <8 x float>* %162, align 4, !tbaa !12, !llvm.access.group !16
  %163 = fsub <8 x float> %wide.load32, %wide.load33
  %164 = add nsw i32 %mul14.i.i.1, %151
  %165 = sext i32 %164 to i64
  %166 = getelementptr inbounds float, float* %9, i64 %165
  %167 = bitcast float* %166 to <8 x float>*
  %wide.load34 = load <8 x float>, <8 x float>* %167, align 4, !tbaa !12, !llvm.access.group !16
  %168 = fadd <8 x float> %163, %wide.load34
  %169 = getelementptr inbounds float, float* %9, i64 %153
  %170 = bitcast float* %169 to <8 x float>*
  %wide.load35 = load <8 x float>, <8 x float>* %170, align 4, !tbaa !12, !llvm.access.group !16
  %171 = fsub <8 x float> %168, %wide.load35
  %172 = fpext <8 x float> %171 to <8 x double>
  %173 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %172, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %156)
  %174 = fptrunc <8 x double> %173 to <8 x float>
  %175 = bitcast float* %154 to <8 x float>*
  store <8 x float> %174, <8 x float>* %175, align 4, !tbaa !12, !llvm.access.group !16
  %176 = trunc i64 %mul.i.i.i to i32
  %177 = or i32 %176, 8
  %178 = add i32 %mul.i.i.1, %177
  %179 = sext i32 %178 to i64
  %180 = getelementptr inbounds float, float* %12, i64 %179
  %181 = bitcast float* %180 to <8 x float>*
  %wide.load31.1 = load <8 x float>, <8 x float>* %181, align 4, !tbaa !12, !llvm.access.group !16
  %182 = fpext <8 x float> %wide.load31.1 to <8 x double>
  %183 = add i32 %178, 1
  %184 = sext i32 %183 to i64
  %185 = getelementptr inbounds float, float* %6, i64 %184
  %186 = bitcast float* %185 to <8 x float>*
  %wide.load32.1 = load <8 x float>, <8 x float>* %186, align 4, !tbaa !12, !llvm.access.group !16
  %187 = getelementptr inbounds float, float* %6, i64 %179
  %188 = bitcast float* %187 to <8 x float>*
  %wide.load33.1 = load <8 x float>, <8 x float>* %188, align 4, !tbaa !12, !llvm.access.group !16
  %189 = fsub <8 x float> %wide.load32.1, %wide.load33.1
  %190 = add nsw i32 %mul14.i.i.1, %177
  %191 = sext i32 %190 to i64
  %192 = getelementptr inbounds float, float* %9, i64 %191
  %193 = bitcast float* %192 to <8 x float>*
  %wide.load34.1 = load <8 x float>, <8 x float>* %193, align 4, !tbaa !12, !llvm.access.group !16
  %194 = fadd <8 x float> %189, %wide.load34.1
  %195 = getelementptr inbounds float, float* %9, i64 %179
  %196 = bitcast float* %195 to <8 x float>*
  %wide.load35.1 = load <8 x float>, <8 x float>* %196, align 4, !tbaa !12, !llvm.access.group !16
  %197 = fsub <8 x float> %194, %wide.load35.1
  %198 = fpext <8 x float> %197 to <8 x double>
  %199 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %198, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %182)
  %200 = fptrunc <8 x double> %199 to <8 x float>
  %201 = bitcast float* %180 to <8 x float>*
  store <8 x float> %200, <8 x float>* %201, align 4, !tbaa !12, !llvm.access.group !16
  %202 = trunc i64 %mul.i.i.i to i32
  %203 = or i32 %202, 16
  %204 = add i32 %mul.i.i.1, %203
  %205 = sext i32 %204 to i64
  %206 = getelementptr inbounds float, float* %12, i64 %205
  %207 = bitcast float* %206 to <8 x float>*
  %wide.load31.2 = load <8 x float>, <8 x float>* %207, align 4, !tbaa !12, !llvm.access.group !16
  %208 = fpext <8 x float> %wide.load31.2 to <8 x double>
  %209 = add i32 %204, 1
  %210 = sext i32 %209 to i64
  %211 = getelementptr inbounds float, float* %6, i64 %210
  %212 = bitcast float* %211 to <8 x float>*
  %wide.load32.2 = load <8 x float>, <8 x float>* %212, align 4, !tbaa !12, !llvm.access.group !16
  %213 = getelementptr inbounds float, float* %6, i64 %205
  %214 = bitcast float* %213 to <8 x float>*
  %wide.load33.2 = load <8 x float>, <8 x float>* %214, align 4, !tbaa !12, !llvm.access.group !16
  %215 = fsub <8 x float> %wide.load32.2, %wide.load33.2
  %216 = add nsw i32 %mul14.i.i.1, %203
  %217 = sext i32 %216 to i64
  %218 = getelementptr inbounds float, float* %9, i64 %217
  %219 = bitcast float* %218 to <8 x float>*
  %wide.load34.2 = load <8 x float>, <8 x float>* %219, align 4, !tbaa !12, !llvm.access.group !16
  %220 = fadd <8 x float> %215, %wide.load34.2
  %221 = getelementptr inbounds float, float* %9, i64 %205
  %222 = bitcast float* %221 to <8 x float>*
  %wide.load35.2 = load <8 x float>, <8 x float>* %222, align 4, !tbaa !12, !llvm.access.group !16
  %223 = fsub <8 x float> %220, %wide.load35.2
  %224 = fpext <8 x float> %223 to <8 x double>
  %225 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %224, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %208)
  %226 = fptrunc <8 x double> %225 to <8 x float>
  %227 = bitcast float* %206 to <8 x float>*
  store <8 x float> %226, <8 x float>* %227, align 4, !tbaa !12, !llvm.access.group !16
  %228 = trunc i64 %mul.i.i.i to i32
  %229 = or i32 %228, 24
  %230 = add i32 %mul.i.i.1, %229
  %231 = sext i32 %230 to i64
  %232 = getelementptr inbounds float, float* %12, i64 %231
  %233 = bitcast float* %232 to <8 x float>*
  %wide.load31.3 = load <8 x float>, <8 x float>* %233, align 4, !tbaa !12, !llvm.access.group !16
  %234 = fpext <8 x float> %wide.load31.3 to <8 x double>
  %235 = add i32 %230, 1
  %236 = sext i32 %235 to i64
  %237 = getelementptr inbounds float, float* %6, i64 %236
  %238 = bitcast float* %237 to <8 x float>*
  %wide.load32.3 = load <8 x float>, <8 x float>* %238, align 4, !tbaa !12, !llvm.access.group !16
  %239 = getelementptr inbounds float, float* %6, i64 %231
  %240 = bitcast float* %239 to <8 x float>*
  %wide.load33.3 = load <8 x float>, <8 x float>* %240, align 4, !tbaa !12, !llvm.access.group !16
  %241 = fsub <8 x float> %wide.load32.3, %wide.load33.3
  %242 = add nsw i32 %mul14.i.i.1, %229
  %243 = sext i32 %242 to i64
  %244 = getelementptr inbounds float, float* %9, i64 %243
  %245 = bitcast float* %244 to <8 x float>*
  %wide.load34.3 = load <8 x float>, <8 x float>* %245, align 4, !tbaa !12, !llvm.access.group !16
  %246 = fadd <8 x float> %241, %wide.load34.3
  %247 = getelementptr inbounds float, float* %9, i64 %231
  %248 = bitcast float* %247 to <8 x float>*
  %wide.load35.3 = load <8 x float>, <8 x float>* %248, align 4, !tbaa !12, !llvm.access.group !16
  %249 = fsub <8 x float> %246, %wide.load35.3
  %250 = fpext <8 x float> %249 to <8 x double>
  %251 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %250, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %234)
  %252 = fptrunc <8 x double> %251 to <8 x float>
  %253 = bitcast float* %232 to <8 x float>*
  store <8 x float> %252, <8 x float>* %253, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.1

pregion_for_entry.entry.i.i:                      ; preds = %pregion_for_entry.entry.i.i, %pregion_for_entry.entry.i.i.preheader
  %_local_id_x.i.0 = phi i64 [ %267, %pregion_for_entry.entry.i.i ], [ 0, %pregion_for_entry.entry.i.i.preheader ]
  %add1.i.i.i = add nuw nsw i64 %_local_id_x.i.0, %mul.i.i.i
  %conv.i.i = trunc i64 %add1.i.i.i to i32
  %add.i.i = add i32 %mul.i.i, %conv.i.i
  %idxprom.i.i = sext i32 %add.i.i to i64
  %arrayidx.i.i = getelementptr inbounds float, float* %12, i64 %idxprom.i.i
  %254 = load float, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i = fpext float %254 to double
  %add6.i.i = or i32 %add.i.i, 1
  %idxprom7.i.i = sext i32 %add6.i.i to i64
  %arrayidx8.i.i = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i
  %255 = load float, float* %arrayidx8.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i = getelementptr inbounds float, float* %6, i64 %idxprom.i.i
  %256 = load float, float* %arrayidx12.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i = fsub float %255, %256
  %add15.i.i = add nsw i32 %mul14.i.i, %conv.i.i
  %idxprom16.i.i = sext i32 %add15.i.i to i64
  %arrayidx17.i.i = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i
  %257 = load float, float* %arrayidx17.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i = fadd float %sub.i.i, %257
  %arrayidx22.i.i = getelementptr inbounds float, float* %9, i64 %idxprom.i.i
  %258 = load float, float* %arrayidx22.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i = fsub float %add18.i.i, %258
  %conv24.i.i = fpext float %sub23.i.i to double
  %259 = tail call double @llvm.fmuladd.f64(double %conv24.i.i, double 0xBFE6666666666666, double %conv3.i.i) #3
  %conv26.i.i = fptrunc double %259 to float
  store float %conv26.i.i, float* %arrayidx.i.i, align 4, !tbaa !12, !llvm.access.group !16
  %260 = or i64 %_local_id_x.i.0, 1
  %add1.i.i.i.1189 = add nuw nsw i64 %260, %mul.i.i.i
  %conv.i.i.1190 = trunc i64 %add1.i.i.i.1189 to i32
  %add.i.i.1191 = add i32 %mul.i.i, %conv.i.i.1190
  %idxprom.i.i.1192 = sext i32 %add.i.i.1191 to i64
  %arrayidx.i.i.1193 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.1192
  %261 = load float, float* %arrayidx.i.i.1193, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.1194 = fpext float %261 to double
  %add6.i.i.1195 = add i32 %add.i.i.1191, 1
  %idxprom7.i.i.1196 = sext i32 %add6.i.i.1195 to i64
  %arrayidx8.i.i.1197 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.1196
  %262 = load float, float* %arrayidx8.i.i.1197, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.1198 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.1192
  %263 = load float, float* %arrayidx12.i.i.1198, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.1199 = fsub float %262, %263
  %add15.i.i.1200 = add nsw i32 %mul14.i.i, %conv.i.i.1190
  %idxprom16.i.i.1201 = sext i32 %add15.i.i.1200 to i64
  %arrayidx17.i.i.1202 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.1201
  %264 = load float, float* %arrayidx17.i.i.1202, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.1203 = fadd float %sub.i.i.1199, %264
  %arrayidx22.i.i.1204 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.1192
  %265 = load float, float* %arrayidx22.i.i.1204, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.1205 = fsub float %add18.i.i.1203, %265
  %conv24.i.i.1206 = fpext float %sub23.i.i.1205 to double
  %266 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.1206, double 0xBFE6666666666666, double %conv3.i.i.1194) #3
  %conv26.i.i.1207 = fptrunc double %266 to float
  store float %conv26.i.i.1207, float* %arrayidx.i.i.1193, align 4, !tbaa !12, !llvm.access.group !16
  %267 = add nuw nsw i64 %_local_id_x.i.0, 2
  %exitcond.not.1 = icmp eq i64 %267, 32
  br i1 %exitcond.not.1, label %pregion_for_end.i.i.loopexit, label %pregion_for_entry.entry.i.i, !llvm.loop !37

pregion_for_entry.entry.i.i.1:                    ; preds = %pregion_for_entry.entry.i.i.1, %pregion_for_entry.entry.i.i.1.preheader
  %_local_id_x.i.0.1 = phi i64 [ %281, %pregion_for_entry.entry.i.i.1 ], [ 0, %pregion_for_entry.entry.i.i.1.preheader ]
  %add1.i.i.i.1 = add nuw nsw i64 %_local_id_x.i.0.1, %mul.i.i.i
  %conv.i.i.1 = trunc i64 %add1.i.i.i.1 to i32
  %add.i.i.1 = add i32 %mul.i.i.1, %conv.i.i.1
  %idxprom.i.i.1 = sext i32 %add.i.i.1 to i64
  %arrayidx.i.i.1 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.1
  %268 = load float, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.1 = fpext float %268 to double
  %add6.i.i.1 = add i32 %add.i.i.1, 1
  %idxprom7.i.i.1 = sext i32 %add6.i.i.1 to i64
  %arrayidx8.i.i.1 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.1
  %269 = load float, float* %arrayidx8.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.1 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.1
  %270 = load float, float* %arrayidx12.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.1 = fsub float %269, %270
  %add15.i.i.1 = add nsw i32 %mul14.i.i.1, %conv.i.i.1
  %idxprom16.i.i.1 = sext i32 %add15.i.i.1 to i64
  %arrayidx17.i.i.1 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.1
  %271 = load float, float* %arrayidx17.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.1 = fadd float %sub.i.i.1, %271
  %arrayidx22.i.i.1 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.1
  %272 = load float, float* %arrayidx22.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.1 = fsub float %add18.i.i.1, %272
  %conv24.i.i.1 = fpext float %sub23.i.i.1 to double
  %273 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.1, double 0xBFE6666666666666, double %conv3.i.i.1) #3
  %conv26.i.i.1 = fptrunc double %273 to float
  store float %conv26.i.i.1, float* %arrayidx.i.i.1, align 4, !tbaa !12, !llvm.access.group !16
  %274 = or i64 %_local_id_x.i.0.1, 1
  %add1.i.i.i.1.1 = add nuw nsw i64 %274, %mul.i.i.i
  %conv.i.i.1.1 = trunc i64 %add1.i.i.i.1.1 to i32
  %add.i.i.1.1 = add i32 %mul.i.i.1, %conv.i.i.1.1
  %idxprom.i.i.1.1 = sext i32 %add.i.i.1.1 to i64
  %arrayidx.i.i.1.1 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.1.1
  %275 = load float, float* %arrayidx.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.1.1 = fpext float %275 to double
  %add6.i.i.1.1 = add i32 %add.i.i.1.1, 1
  %idxprom7.i.i.1.1 = sext i32 %add6.i.i.1.1 to i64
  %arrayidx8.i.i.1.1 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.1.1
  %276 = load float, float* %arrayidx8.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.1.1 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.1.1
  %277 = load float, float* %arrayidx12.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.1.1 = fsub float %276, %277
  %add15.i.i.1.1 = add nsw i32 %mul14.i.i.1, %conv.i.i.1.1
  %idxprom16.i.i.1.1 = sext i32 %add15.i.i.1.1 to i64
  %arrayidx17.i.i.1.1 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.1.1
  %278 = load float, float* %arrayidx17.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.1.1 = fadd float %sub.i.i.1.1, %278
  %arrayidx22.i.i.1.1 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.1.1
  %279 = load float, float* %arrayidx22.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.1.1 = fsub float %add18.i.i.1.1, %279
  %conv24.i.i.1.1 = fpext float %sub23.i.i.1.1 to double
  %280 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.1.1, double 0xBFE6666666666666, double %conv3.i.i.1.1) #3
  %conv26.i.i.1.1 = fptrunc double %280 to float
  store float %conv26.i.i.1.1, float* %arrayidx.i.i.1.1, align 4, !tbaa !12, !llvm.access.group !16
  %281 = add nuw nsw i64 %_local_id_x.i.0.1, 2
  %exitcond.1.not.1 = icmp eq i64 %281, 32
  br i1 %exitcond.1.not.1, label %pregion_for_end.i.i.1.loopexit, label %pregion_for_entry.entry.i.i.1, !llvm.loop !38

pregion_for_end.i.i.1.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.1
  br label %pregion_for_end.i.i.1

pregion_for_end.i.i.1:                            ; preds = %pregion_for_end.i.i.1.loopexit, %vector.body14
  %282 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.2 = or i32 %282, 2
  %mul.i.i.2 = mul nsw i32 %16, %conv2.i.i.2
  %add13.i.i.2 = or i32 %282, 3
  %mul14.i.i.2 = mul nsw i32 %add13.i.i.2, %16
  %283 = mul i32 %16, %conv2.i.i.2
  %284 = trunc i64 %2 to i32
  %285 = shl i32 %284, 5
  %286 = add nsw i32 %283, %285
  %287 = icmp sgt i32 %286, 2147483616
  %288 = add i32 %283, %285
  %289 = or i32 %288, 1
  %290 = icmp sgt i32 %289, 2147483616
  %291 = or i1 %287, %290
  %292 = mul i32 %16, %add13.i.i.2
  %293 = add nsw i32 %292, %285
  %294 = icmp sgt i32 %293, 2147483616
  %295 = or i1 %291, %294
  br i1 %295, label %pregion_for_entry.entry.i.i.2.preheader, label %vector.body38

pregion_for_entry.entry.i.i.2.preheader:          ; preds = %pregion_for_end.i.i.1
  br label %pregion_for_entry.entry.i.i.2

vector.body38:                                    ; preds = %pregion_for_end.i.i.1
  %296 = trunc i64 %mul.i.i.i to i32
  %297 = add i32 %mul.i.i.2, %296
  %298 = sext i32 %297 to i64
  %299 = getelementptr inbounds float, float* %12, i64 %298
  %300 = bitcast float* %299 to <8 x float>*
  %wide.load55 = load <8 x float>, <8 x float>* %300, align 4, !tbaa !12, !llvm.access.group !16
  %301 = fpext <8 x float> %wide.load55 to <8 x double>
  %302 = or i32 %297, 1
  %303 = sext i32 %302 to i64
  %304 = getelementptr inbounds float, float* %6, i64 %303
  %305 = bitcast float* %304 to <8 x float>*
  %wide.load56 = load <8 x float>, <8 x float>* %305, align 4, !tbaa !12, !llvm.access.group !16
  %306 = getelementptr inbounds float, float* %6, i64 %298
  %307 = bitcast float* %306 to <8 x float>*
  %wide.load57 = load <8 x float>, <8 x float>* %307, align 4, !tbaa !12, !llvm.access.group !16
  %308 = fsub <8 x float> %wide.load56, %wide.load57
  %309 = add nsw i32 %mul14.i.i.2, %296
  %310 = sext i32 %309 to i64
  %311 = getelementptr inbounds float, float* %9, i64 %310
  %312 = bitcast float* %311 to <8 x float>*
  %wide.load58 = load <8 x float>, <8 x float>* %312, align 4, !tbaa !12, !llvm.access.group !16
  %313 = fadd <8 x float> %308, %wide.load58
  %314 = getelementptr inbounds float, float* %9, i64 %298
  %315 = bitcast float* %314 to <8 x float>*
  %wide.load59 = load <8 x float>, <8 x float>* %315, align 4, !tbaa !12, !llvm.access.group !16
  %316 = fsub <8 x float> %313, %wide.load59
  %317 = fpext <8 x float> %316 to <8 x double>
  %318 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %317, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %301)
  %319 = fptrunc <8 x double> %318 to <8 x float>
  %320 = bitcast float* %299 to <8 x float>*
  store <8 x float> %319, <8 x float>* %320, align 4, !tbaa !12, !llvm.access.group !16
  %321 = trunc i64 %mul.i.i.i to i32
  %322 = or i32 %321, 8
  %323 = add i32 %mul.i.i.2, %322
  %324 = sext i32 %323 to i64
  %325 = getelementptr inbounds float, float* %12, i64 %324
  %326 = bitcast float* %325 to <8 x float>*
  %wide.load55.1 = load <8 x float>, <8 x float>* %326, align 4, !tbaa !12, !llvm.access.group !16
  %327 = fpext <8 x float> %wide.load55.1 to <8 x double>
  %328 = or i32 %323, 1
  %329 = sext i32 %328 to i64
  %330 = getelementptr inbounds float, float* %6, i64 %329
  %331 = bitcast float* %330 to <8 x float>*
  %wide.load56.1 = load <8 x float>, <8 x float>* %331, align 4, !tbaa !12, !llvm.access.group !16
  %332 = getelementptr inbounds float, float* %6, i64 %324
  %333 = bitcast float* %332 to <8 x float>*
  %wide.load57.1 = load <8 x float>, <8 x float>* %333, align 4, !tbaa !12, !llvm.access.group !16
  %334 = fsub <8 x float> %wide.load56.1, %wide.load57.1
  %335 = add nsw i32 %mul14.i.i.2, %322
  %336 = sext i32 %335 to i64
  %337 = getelementptr inbounds float, float* %9, i64 %336
  %338 = bitcast float* %337 to <8 x float>*
  %wide.load58.1 = load <8 x float>, <8 x float>* %338, align 4, !tbaa !12, !llvm.access.group !16
  %339 = fadd <8 x float> %334, %wide.load58.1
  %340 = getelementptr inbounds float, float* %9, i64 %324
  %341 = bitcast float* %340 to <8 x float>*
  %wide.load59.1 = load <8 x float>, <8 x float>* %341, align 4, !tbaa !12, !llvm.access.group !16
  %342 = fsub <8 x float> %339, %wide.load59.1
  %343 = fpext <8 x float> %342 to <8 x double>
  %344 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %343, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %327)
  %345 = fptrunc <8 x double> %344 to <8 x float>
  %346 = bitcast float* %325 to <8 x float>*
  store <8 x float> %345, <8 x float>* %346, align 4, !tbaa !12, !llvm.access.group !16
  %347 = trunc i64 %mul.i.i.i to i32
  %348 = or i32 %347, 16
  %349 = add i32 %mul.i.i.2, %348
  %350 = sext i32 %349 to i64
  %351 = getelementptr inbounds float, float* %12, i64 %350
  %352 = bitcast float* %351 to <8 x float>*
  %wide.load55.2 = load <8 x float>, <8 x float>* %352, align 4, !tbaa !12, !llvm.access.group !16
  %353 = fpext <8 x float> %wide.load55.2 to <8 x double>
  %354 = or i32 %349, 1
  %355 = sext i32 %354 to i64
  %356 = getelementptr inbounds float, float* %6, i64 %355
  %357 = bitcast float* %356 to <8 x float>*
  %wide.load56.2 = load <8 x float>, <8 x float>* %357, align 4, !tbaa !12, !llvm.access.group !16
  %358 = getelementptr inbounds float, float* %6, i64 %350
  %359 = bitcast float* %358 to <8 x float>*
  %wide.load57.2 = load <8 x float>, <8 x float>* %359, align 4, !tbaa !12, !llvm.access.group !16
  %360 = fsub <8 x float> %wide.load56.2, %wide.load57.2
  %361 = add nsw i32 %mul14.i.i.2, %348
  %362 = sext i32 %361 to i64
  %363 = getelementptr inbounds float, float* %9, i64 %362
  %364 = bitcast float* %363 to <8 x float>*
  %wide.load58.2 = load <8 x float>, <8 x float>* %364, align 4, !tbaa !12, !llvm.access.group !16
  %365 = fadd <8 x float> %360, %wide.load58.2
  %366 = getelementptr inbounds float, float* %9, i64 %350
  %367 = bitcast float* %366 to <8 x float>*
  %wide.load59.2 = load <8 x float>, <8 x float>* %367, align 4, !tbaa !12, !llvm.access.group !16
  %368 = fsub <8 x float> %365, %wide.load59.2
  %369 = fpext <8 x float> %368 to <8 x double>
  %370 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %369, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %353)
  %371 = fptrunc <8 x double> %370 to <8 x float>
  %372 = bitcast float* %351 to <8 x float>*
  store <8 x float> %371, <8 x float>* %372, align 4, !tbaa !12, !llvm.access.group !16
  %373 = trunc i64 %mul.i.i.i to i32
  %374 = or i32 %373, 24
  %375 = add i32 %mul.i.i.2, %374
  %376 = sext i32 %375 to i64
  %377 = getelementptr inbounds float, float* %12, i64 %376
  %378 = bitcast float* %377 to <8 x float>*
  %wide.load55.3 = load <8 x float>, <8 x float>* %378, align 4, !tbaa !12, !llvm.access.group !16
  %379 = fpext <8 x float> %wide.load55.3 to <8 x double>
  %380 = or i32 %375, 1
  %381 = sext i32 %380 to i64
  %382 = getelementptr inbounds float, float* %6, i64 %381
  %383 = bitcast float* %382 to <8 x float>*
  %wide.load56.3 = load <8 x float>, <8 x float>* %383, align 4, !tbaa !12, !llvm.access.group !16
  %384 = getelementptr inbounds float, float* %6, i64 %376
  %385 = bitcast float* %384 to <8 x float>*
  %wide.load57.3 = load <8 x float>, <8 x float>* %385, align 4, !tbaa !12, !llvm.access.group !16
  %386 = fsub <8 x float> %wide.load56.3, %wide.load57.3
  %387 = add nsw i32 %mul14.i.i.2, %374
  %388 = sext i32 %387 to i64
  %389 = getelementptr inbounds float, float* %9, i64 %388
  %390 = bitcast float* %389 to <8 x float>*
  %wide.load58.3 = load <8 x float>, <8 x float>* %390, align 4, !tbaa !12, !llvm.access.group !16
  %391 = fadd <8 x float> %386, %wide.load58.3
  %392 = getelementptr inbounds float, float* %9, i64 %376
  %393 = bitcast float* %392 to <8 x float>*
  %wide.load59.3 = load <8 x float>, <8 x float>* %393, align 4, !tbaa !12, !llvm.access.group !16
  %394 = fsub <8 x float> %391, %wide.load59.3
  %395 = fpext <8 x float> %394 to <8 x double>
  %396 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %395, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %379)
  %397 = fptrunc <8 x double> %396 to <8 x float>
  %398 = bitcast float* %377 to <8 x float>*
  store <8 x float> %397, <8 x float>* %398, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.2

pregion_for_entry.entry.i.i.2:                    ; preds = %pregion_for_entry.entry.i.i.2, %pregion_for_entry.entry.i.i.2.preheader
  %_local_id_x.i.0.2 = phi i64 [ %412, %pregion_for_entry.entry.i.i.2 ], [ 0, %pregion_for_entry.entry.i.i.2.preheader ]
  %add1.i.i.i.2 = add nuw nsw i64 %_local_id_x.i.0.2, %mul.i.i.i
  %conv.i.i.2 = trunc i64 %add1.i.i.i.2 to i32
  %add.i.i.2 = add i32 %mul.i.i.2, %conv.i.i.2
  %idxprom.i.i.2 = sext i32 %add.i.i.2 to i64
  %arrayidx.i.i.2 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.2
  %399 = load float, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.2 = fpext float %399 to double
  %add6.i.i.2 = or i32 %add.i.i.2, 1
  %idxprom7.i.i.2 = sext i32 %add6.i.i.2 to i64
  %arrayidx8.i.i.2 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.2
  %400 = load float, float* %arrayidx8.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.2 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.2
  %401 = load float, float* %arrayidx12.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.2 = fsub float %400, %401
  %add15.i.i.2 = add nsw i32 %mul14.i.i.2, %conv.i.i.2
  %idxprom16.i.i.2 = sext i32 %add15.i.i.2 to i64
  %arrayidx17.i.i.2 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.2
  %402 = load float, float* %arrayidx17.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.2 = fadd float %sub.i.i.2, %402
  %arrayidx22.i.i.2 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.2
  %403 = load float, float* %arrayidx22.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.2 = fsub float %add18.i.i.2, %403
  %conv24.i.i.2 = fpext float %sub23.i.i.2 to double
  %404 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.2, double 0xBFE6666666666666, double %conv3.i.i.2) #3
  %conv26.i.i.2 = fptrunc double %404 to float
  store float %conv26.i.i.2, float* %arrayidx.i.i.2, align 4, !tbaa !12, !llvm.access.group !16
  %405 = or i64 %_local_id_x.i.0.2, 1
  %add1.i.i.i.2.1 = add nuw nsw i64 %405, %mul.i.i.i
  %conv.i.i.2.1 = trunc i64 %add1.i.i.i.2.1 to i32
  %add.i.i.2.1 = add i32 %mul.i.i.2, %conv.i.i.2.1
  %idxprom.i.i.2.1 = sext i32 %add.i.i.2.1 to i64
  %arrayidx.i.i.2.1 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.2.1
  %406 = load float, float* %arrayidx.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.2.1 = fpext float %406 to double
  %add6.i.i.2.1 = add i32 %add.i.i.2.1, 1
  %idxprom7.i.i.2.1 = sext i32 %add6.i.i.2.1 to i64
  %arrayidx8.i.i.2.1 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.2.1
  %407 = load float, float* %arrayidx8.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.2.1 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.2.1
  %408 = load float, float* %arrayidx12.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.2.1 = fsub float %407, %408
  %add15.i.i.2.1 = add nsw i32 %mul14.i.i.2, %conv.i.i.2.1
  %idxprom16.i.i.2.1 = sext i32 %add15.i.i.2.1 to i64
  %arrayidx17.i.i.2.1 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.2.1
  %409 = load float, float* %arrayidx17.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.2.1 = fadd float %sub.i.i.2.1, %409
  %arrayidx22.i.i.2.1 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.2.1
  %410 = load float, float* %arrayidx22.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.2.1 = fsub float %add18.i.i.2.1, %410
  %conv24.i.i.2.1 = fpext float %sub23.i.i.2.1 to double
  %411 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.2.1, double 0xBFE6666666666666, double %conv3.i.i.2.1) #3
  %conv26.i.i.2.1 = fptrunc double %411 to float
  store float %conv26.i.i.2.1, float* %arrayidx.i.i.2.1, align 4, !tbaa !12, !llvm.access.group !16
  %412 = add nuw nsw i64 %_local_id_x.i.0.2, 2
  %exitcond.2.not.1 = icmp eq i64 %412, 32
  br i1 %exitcond.2.not.1, label %pregion_for_end.i.i.2.loopexit, label %pregion_for_entry.entry.i.i.2, !llvm.loop !39

pregion_for_end.i.i.2.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.2
  br label %pregion_for_end.i.i.2

pregion_for_end.i.i.2:                            ; preds = %pregion_for_end.i.i.2.loopexit, %vector.body38
  %413 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.3 = or i32 %413, 3
  %mul.i.i.3 = mul nsw i32 %16, %conv2.i.i.3
  %add13.i.i.3 = add nuw nsw i32 %conv2.i.i.3, 1
  %mul14.i.i.3 = mul nsw i32 %add13.i.i.3, %16
  %414 = mul i32 %16, %add13.i.i.2
  %415 = trunc i64 %2 to i32
  %416 = shl i32 %415, 5
  %417 = add nsw i32 %414, %416
  %418 = icmp sgt i32 %417, 2147483616
  %419 = add i32 %414, %416
  %420 = add i32 %419, 1
  %421 = add i32 %419, 32
  %422 = icmp slt i32 %421, %420
  %423 = or i1 %418, %422
  %424 = or i32 %conv2.i.i, 4
  %425 = mul i32 %16, %424
  %426 = add nsw i32 %425, %416
  %427 = icmp sgt i32 %426, 2147483616
  %428 = or i1 %423, %427
  br i1 %428, label %pregion_for_entry.entry.i.i.3.preheader, label %vector.body62

pregion_for_entry.entry.i.i.3.preheader:          ; preds = %pregion_for_end.i.i.2
  br label %pregion_for_entry.entry.i.i.3

vector.body62:                                    ; preds = %pregion_for_end.i.i.2
  %429 = trunc i64 %mul.i.i.i to i32
  %430 = add i32 %mul.i.i.3, %429
  %431 = sext i32 %430 to i64
  %432 = getelementptr inbounds float, float* %12, i64 %431
  %433 = bitcast float* %432 to <8 x float>*
  %wide.load79 = load <8 x float>, <8 x float>* %433, align 4, !tbaa !12, !llvm.access.group !16
  %434 = fpext <8 x float> %wide.load79 to <8 x double>
  %435 = add i32 %430, 1
  %436 = sext i32 %435 to i64
  %437 = getelementptr inbounds float, float* %6, i64 %436
  %438 = bitcast float* %437 to <8 x float>*
  %wide.load80 = load <8 x float>, <8 x float>* %438, align 4, !tbaa !12, !llvm.access.group !16
  %439 = getelementptr inbounds float, float* %6, i64 %431
  %440 = bitcast float* %439 to <8 x float>*
  %wide.load81 = load <8 x float>, <8 x float>* %440, align 4, !tbaa !12, !llvm.access.group !16
  %441 = fsub <8 x float> %wide.load80, %wide.load81
  %442 = add nsw i32 %mul14.i.i.3, %429
  %443 = sext i32 %442 to i64
  %444 = getelementptr inbounds float, float* %9, i64 %443
  %445 = bitcast float* %444 to <8 x float>*
  %wide.load82 = load <8 x float>, <8 x float>* %445, align 4, !tbaa !12, !llvm.access.group !16
  %446 = fadd <8 x float> %441, %wide.load82
  %447 = getelementptr inbounds float, float* %9, i64 %431
  %448 = bitcast float* %447 to <8 x float>*
  %wide.load83 = load <8 x float>, <8 x float>* %448, align 4, !tbaa !12, !llvm.access.group !16
  %449 = fsub <8 x float> %446, %wide.load83
  %450 = fpext <8 x float> %449 to <8 x double>
  %451 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %450, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %434)
  %452 = fptrunc <8 x double> %451 to <8 x float>
  %453 = bitcast float* %432 to <8 x float>*
  store <8 x float> %452, <8 x float>* %453, align 4, !tbaa !12, !llvm.access.group !16
  %454 = trunc i64 %mul.i.i.i to i32
  %455 = or i32 %454, 8
  %456 = add i32 %mul.i.i.3, %455
  %457 = sext i32 %456 to i64
  %458 = getelementptr inbounds float, float* %12, i64 %457
  %459 = bitcast float* %458 to <8 x float>*
  %wide.load79.1 = load <8 x float>, <8 x float>* %459, align 4, !tbaa !12, !llvm.access.group !16
  %460 = fpext <8 x float> %wide.load79.1 to <8 x double>
  %461 = add i32 %456, 1
  %462 = sext i32 %461 to i64
  %463 = getelementptr inbounds float, float* %6, i64 %462
  %464 = bitcast float* %463 to <8 x float>*
  %wide.load80.1 = load <8 x float>, <8 x float>* %464, align 4, !tbaa !12, !llvm.access.group !16
  %465 = getelementptr inbounds float, float* %6, i64 %457
  %466 = bitcast float* %465 to <8 x float>*
  %wide.load81.1 = load <8 x float>, <8 x float>* %466, align 4, !tbaa !12, !llvm.access.group !16
  %467 = fsub <8 x float> %wide.load80.1, %wide.load81.1
  %468 = add nsw i32 %mul14.i.i.3, %455
  %469 = sext i32 %468 to i64
  %470 = getelementptr inbounds float, float* %9, i64 %469
  %471 = bitcast float* %470 to <8 x float>*
  %wide.load82.1 = load <8 x float>, <8 x float>* %471, align 4, !tbaa !12, !llvm.access.group !16
  %472 = fadd <8 x float> %467, %wide.load82.1
  %473 = getelementptr inbounds float, float* %9, i64 %457
  %474 = bitcast float* %473 to <8 x float>*
  %wide.load83.1 = load <8 x float>, <8 x float>* %474, align 4, !tbaa !12, !llvm.access.group !16
  %475 = fsub <8 x float> %472, %wide.load83.1
  %476 = fpext <8 x float> %475 to <8 x double>
  %477 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %476, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %460)
  %478 = fptrunc <8 x double> %477 to <8 x float>
  %479 = bitcast float* %458 to <8 x float>*
  store <8 x float> %478, <8 x float>* %479, align 4, !tbaa !12, !llvm.access.group !16
  %480 = trunc i64 %mul.i.i.i to i32
  %481 = or i32 %480, 16
  %482 = add i32 %mul.i.i.3, %481
  %483 = sext i32 %482 to i64
  %484 = getelementptr inbounds float, float* %12, i64 %483
  %485 = bitcast float* %484 to <8 x float>*
  %wide.load79.2 = load <8 x float>, <8 x float>* %485, align 4, !tbaa !12, !llvm.access.group !16
  %486 = fpext <8 x float> %wide.load79.2 to <8 x double>
  %487 = add i32 %482, 1
  %488 = sext i32 %487 to i64
  %489 = getelementptr inbounds float, float* %6, i64 %488
  %490 = bitcast float* %489 to <8 x float>*
  %wide.load80.2 = load <8 x float>, <8 x float>* %490, align 4, !tbaa !12, !llvm.access.group !16
  %491 = getelementptr inbounds float, float* %6, i64 %483
  %492 = bitcast float* %491 to <8 x float>*
  %wide.load81.2 = load <8 x float>, <8 x float>* %492, align 4, !tbaa !12, !llvm.access.group !16
  %493 = fsub <8 x float> %wide.load80.2, %wide.load81.2
  %494 = add nsw i32 %mul14.i.i.3, %481
  %495 = sext i32 %494 to i64
  %496 = getelementptr inbounds float, float* %9, i64 %495
  %497 = bitcast float* %496 to <8 x float>*
  %wide.load82.2 = load <8 x float>, <8 x float>* %497, align 4, !tbaa !12, !llvm.access.group !16
  %498 = fadd <8 x float> %493, %wide.load82.2
  %499 = getelementptr inbounds float, float* %9, i64 %483
  %500 = bitcast float* %499 to <8 x float>*
  %wide.load83.2 = load <8 x float>, <8 x float>* %500, align 4, !tbaa !12, !llvm.access.group !16
  %501 = fsub <8 x float> %498, %wide.load83.2
  %502 = fpext <8 x float> %501 to <8 x double>
  %503 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %502, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %486)
  %504 = fptrunc <8 x double> %503 to <8 x float>
  %505 = bitcast float* %484 to <8 x float>*
  store <8 x float> %504, <8 x float>* %505, align 4, !tbaa !12, !llvm.access.group !16
  %506 = trunc i64 %mul.i.i.i to i32
  %507 = or i32 %506, 24
  %508 = add i32 %mul.i.i.3, %507
  %509 = sext i32 %508 to i64
  %510 = getelementptr inbounds float, float* %12, i64 %509
  %511 = bitcast float* %510 to <8 x float>*
  %wide.load79.3 = load <8 x float>, <8 x float>* %511, align 4, !tbaa !12, !llvm.access.group !16
  %512 = fpext <8 x float> %wide.load79.3 to <8 x double>
  %513 = add i32 %508, 1
  %514 = sext i32 %513 to i64
  %515 = getelementptr inbounds float, float* %6, i64 %514
  %516 = bitcast float* %515 to <8 x float>*
  %wide.load80.3 = load <8 x float>, <8 x float>* %516, align 4, !tbaa !12, !llvm.access.group !16
  %517 = getelementptr inbounds float, float* %6, i64 %509
  %518 = bitcast float* %517 to <8 x float>*
  %wide.load81.3 = load <8 x float>, <8 x float>* %518, align 4, !tbaa !12, !llvm.access.group !16
  %519 = fsub <8 x float> %wide.load80.3, %wide.load81.3
  %520 = add nsw i32 %mul14.i.i.3, %507
  %521 = sext i32 %520 to i64
  %522 = getelementptr inbounds float, float* %9, i64 %521
  %523 = bitcast float* %522 to <8 x float>*
  %wide.load82.3 = load <8 x float>, <8 x float>* %523, align 4, !tbaa !12, !llvm.access.group !16
  %524 = fadd <8 x float> %519, %wide.load82.3
  %525 = getelementptr inbounds float, float* %9, i64 %509
  %526 = bitcast float* %525 to <8 x float>*
  %wide.load83.3 = load <8 x float>, <8 x float>* %526, align 4, !tbaa !12, !llvm.access.group !16
  %527 = fsub <8 x float> %524, %wide.load83.3
  %528 = fpext <8 x float> %527 to <8 x double>
  %529 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %528, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %512)
  %530 = fptrunc <8 x double> %529 to <8 x float>
  %531 = bitcast float* %510 to <8 x float>*
  store <8 x float> %530, <8 x float>* %531, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.3

pregion_for_entry.entry.i.i.3:                    ; preds = %pregion_for_entry.entry.i.i.3, %pregion_for_entry.entry.i.i.3.preheader
  %_local_id_x.i.0.3 = phi i64 [ %545, %pregion_for_entry.entry.i.i.3 ], [ 0, %pregion_for_entry.entry.i.i.3.preheader ]
  %add1.i.i.i.3 = add nuw nsw i64 %_local_id_x.i.0.3, %mul.i.i.i
  %conv.i.i.3 = trunc i64 %add1.i.i.i.3 to i32
  %add.i.i.3 = add i32 %mul.i.i.3, %conv.i.i.3
  %idxprom.i.i.3 = sext i32 %add.i.i.3 to i64
  %arrayidx.i.i.3 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.3
  %532 = load float, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.3 = fpext float %532 to double
  %add6.i.i.3 = add i32 %add.i.i.3, 1
  %idxprom7.i.i.3 = sext i32 %add6.i.i.3 to i64
  %arrayidx8.i.i.3 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.3
  %533 = load float, float* %arrayidx8.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.3 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.3
  %534 = load float, float* %arrayidx12.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.3 = fsub float %533, %534
  %add15.i.i.3 = add nsw i32 %mul14.i.i.3, %conv.i.i.3
  %idxprom16.i.i.3 = sext i32 %add15.i.i.3 to i64
  %arrayidx17.i.i.3 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.3
  %535 = load float, float* %arrayidx17.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.3 = fadd float %sub.i.i.3, %535
  %arrayidx22.i.i.3 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.3
  %536 = load float, float* %arrayidx22.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.3 = fsub float %add18.i.i.3, %536
  %conv24.i.i.3 = fpext float %sub23.i.i.3 to double
  %537 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.3, double 0xBFE6666666666666, double %conv3.i.i.3) #3
  %conv26.i.i.3 = fptrunc double %537 to float
  store float %conv26.i.i.3, float* %arrayidx.i.i.3, align 4, !tbaa !12, !llvm.access.group !16
  %538 = or i64 %_local_id_x.i.0.3, 1
  %add1.i.i.i.3.1 = add nuw nsw i64 %538, %mul.i.i.i
  %conv.i.i.3.1 = trunc i64 %add1.i.i.i.3.1 to i32
  %add.i.i.3.1 = add i32 %mul.i.i.3, %conv.i.i.3.1
  %idxprom.i.i.3.1 = sext i32 %add.i.i.3.1 to i64
  %arrayidx.i.i.3.1 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.3.1
  %539 = load float, float* %arrayidx.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.3.1 = fpext float %539 to double
  %add6.i.i.3.1 = add i32 %add.i.i.3.1, 1
  %idxprom7.i.i.3.1 = sext i32 %add6.i.i.3.1 to i64
  %arrayidx8.i.i.3.1 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.3.1
  %540 = load float, float* %arrayidx8.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.3.1 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.3.1
  %541 = load float, float* %arrayidx12.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.3.1 = fsub float %540, %541
  %add15.i.i.3.1 = add nsw i32 %mul14.i.i.3, %conv.i.i.3.1
  %idxprom16.i.i.3.1 = sext i32 %add15.i.i.3.1 to i64
  %arrayidx17.i.i.3.1 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.3.1
  %542 = load float, float* %arrayidx17.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.3.1 = fadd float %sub.i.i.3.1, %542
  %arrayidx22.i.i.3.1 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.3.1
  %543 = load float, float* %arrayidx22.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.3.1 = fsub float %add18.i.i.3.1, %543
  %conv24.i.i.3.1 = fpext float %sub23.i.i.3.1 to double
  %544 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.3.1, double 0xBFE6666666666666, double %conv3.i.i.3.1) #3
  %conv26.i.i.3.1 = fptrunc double %544 to float
  store float %conv26.i.i.3.1, float* %arrayidx.i.i.3.1, align 4, !tbaa !12, !llvm.access.group !16
  %545 = add nuw nsw i64 %_local_id_x.i.0.3, 2
  %exitcond.3.not.1 = icmp eq i64 %545, 32
  br i1 %exitcond.3.not.1, label %pregion_for_end.i.i.3.loopexit, label %pregion_for_entry.entry.i.i.3, !llvm.loop !40

pregion_for_end.i.i.3.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.3
  br label %pregion_for_end.i.i.3

pregion_for_end.i.i.3:                            ; preds = %pregion_for_end.i.i.3.loopexit, %vector.body62
  %546 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.4 = or i32 %546, 4
  %mul.i.i.4 = mul nsw i32 %16, %conv2.i.i.4
  %add13.i.i.4 = or i32 %546, 5
  %mul14.i.i.4 = mul nsw i32 %add13.i.i.4, %16
  %547 = mul i32 %16, %conv2.i.i.4
  %548 = trunc i64 %2 to i32
  %549 = shl i32 %548, 5
  %550 = add nsw i32 %547, %549
  %551 = icmp sgt i32 %550, 2147483616
  %552 = add i32 %547, %549
  %553 = or i32 %552, 1
  %554 = icmp sgt i32 %553, 2147483616
  %555 = or i1 %551, %554
  %556 = mul i32 %16, %add13.i.i.4
  %557 = add nsw i32 %556, %549
  %558 = icmp sgt i32 %557, 2147483616
  %559 = or i1 %555, %558
  br i1 %559, label %pregion_for_entry.entry.i.i.4.preheader, label %vector.body86

pregion_for_entry.entry.i.i.4.preheader:          ; preds = %pregion_for_end.i.i.3
  br label %pregion_for_entry.entry.i.i.4

vector.body86:                                    ; preds = %pregion_for_end.i.i.3
  %560 = trunc i64 %mul.i.i.i to i32
  %561 = add i32 %mul.i.i.4, %560
  %562 = sext i32 %561 to i64
  %563 = getelementptr inbounds float, float* %12, i64 %562
  %564 = bitcast float* %563 to <8 x float>*
  %wide.load103 = load <8 x float>, <8 x float>* %564, align 4, !tbaa !12, !llvm.access.group !16
  %565 = fpext <8 x float> %wide.load103 to <8 x double>
  %566 = or i32 %561, 1
  %567 = sext i32 %566 to i64
  %568 = getelementptr inbounds float, float* %6, i64 %567
  %569 = bitcast float* %568 to <8 x float>*
  %wide.load104 = load <8 x float>, <8 x float>* %569, align 4, !tbaa !12, !llvm.access.group !16
  %570 = getelementptr inbounds float, float* %6, i64 %562
  %571 = bitcast float* %570 to <8 x float>*
  %wide.load105 = load <8 x float>, <8 x float>* %571, align 4, !tbaa !12, !llvm.access.group !16
  %572 = fsub <8 x float> %wide.load104, %wide.load105
  %573 = add nsw i32 %mul14.i.i.4, %560
  %574 = sext i32 %573 to i64
  %575 = getelementptr inbounds float, float* %9, i64 %574
  %576 = bitcast float* %575 to <8 x float>*
  %wide.load106 = load <8 x float>, <8 x float>* %576, align 4, !tbaa !12, !llvm.access.group !16
  %577 = fadd <8 x float> %572, %wide.load106
  %578 = getelementptr inbounds float, float* %9, i64 %562
  %579 = bitcast float* %578 to <8 x float>*
  %wide.load107 = load <8 x float>, <8 x float>* %579, align 4, !tbaa !12, !llvm.access.group !16
  %580 = fsub <8 x float> %577, %wide.load107
  %581 = fpext <8 x float> %580 to <8 x double>
  %582 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %581, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %565)
  %583 = fptrunc <8 x double> %582 to <8 x float>
  %584 = bitcast float* %563 to <8 x float>*
  store <8 x float> %583, <8 x float>* %584, align 4, !tbaa !12, !llvm.access.group !16
  %585 = trunc i64 %mul.i.i.i to i32
  %586 = or i32 %585, 8
  %587 = add i32 %mul.i.i.4, %586
  %588 = sext i32 %587 to i64
  %589 = getelementptr inbounds float, float* %12, i64 %588
  %590 = bitcast float* %589 to <8 x float>*
  %wide.load103.1 = load <8 x float>, <8 x float>* %590, align 4, !tbaa !12, !llvm.access.group !16
  %591 = fpext <8 x float> %wide.load103.1 to <8 x double>
  %592 = or i32 %587, 1
  %593 = sext i32 %592 to i64
  %594 = getelementptr inbounds float, float* %6, i64 %593
  %595 = bitcast float* %594 to <8 x float>*
  %wide.load104.1 = load <8 x float>, <8 x float>* %595, align 4, !tbaa !12, !llvm.access.group !16
  %596 = getelementptr inbounds float, float* %6, i64 %588
  %597 = bitcast float* %596 to <8 x float>*
  %wide.load105.1 = load <8 x float>, <8 x float>* %597, align 4, !tbaa !12, !llvm.access.group !16
  %598 = fsub <8 x float> %wide.load104.1, %wide.load105.1
  %599 = add nsw i32 %mul14.i.i.4, %586
  %600 = sext i32 %599 to i64
  %601 = getelementptr inbounds float, float* %9, i64 %600
  %602 = bitcast float* %601 to <8 x float>*
  %wide.load106.1 = load <8 x float>, <8 x float>* %602, align 4, !tbaa !12, !llvm.access.group !16
  %603 = fadd <8 x float> %598, %wide.load106.1
  %604 = getelementptr inbounds float, float* %9, i64 %588
  %605 = bitcast float* %604 to <8 x float>*
  %wide.load107.1 = load <8 x float>, <8 x float>* %605, align 4, !tbaa !12, !llvm.access.group !16
  %606 = fsub <8 x float> %603, %wide.load107.1
  %607 = fpext <8 x float> %606 to <8 x double>
  %608 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %607, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %591)
  %609 = fptrunc <8 x double> %608 to <8 x float>
  %610 = bitcast float* %589 to <8 x float>*
  store <8 x float> %609, <8 x float>* %610, align 4, !tbaa !12, !llvm.access.group !16
  %611 = trunc i64 %mul.i.i.i to i32
  %612 = or i32 %611, 16
  %613 = add i32 %mul.i.i.4, %612
  %614 = sext i32 %613 to i64
  %615 = getelementptr inbounds float, float* %12, i64 %614
  %616 = bitcast float* %615 to <8 x float>*
  %wide.load103.2 = load <8 x float>, <8 x float>* %616, align 4, !tbaa !12, !llvm.access.group !16
  %617 = fpext <8 x float> %wide.load103.2 to <8 x double>
  %618 = or i32 %613, 1
  %619 = sext i32 %618 to i64
  %620 = getelementptr inbounds float, float* %6, i64 %619
  %621 = bitcast float* %620 to <8 x float>*
  %wide.load104.2 = load <8 x float>, <8 x float>* %621, align 4, !tbaa !12, !llvm.access.group !16
  %622 = getelementptr inbounds float, float* %6, i64 %614
  %623 = bitcast float* %622 to <8 x float>*
  %wide.load105.2 = load <8 x float>, <8 x float>* %623, align 4, !tbaa !12, !llvm.access.group !16
  %624 = fsub <8 x float> %wide.load104.2, %wide.load105.2
  %625 = add nsw i32 %mul14.i.i.4, %612
  %626 = sext i32 %625 to i64
  %627 = getelementptr inbounds float, float* %9, i64 %626
  %628 = bitcast float* %627 to <8 x float>*
  %wide.load106.2 = load <8 x float>, <8 x float>* %628, align 4, !tbaa !12, !llvm.access.group !16
  %629 = fadd <8 x float> %624, %wide.load106.2
  %630 = getelementptr inbounds float, float* %9, i64 %614
  %631 = bitcast float* %630 to <8 x float>*
  %wide.load107.2 = load <8 x float>, <8 x float>* %631, align 4, !tbaa !12, !llvm.access.group !16
  %632 = fsub <8 x float> %629, %wide.load107.2
  %633 = fpext <8 x float> %632 to <8 x double>
  %634 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %633, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %617)
  %635 = fptrunc <8 x double> %634 to <8 x float>
  %636 = bitcast float* %615 to <8 x float>*
  store <8 x float> %635, <8 x float>* %636, align 4, !tbaa !12, !llvm.access.group !16
  %637 = trunc i64 %mul.i.i.i to i32
  %638 = or i32 %637, 24
  %639 = add i32 %mul.i.i.4, %638
  %640 = sext i32 %639 to i64
  %641 = getelementptr inbounds float, float* %12, i64 %640
  %642 = bitcast float* %641 to <8 x float>*
  %wide.load103.3 = load <8 x float>, <8 x float>* %642, align 4, !tbaa !12, !llvm.access.group !16
  %643 = fpext <8 x float> %wide.load103.3 to <8 x double>
  %644 = or i32 %639, 1
  %645 = sext i32 %644 to i64
  %646 = getelementptr inbounds float, float* %6, i64 %645
  %647 = bitcast float* %646 to <8 x float>*
  %wide.load104.3 = load <8 x float>, <8 x float>* %647, align 4, !tbaa !12, !llvm.access.group !16
  %648 = getelementptr inbounds float, float* %6, i64 %640
  %649 = bitcast float* %648 to <8 x float>*
  %wide.load105.3 = load <8 x float>, <8 x float>* %649, align 4, !tbaa !12, !llvm.access.group !16
  %650 = fsub <8 x float> %wide.load104.3, %wide.load105.3
  %651 = add nsw i32 %mul14.i.i.4, %638
  %652 = sext i32 %651 to i64
  %653 = getelementptr inbounds float, float* %9, i64 %652
  %654 = bitcast float* %653 to <8 x float>*
  %wide.load106.3 = load <8 x float>, <8 x float>* %654, align 4, !tbaa !12, !llvm.access.group !16
  %655 = fadd <8 x float> %650, %wide.load106.3
  %656 = getelementptr inbounds float, float* %9, i64 %640
  %657 = bitcast float* %656 to <8 x float>*
  %wide.load107.3 = load <8 x float>, <8 x float>* %657, align 4, !tbaa !12, !llvm.access.group !16
  %658 = fsub <8 x float> %655, %wide.load107.3
  %659 = fpext <8 x float> %658 to <8 x double>
  %660 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %659, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %643)
  %661 = fptrunc <8 x double> %660 to <8 x float>
  %662 = bitcast float* %641 to <8 x float>*
  store <8 x float> %661, <8 x float>* %662, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.4

pregion_for_entry.entry.i.i.4:                    ; preds = %pregion_for_entry.entry.i.i.4, %pregion_for_entry.entry.i.i.4.preheader
  %_local_id_x.i.0.4 = phi i64 [ %676, %pregion_for_entry.entry.i.i.4 ], [ 0, %pregion_for_entry.entry.i.i.4.preheader ]
  %add1.i.i.i.4 = add nuw nsw i64 %_local_id_x.i.0.4, %mul.i.i.i
  %conv.i.i.4 = trunc i64 %add1.i.i.i.4 to i32
  %add.i.i.4 = add i32 %mul.i.i.4, %conv.i.i.4
  %idxprom.i.i.4 = sext i32 %add.i.i.4 to i64
  %arrayidx.i.i.4 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.4
  %663 = load float, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.4 = fpext float %663 to double
  %add6.i.i.4 = or i32 %add.i.i.4, 1
  %idxprom7.i.i.4 = sext i32 %add6.i.i.4 to i64
  %arrayidx8.i.i.4 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.4
  %664 = load float, float* %arrayidx8.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.4 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.4
  %665 = load float, float* %arrayidx12.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.4 = fsub float %664, %665
  %add15.i.i.4 = add nsw i32 %mul14.i.i.4, %conv.i.i.4
  %idxprom16.i.i.4 = sext i32 %add15.i.i.4 to i64
  %arrayidx17.i.i.4 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.4
  %666 = load float, float* %arrayidx17.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.4 = fadd float %sub.i.i.4, %666
  %arrayidx22.i.i.4 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.4
  %667 = load float, float* %arrayidx22.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.4 = fsub float %add18.i.i.4, %667
  %conv24.i.i.4 = fpext float %sub23.i.i.4 to double
  %668 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.4, double 0xBFE6666666666666, double %conv3.i.i.4) #3
  %conv26.i.i.4 = fptrunc double %668 to float
  store float %conv26.i.i.4, float* %arrayidx.i.i.4, align 4, !tbaa !12, !llvm.access.group !16
  %669 = or i64 %_local_id_x.i.0.4, 1
  %add1.i.i.i.4.1 = add nuw nsw i64 %669, %mul.i.i.i
  %conv.i.i.4.1 = trunc i64 %add1.i.i.i.4.1 to i32
  %add.i.i.4.1 = add i32 %mul.i.i.4, %conv.i.i.4.1
  %idxprom.i.i.4.1 = sext i32 %add.i.i.4.1 to i64
  %arrayidx.i.i.4.1 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.4.1
  %670 = load float, float* %arrayidx.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.4.1 = fpext float %670 to double
  %add6.i.i.4.1 = add i32 %add.i.i.4.1, 1
  %idxprom7.i.i.4.1 = sext i32 %add6.i.i.4.1 to i64
  %arrayidx8.i.i.4.1 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.4.1
  %671 = load float, float* %arrayidx8.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.4.1 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.4.1
  %672 = load float, float* %arrayidx12.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.4.1 = fsub float %671, %672
  %add15.i.i.4.1 = add nsw i32 %mul14.i.i.4, %conv.i.i.4.1
  %idxprom16.i.i.4.1 = sext i32 %add15.i.i.4.1 to i64
  %arrayidx17.i.i.4.1 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.4.1
  %673 = load float, float* %arrayidx17.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.4.1 = fadd float %sub.i.i.4.1, %673
  %arrayidx22.i.i.4.1 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.4.1
  %674 = load float, float* %arrayidx22.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.4.1 = fsub float %add18.i.i.4.1, %674
  %conv24.i.i.4.1 = fpext float %sub23.i.i.4.1 to double
  %675 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.4.1, double 0xBFE6666666666666, double %conv3.i.i.4.1) #3
  %conv26.i.i.4.1 = fptrunc double %675 to float
  store float %conv26.i.i.4.1, float* %arrayidx.i.i.4.1, align 4, !tbaa !12, !llvm.access.group !16
  %676 = add nuw nsw i64 %_local_id_x.i.0.4, 2
  %exitcond.4.not.1 = icmp eq i64 %676, 32
  br i1 %exitcond.4.not.1, label %pregion_for_end.i.i.4.loopexit, label %pregion_for_entry.entry.i.i.4, !llvm.loop !41

pregion_for_end.i.i.4.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.4
  br label %pregion_for_end.i.i.4

pregion_for_end.i.i.4:                            ; preds = %pregion_for_end.i.i.4.loopexit, %vector.body86
  %677 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.5 = or i32 %677, 5
  %mul.i.i.5 = mul nsw i32 %16, %conv2.i.i.5
  %add13.i.i.5 = add nuw nsw i32 %conv2.i.i.5, 1
  %mul14.i.i.5 = mul nsw i32 %add13.i.i.5, %16
  %678 = mul i32 %16, %add13.i.i.4
  %679 = trunc i64 %2 to i32
  %680 = shl i32 %679, 5
  %681 = add nsw i32 %678, %680
  %682 = icmp sgt i32 %681, 2147483616
  %683 = add i32 %678, %680
  %684 = add i32 %683, 1
  %685 = add i32 %683, 32
  %686 = icmp slt i32 %685, %684
  %687 = or i1 %682, %686
  %688 = or i32 %conv2.i.i, 6
  %689 = mul i32 %16, %688
  %690 = add nsw i32 %689, %680
  %691 = icmp sgt i32 %690, 2147483616
  %692 = or i1 %687, %691
  br i1 %692, label %pregion_for_entry.entry.i.i.5.preheader, label %vector.body110

pregion_for_entry.entry.i.i.5.preheader:          ; preds = %pregion_for_end.i.i.4
  br label %pregion_for_entry.entry.i.i.5

vector.body110:                                   ; preds = %pregion_for_end.i.i.4
  %693 = trunc i64 %mul.i.i.i to i32
  %694 = add i32 %mul.i.i.5, %693
  %695 = sext i32 %694 to i64
  %696 = getelementptr inbounds float, float* %12, i64 %695
  %697 = bitcast float* %696 to <8 x float>*
  %wide.load127 = load <8 x float>, <8 x float>* %697, align 4, !tbaa !12, !llvm.access.group !16
  %698 = fpext <8 x float> %wide.load127 to <8 x double>
  %699 = add i32 %694, 1
  %700 = sext i32 %699 to i64
  %701 = getelementptr inbounds float, float* %6, i64 %700
  %702 = bitcast float* %701 to <8 x float>*
  %wide.load128 = load <8 x float>, <8 x float>* %702, align 4, !tbaa !12, !llvm.access.group !16
  %703 = getelementptr inbounds float, float* %6, i64 %695
  %704 = bitcast float* %703 to <8 x float>*
  %wide.load129 = load <8 x float>, <8 x float>* %704, align 4, !tbaa !12, !llvm.access.group !16
  %705 = fsub <8 x float> %wide.load128, %wide.load129
  %706 = add nsw i32 %mul14.i.i.5, %693
  %707 = sext i32 %706 to i64
  %708 = getelementptr inbounds float, float* %9, i64 %707
  %709 = bitcast float* %708 to <8 x float>*
  %wide.load130 = load <8 x float>, <8 x float>* %709, align 4, !tbaa !12, !llvm.access.group !16
  %710 = fadd <8 x float> %705, %wide.load130
  %711 = getelementptr inbounds float, float* %9, i64 %695
  %712 = bitcast float* %711 to <8 x float>*
  %wide.load131 = load <8 x float>, <8 x float>* %712, align 4, !tbaa !12, !llvm.access.group !16
  %713 = fsub <8 x float> %710, %wide.load131
  %714 = fpext <8 x float> %713 to <8 x double>
  %715 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %714, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %698)
  %716 = fptrunc <8 x double> %715 to <8 x float>
  %717 = bitcast float* %696 to <8 x float>*
  store <8 x float> %716, <8 x float>* %717, align 4, !tbaa !12, !llvm.access.group !16
  %718 = trunc i64 %mul.i.i.i to i32
  %719 = or i32 %718, 8
  %720 = add i32 %mul.i.i.5, %719
  %721 = sext i32 %720 to i64
  %722 = getelementptr inbounds float, float* %12, i64 %721
  %723 = bitcast float* %722 to <8 x float>*
  %wide.load127.1 = load <8 x float>, <8 x float>* %723, align 4, !tbaa !12, !llvm.access.group !16
  %724 = fpext <8 x float> %wide.load127.1 to <8 x double>
  %725 = add i32 %720, 1
  %726 = sext i32 %725 to i64
  %727 = getelementptr inbounds float, float* %6, i64 %726
  %728 = bitcast float* %727 to <8 x float>*
  %wide.load128.1 = load <8 x float>, <8 x float>* %728, align 4, !tbaa !12, !llvm.access.group !16
  %729 = getelementptr inbounds float, float* %6, i64 %721
  %730 = bitcast float* %729 to <8 x float>*
  %wide.load129.1 = load <8 x float>, <8 x float>* %730, align 4, !tbaa !12, !llvm.access.group !16
  %731 = fsub <8 x float> %wide.load128.1, %wide.load129.1
  %732 = add nsw i32 %mul14.i.i.5, %719
  %733 = sext i32 %732 to i64
  %734 = getelementptr inbounds float, float* %9, i64 %733
  %735 = bitcast float* %734 to <8 x float>*
  %wide.load130.1 = load <8 x float>, <8 x float>* %735, align 4, !tbaa !12, !llvm.access.group !16
  %736 = fadd <8 x float> %731, %wide.load130.1
  %737 = getelementptr inbounds float, float* %9, i64 %721
  %738 = bitcast float* %737 to <8 x float>*
  %wide.load131.1 = load <8 x float>, <8 x float>* %738, align 4, !tbaa !12, !llvm.access.group !16
  %739 = fsub <8 x float> %736, %wide.load131.1
  %740 = fpext <8 x float> %739 to <8 x double>
  %741 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %740, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %724)
  %742 = fptrunc <8 x double> %741 to <8 x float>
  %743 = bitcast float* %722 to <8 x float>*
  store <8 x float> %742, <8 x float>* %743, align 4, !tbaa !12, !llvm.access.group !16
  %744 = trunc i64 %mul.i.i.i to i32
  %745 = or i32 %744, 16
  %746 = add i32 %mul.i.i.5, %745
  %747 = sext i32 %746 to i64
  %748 = getelementptr inbounds float, float* %12, i64 %747
  %749 = bitcast float* %748 to <8 x float>*
  %wide.load127.2 = load <8 x float>, <8 x float>* %749, align 4, !tbaa !12, !llvm.access.group !16
  %750 = fpext <8 x float> %wide.load127.2 to <8 x double>
  %751 = add i32 %746, 1
  %752 = sext i32 %751 to i64
  %753 = getelementptr inbounds float, float* %6, i64 %752
  %754 = bitcast float* %753 to <8 x float>*
  %wide.load128.2 = load <8 x float>, <8 x float>* %754, align 4, !tbaa !12, !llvm.access.group !16
  %755 = getelementptr inbounds float, float* %6, i64 %747
  %756 = bitcast float* %755 to <8 x float>*
  %wide.load129.2 = load <8 x float>, <8 x float>* %756, align 4, !tbaa !12, !llvm.access.group !16
  %757 = fsub <8 x float> %wide.load128.2, %wide.load129.2
  %758 = add nsw i32 %mul14.i.i.5, %745
  %759 = sext i32 %758 to i64
  %760 = getelementptr inbounds float, float* %9, i64 %759
  %761 = bitcast float* %760 to <8 x float>*
  %wide.load130.2 = load <8 x float>, <8 x float>* %761, align 4, !tbaa !12, !llvm.access.group !16
  %762 = fadd <8 x float> %757, %wide.load130.2
  %763 = getelementptr inbounds float, float* %9, i64 %747
  %764 = bitcast float* %763 to <8 x float>*
  %wide.load131.2 = load <8 x float>, <8 x float>* %764, align 4, !tbaa !12, !llvm.access.group !16
  %765 = fsub <8 x float> %762, %wide.load131.2
  %766 = fpext <8 x float> %765 to <8 x double>
  %767 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %766, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %750)
  %768 = fptrunc <8 x double> %767 to <8 x float>
  %769 = bitcast float* %748 to <8 x float>*
  store <8 x float> %768, <8 x float>* %769, align 4, !tbaa !12, !llvm.access.group !16
  %770 = trunc i64 %mul.i.i.i to i32
  %771 = or i32 %770, 24
  %772 = add i32 %mul.i.i.5, %771
  %773 = sext i32 %772 to i64
  %774 = getelementptr inbounds float, float* %12, i64 %773
  %775 = bitcast float* %774 to <8 x float>*
  %wide.load127.3 = load <8 x float>, <8 x float>* %775, align 4, !tbaa !12, !llvm.access.group !16
  %776 = fpext <8 x float> %wide.load127.3 to <8 x double>
  %777 = add i32 %772, 1
  %778 = sext i32 %777 to i64
  %779 = getelementptr inbounds float, float* %6, i64 %778
  %780 = bitcast float* %779 to <8 x float>*
  %wide.load128.3 = load <8 x float>, <8 x float>* %780, align 4, !tbaa !12, !llvm.access.group !16
  %781 = getelementptr inbounds float, float* %6, i64 %773
  %782 = bitcast float* %781 to <8 x float>*
  %wide.load129.3 = load <8 x float>, <8 x float>* %782, align 4, !tbaa !12, !llvm.access.group !16
  %783 = fsub <8 x float> %wide.load128.3, %wide.load129.3
  %784 = add nsw i32 %mul14.i.i.5, %771
  %785 = sext i32 %784 to i64
  %786 = getelementptr inbounds float, float* %9, i64 %785
  %787 = bitcast float* %786 to <8 x float>*
  %wide.load130.3 = load <8 x float>, <8 x float>* %787, align 4, !tbaa !12, !llvm.access.group !16
  %788 = fadd <8 x float> %783, %wide.load130.3
  %789 = getelementptr inbounds float, float* %9, i64 %773
  %790 = bitcast float* %789 to <8 x float>*
  %wide.load131.3 = load <8 x float>, <8 x float>* %790, align 4, !tbaa !12, !llvm.access.group !16
  %791 = fsub <8 x float> %788, %wide.load131.3
  %792 = fpext <8 x float> %791 to <8 x double>
  %793 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %792, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %776)
  %794 = fptrunc <8 x double> %793 to <8 x float>
  %795 = bitcast float* %774 to <8 x float>*
  store <8 x float> %794, <8 x float>* %795, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.5

pregion_for_entry.entry.i.i.5:                    ; preds = %pregion_for_entry.entry.i.i.5, %pregion_for_entry.entry.i.i.5.preheader
  %_local_id_x.i.0.5 = phi i64 [ %809, %pregion_for_entry.entry.i.i.5 ], [ 0, %pregion_for_entry.entry.i.i.5.preheader ]
  %add1.i.i.i.5 = add nuw nsw i64 %_local_id_x.i.0.5, %mul.i.i.i
  %conv.i.i.5 = trunc i64 %add1.i.i.i.5 to i32
  %add.i.i.5 = add i32 %mul.i.i.5, %conv.i.i.5
  %idxprom.i.i.5 = sext i32 %add.i.i.5 to i64
  %arrayidx.i.i.5 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.5
  %796 = load float, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.5 = fpext float %796 to double
  %add6.i.i.5 = add i32 %add.i.i.5, 1
  %idxprom7.i.i.5 = sext i32 %add6.i.i.5 to i64
  %arrayidx8.i.i.5 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.5
  %797 = load float, float* %arrayidx8.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.5 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.5
  %798 = load float, float* %arrayidx12.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.5 = fsub float %797, %798
  %add15.i.i.5 = add nsw i32 %mul14.i.i.5, %conv.i.i.5
  %idxprom16.i.i.5 = sext i32 %add15.i.i.5 to i64
  %arrayidx17.i.i.5 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.5
  %799 = load float, float* %arrayidx17.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.5 = fadd float %sub.i.i.5, %799
  %arrayidx22.i.i.5 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.5
  %800 = load float, float* %arrayidx22.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.5 = fsub float %add18.i.i.5, %800
  %conv24.i.i.5 = fpext float %sub23.i.i.5 to double
  %801 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.5, double 0xBFE6666666666666, double %conv3.i.i.5) #3
  %conv26.i.i.5 = fptrunc double %801 to float
  store float %conv26.i.i.5, float* %arrayidx.i.i.5, align 4, !tbaa !12, !llvm.access.group !16
  %802 = or i64 %_local_id_x.i.0.5, 1
  %add1.i.i.i.5.1 = add nuw nsw i64 %802, %mul.i.i.i
  %conv.i.i.5.1 = trunc i64 %add1.i.i.i.5.1 to i32
  %add.i.i.5.1 = add i32 %mul.i.i.5, %conv.i.i.5.1
  %idxprom.i.i.5.1 = sext i32 %add.i.i.5.1 to i64
  %arrayidx.i.i.5.1 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.5.1
  %803 = load float, float* %arrayidx.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.5.1 = fpext float %803 to double
  %add6.i.i.5.1 = add i32 %add.i.i.5.1, 1
  %idxprom7.i.i.5.1 = sext i32 %add6.i.i.5.1 to i64
  %arrayidx8.i.i.5.1 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.5.1
  %804 = load float, float* %arrayidx8.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.5.1 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.5.1
  %805 = load float, float* %arrayidx12.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.5.1 = fsub float %804, %805
  %add15.i.i.5.1 = add nsw i32 %mul14.i.i.5, %conv.i.i.5.1
  %idxprom16.i.i.5.1 = sext i32 %add15.i.i.5.1 to i64
  %arrayidx17.i.i.5.1 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.5.1
  %806 = load float, float* %arrayidx17.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.5.1 = fadd float %sub.i.i.5.1, %806
  %arrayidx22.i.i.5.1 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.5.1
  %807 = load float, float* %arrayidx22.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.5.1 = fsub float %add18.i.i.5.1, %807
  %conv24.i.i.5.1 = fpext float %sub23.i.i.5.1 to double
  %808 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.5.1, double 0xBFE6666666666666, double %conv3.i.i.5.1) #3
  %conv26.i.i.5.1 = fptrunc double %808 to float
  store float %conv26.i.i.5.1, float* %arrayidx.i.i.5.1, align 4, !tbaa !12, !llvm.access.group !16
  %809 = add nuw nsw i64 %_local_id_x.i.0.5, 2
  %exitcond.5.not.1 = icmp eq i64 %809, 32
  br i1 %exitcond.5.not.1, label %pregion_for_end.i.i.5.loopexit, label %pregion_for_entry.entry.i.i.5, !llvm.loop !42

pregion_for_end.i.i.5.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.5
  br label %pregion_for_end.i.i.5

pregion_for_end.i.i.5:                            ; preds = %pregion_for_end.i.i.5.loopexit, %vector.body110
  %810 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.6 = or i32 %810, 6
  %mul.i.i.6 = mul nsw i32 %16, %conv2.i.i.6
  %add13.i.i.6 = or i32 %810, 7
  %mul14.i.i.6 = mul nsw i32 %add13.i.i.6, %16
  %811 = mul i32 %16, %conv2.i.i.6
  %812 = trunc i64 %2 to i32
  %813 = shl i32 %812, 5
  %814 = add nsw i32 %811, %813
  %815 = icmp sgt i32 %814, 2147483616
  %816 = add i32 %811, %813
  %817 = or i32 %816, 1
  %818 = icmp sgt i32 %817, 2147483616
  %819 = or i1 %815, %818
  %820 = mul i32 %16, %add13.i.i.6
  %821 = add nsw i32 %820, %813
  %822 = icmp sgt i32 %821, 2147483616
  %823 = or i1 %819, %822
  br i1 %823, label %pregion_for_entry.entry.i.i.6.preheader, label %vector.body134

pregion_for_entry.entry.i.i.6.preheader:          ; preds = %pregion_for_end.i.i.5
  br label %pregion_for_entry.entry.i.i.6

vector.body134:                                   ; preds = %pregion_for_end.i.i.5
  %824 = trunc i64 %mul.i.i.i to i32
  %825 = add i32 %mul.i.i.6, %824
  %826 = sext i32 %825 to i64
  %827 = getelementptr inbounds float, float* %12, i64 %826
  %828 = bitcast float* %827 to <8 x float>*
  %wide.load151 = load <8 x float>, <8 x float>* %828, align 4, !tbaa !12, !llvm.access.group !16
  %829 = fpext <8 x float> %wide.load151 to <8 x double>
  %830 = or i32 %825, 1
  %831 = sext i32 %830 to i64
  %832 = getelementptr inbounds float, float* %6, i64 %831
  %833 = bitcast float* %832 to <8 x float>*
  %wide.load152 = load <8 x float>, <8 x float>* %833, align 4, !tbaa !12, !llvm.access.group !16
  %834 = getelementptr inbounds float, float* %6, i64 %826
  %835 = bitcast float* %834 to <8 x float>*
  %wide.load153 = load <8 x float>, <8 x float>* %835, align 4, !tbaa !12, !llvm.access.group !16
  %836 = fsub <8 x float> %wide.load152, %wide.load153
  %837 = add nsw i32 %mul14.i.i.6, %824
  %838 = sext i32 %837 to i64
  %839 = getelementptr inbounds float, float* %9, i64 %838
  %840 = bitcast float* %839 to <8 x float>*
  %wide.load154 = load <8 x float>, <8 x float>* %840, align 4, !tbaa !12, !llvm.access.group !16
  %841 = fadd <8 x float> %836, %wide.load154
  %842 = getelementptr inbounds float, float* %9, i64 %826
  %843 = bitcast float* %842 to <8 x float>*
  %wide.load155 = load <8 x float>, <8 x float>* %843, align 4, !tbaa !12, !llvm.access.group !16
  %844 = fsub <8 x float> %841, %wide.load155
  %845 = fpext <8 x float> %844 to <8 x double>
  %846 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %845, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %829)
  %847 = fptrunc <8 x double> %846 to <8 x float>
  %848 = bitcast float* %827 to <8 x float>*
  store <8 x float> %847, <8 x float>* %848, align 4, !tbaa !12, !llvm.access.group !16
  %849 = trunc i64 %mul.i.i.i to i32
  %850 = or i32 %849, 8
  %851 = add i32 %mul.i.i.6, %850
  %852 = sext i32 %851 to i64
  %853 = getelementptr inbounds float, float* %12, i64 %852
  %854 = bitcast float* %853 to <8 x float>*
  %wide.load151.1 = load <8 x float>, <8 x float>* %854, align 4, !tbaa !12, !llvm.access.group !16
  %855 = fpext <8 x float> %wide.load151.1 to <8 x double>
  %856 = or i32 %851, 1
  %857 = sext i32 %856 to i64
  %858 = getelementptr inbounds float, float* %6, i64 %857
  %859 = bitcast float* %858 to <8 x float>*
  %wide.load152.1 = load <8 x float>, <8 x float>* %859, align 4, !tbaa !12, !llvm.access.group !16
  %860 = getelementptr inbounds float, float* %6, i64 %852
  %861 = bitcast float* %860 to <8 x float>*
  %wide.load153.1 = load <8 x float>, <8 x float>* %861, align 4, !tbaa !12, !llvm.access.group !16
  %862 = fsub <8 x float> %wide.load152.1, %wide.load153.1
  %863 = add nsw i32 %mul14.i.i.6, %850
  %864 = sext i32 %863 to i64
  %865 = getelementptr inbounds float, float* %9, i64 %864
  %866 = bitcast float* %865 to <8 x float>*
  %wide.load154.1 = load <8 x float>, <8 x float>* %866, align 4, !tbaa !12, !llvm.access.group !16
  %867 = fadd <8 x float> %862, %wide.load154.1
  %868 = getelementptr inbounds float, float* %9, i64 %852
  %869 = bitcast float* %868 to <8 x float>*
  %wide.load155.1 = load <8 x float>, <8 x float>* %869, align 4, !tbaa !12, !llvm.access.group !16
  %870 = fsub <8 x float> %867, %wide.load155.1
  %871 = fpext <8 x float> %870 to <8 x double>
  %872 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %871, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %855)
  %873 = fptrunc <8 x double> %872 to <8 x float>
  %874 = bitcast float* %853 to <8 x float>*
  store <8 x float> %873, <8 x float>* %874, align 4, !tbaa !12, !llvm.access.group !16
  %875 = trunc i64 %mul.i.i.i to i32
  %876 = or i32 %875, 16
  %877 = add i32 %mul.i.i.6, %876
  %878 = sext i32 %877 to i64
  %879 = getelementptr inbounds float, float* %12, i64 %878
  %880 = bitcast float* %879 to <8 x float>*
  %wide.load151.2 = load <8 x float>, <8 x float>* %880, align 4, !tbaa !12, !llvm.access.group !16
  %881 = fpext <8 x float> %wide.load151.2 to <8 x double>
  %882 = or i32 %877, 1
  %883 = sext i32 %882 to i64
  %884 = getelementptr inbounds float, float* %6, i64 %883
  %885 = bitcast float* %884 to <8 x float>*
  %wide.load152.2 = load <8 x float>, <8 x float>* %885, align 4, !tbaa !12, !llvm.access.group !16
  %886 = getelementptr inbounds float, float* %6, i64 %878
  %887 = bitcast float* %886 to <8 x float>*
  %wide.load153.2 = load <8 x float>, <8 x float>* %887, align 4, !tbaa !12, !llvm.access.group !16
  %888 = fsub <8 x float> %wide.load152.2, %wide.load153.2
  %889 = add nsw i32 %mul14.i.i.6, %876
  %890 = sext i32 %889 to i64
  %891 = getelementptr inbounds float, float* %9, i64 %890
  %892 = bitcast float* %891 to <8 x float>*
  %wide.load154.2 = load <8 x float>, <8 x float>* %892, align 4, !tbaa !12, !llvm.access.group !16
  %893 = fadd <8 x float> %888, %wide.load154.2
  %894 = getelementptr inbounds float, float* %9, i64 %878
  %895 = bitcast float* %894 to <8 x float>*
  %wide.load155.2 = load <8 x float>, <8 x float>* %895, align 4, !tbaa !12, !llvm.access.group !16
  %896 = fsub <8 x float> %893, %wide.load155.2
  %897 = fpext <8 x float> %896 to <8 x double>
  %898 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %897, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %881)
  %899 = fptrunc <8 x double> %898 to <8 x float>
  %900 = bitcast float* %879 to <8 x float>*
  store <8 x float> %899, <8 x float>* %900, align 4, !tbaa !12, !llvm.access.group !16
  %901 = trunc i64 %mul.i.i.i to i32
  %902 = or i32 %901, 24
  %903 = add i32 %mul.i.i.6, %902
  %904 = sext i32 %903 to i64
  %905 = getelementptr inbounds float, float* %12, i64 %904
  %906 = bitcast float* %905 to <8 x float>*
  %wide.load151.3 = load <8 x float>, <8 x float>* %906, align 4, !tbaa !12, !llvm.access.group !16
  %907 = fpext <8 x float> %wide.load151.3 to <8 x double>
  %908 = or i32 %903, 1
  %909 = sext i32 %908 to i64
  %910 = getelementptr inbounds float, float* %6, i64 %909
  %911 = bitcast float* %910 to <8 x float>*
  %wide.load152.3 = load <8 x float>, <8 x float>* %911, align 4, !tbaa !12, !llvm.access.group !16
  %912 = getelementptr inbounds float, float* %6, i64 %904
  %913 = bitcast float* %912 to <8 x float>*
  %wide.load153.3 = load <8 x float>, <8 x float>* %913, align 4, !tbaa !12, !llvm.access.group !16
  %914 = fsub <8 x float> %wide.load152.3, %wide.load153.3
  %915 = add nsw i32 %mul14.i.i.6, %902
  %916 = sext i32 %915 to i64
  %917 = getelementptr inbounds float, float* %9, i64 %916
  %918 = bitcast float* %917 to <8 x float>*
  %wide.load154.3 = load <8 x float>, <8 x float>* %918, align 4, !tbaa !12, !llvm.access.group !16
  %919 = fadd <8 x float> %914, %wide.load154.3
  %920 = getelementptr inbounds float, float* %9, i64 %904
  %921 = bitcast float* %920 to <8 x float>*
  %wide.load155.3 = load <8 x float>, <8 x float>* %921, align 4, !tbaa !12, !llvm.access.group !16
  %922 = fsub <8 x float> %919, %wide.load155.3
  %923 = fpext <8 x float> %922 to <8 x double>
  %924 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %923, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %907)
  %925 = fptrunc <8 x double> %924 to <8 x float>
  %926 = bitcast float* %905 to <8 x float>*
  store <8 x float> %925, <8 x float>* %926, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.6

pregion_for_entry.entry.i.i.6:                    ; preds = %pregion_for_entry.entry.i.i.6, %pregion_for_entry.entry.i.i.6.preheader
  %_local_id_x.i.0.6 = phi i64 [ %940, %pregion_for_entry.entry.i.i.6 ], [ 0, %pregion_for_entry.entry.i.i.6.preheader ]
  %add1.i.i.i.6 = add nuw nsw i64 %_local_id_x.i.0.6, %mul.i.i.i
  %conv.i.i.6 = trunc i64 %add1.i.i.i.6 to i32
  %add.i.i.6 = add i32 %mul.i.i.6, %conv.i.i.6
  %idxprom.i.i.6 = sext i32 %add.i.i.6 to i64
  %arrayidx.i.i.6 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.6
  %927 = load float, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.6 = fpext float %927 to double
  %add6.i.i.6 = or i32 %add.i.i.6, 1
  %idxprom7.i.i.6 = sext i32 %add6.i.i.6 to i64
  %arrayidx8.i.i.6 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.6
  %928 = load float, float* %arrayidx8.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.6 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.6
  %929 = load float, float* %arrayidx12.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.6 = fsub float %928, %929
  %add15.i.i.6 = add nsw i32 %mul14.i.i.6, %conv.i.i.6
  %idxprom16.i.i.6 = sext i32 %add15.i.i.6 to i64
  %arrayidx17.i.i.6 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.6
  %930 = load float, float* %arrayidx17.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.6 = fadd float %sub.i.i.6, %930
  %arrayidx22.i.i.6 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.6
  %931 = load float, float* %arrayidx22.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.6 = fsub float %add18.i.i.6, %931
  %conv24.i.i.6 = fpext float %sub23.i.i.6 to double
  %932 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.6, double 0xBFE6666666666666, double %conv3.i.i.6) #3
  %conv26.i.i.6 = fptrunc double %932 to float
  store float %conv26.i.i.6, float* %arrayidx.i.i.6, align 4, !tbaa !12, !llvm.access.group !16
  %933 = or i64 %_local_id_x.i.0.6, 1
  %add1.i.i.i.6.1 = add nuw nsw i64 %933, %mul.i.i.i
  %conv.i.i.6.1 = trunc i64 %add1.i.i.i.6.1 to i32
  %add.i.i.6.1 = add i32 %mul.i.i.6, %conv.i.i.6.1
  %idxprom.i.i.6.1 = sext i32 %add.i.i.6.1 to i64
  %arrayidx.i.i.6.1 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.6.1
  %934 = load float, float* %arrayidx.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.6.1 = fpext float %934 to double
  %add6.i.i.6.1 = add i32 %add.i.i.6.1, 1
  %idxprom7.i.i.6.1 = sext i32 %add6.i.i.6.1 to i64
  %arrayidx8.i.i.6.1 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.6.1
  %935 = load float, float* %arrayidx8.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.6.1 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.6.1
  %936 = load float, float* %arrayidx12.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.6.1 = fsub float %935, %936
  %add15.i.i.6.1 = add nsw i32 %mul14.i.i.6, %conv.i.i.6.1
  %idxprom16.i.i.6.1 = sext i32 %add15.i.i.6.1 to i64
  %arrayidx17.i.i.6.1 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.6.1
  %937 = load float, float* %arrayidx17.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.6.1 = fadd float %sub.i.i.6.1, %937
  %arrayidx22.i.i.6.1 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.6.1
  %938 = load float, float* %arrayidx22.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.6.1 = fsub float %add18.i.i.6.1, %938
  %conv24.i.i.6.1 = fpext float %sub23.i.i.6.1 to double
  %939 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.6.1, double 0xBFE6666666666666, double %conv3.i.i.6.1) #3
  %conv26.i.i.6.1 = fptrunc double %939 to float
  store float %conv26.i.i.6.1, float* %arrayidx.i.i.6.1, align 4, !tbaa !12, !llvm.access.group !16
  %940 = add nuw nsw i64 %_local_id_x.i.0.6, 2
  %exitcond.6.not.1 = icmp eq i64 %940, 32
  br i1 %exitcond.6.not.1, label %pregion_for_end.i.i.6.loopexit, label %pregion_for_entry.entry.i.i.6, !llvm.loop !43

pregion_for_end.i.i.6.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.6
  br label %pregion_for_end.i.i.6

pregion_for_end.i.i.6:                            ; preds = %pregion_for_end.i.i.6.loopexit, %vector.body134
  %941 = trunc i64 %mul3.i.i.i to i32
  %conv2.i.i.7 = or i32 %941, 7
  %mul.i.i.7 = mul nsw i32 %16, %conv2.i.i.7
  %add13.i.i.7 = add nsw i32 %conv2.i.i.7, 1
  %mul14.i.i.7 = mul nsw i32 %add13.i.i.7, %16
  %942 = mul i32 %16, %add13.i.i.6
  %943 = trunc i64 %2 to i32
  %944 = shl i32 %943, 5
  %945 = add nsw i32 %942, %944
  %946 = icmp sgt i32 %945, 2147483616
  %947 = add i32 %942, %944
  %948 = add i32 %947, 1
  %949 = add i32 %947, 32
  %950 = icmp slt i32 %949, %948
  %951 = or i1 %946, %950
  %952 = add i32 %conv2.i.i, 8
  %953 = mul i32 %16, %952
  %954 = add nsw i32 %953, %944
  %955 = icmp sgt i32 %954, 2147483616
  %956 = or i1 %951, %955
  br i1 %956, label %pregion_for_entry.entry.i.i.7.preheader, label %vector.body158

pregion_for_entry.entry.i.i.7.preheader:          ; preds = %pregion_for_end.i.i.6
  br label %pregion_for_entry.entry.i.i.7

vector.body158:                                   ; preds = %pregion_for_end.i.i.6
  %957 = trunc i64 %mul.i.i.i to i32
  %958 = add i32 %mul.i.i.7, %957
  %959 = sext i32 %958 to i64
  %960 = getelementptr inbounds float, float* %12, i64 %959
  %961 = bitcast float* %960 to <8 x float>*
  %wide.load175 = load <8 x float>, <8 x float>* %961, align 4, !tbaa !12, !llvm.access.group !16
  %962 = fpext <8 x float> %wide.load175 to <8 x double>
  %963 = add i32 %958, 1
  %964 = sext i32 %963 to i64
  %965 = getelementptr inbounds float, float* %6, i64 %964
  %966 = bitcast float* %965 to <8 x float>*
  %wide.load176 = load <8 x float>, <8 x float>* %966, align 4, !tbaa !12, !llvm.access.group !16
  %967 = getelementptr inbounds float, float* %6, i64 %959
  %968 = bitcast float* %967 to <8 x float>*
  %wide.load177 = load <8 x float>, <8 x float>* %968, align 4, !tbaa !12, !llvm.access.group !16
  %969 = fsub <8 x float> %wide.load176, %wide.load177
  %970 = add nsw i32 %mul14.i.i.7, %957
  %971 = sext i32 %970 to i64
  %972 = getelementptr inbounds float, float* %9, i64 %971
  %973 = bitcast float* %972 to <8 x float>*
  %wide.load178 = load <8 x float>, <8 x float>* %973, align 4, !tbaa !12, !llvm.access.group !16
  %974 = fadd <8 x float> %969, %wide.load178
  %975 = getelementptr inbounds float, float* %9, i64 %959
  %976 = bitcast float* %975 to <8 x float>*
  %wide.load179 = load <8 x float>, <8 x float>* %976, align 4, !tbaa !12, !llvm.access.group !16
  %977 = fsub <8 x float> %974, %wide.load179
  %978 = fpext <8 x float> %977 to <8 x double>
  %979 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %978, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %962)
  %980 = fptrunc <8 x double> %979 to <8 x float>
  %981 = bitcast float* %960 to <8 x float>*
  store <8 x float> %980, <8 x float>* %981, align 4, !tbaa !12, !llvm.access.group !16
  %982 = trunc i64 %mul.i.i.i to i32
  %983 = or i32 %982, 8
  %984 = add i32 %mul.i.i.7, %983
  %985 = sext i32 %984 to i64
  %986 = getelementptr inbounds float, float* %12, i64 %985
  %987 = bitcast float* %986 to <8 x float>*
  %wide.load175.1 = load <8 x float>, <8 x float>* %987, align 4, !tbaa !12, !llvm.access.group !16
  %988 = fpext <8 x float> %wide.load175.1 to <8 x double>
  %989 = add i32 %984, 1
  %990 = sext i32 %989 to i64
  %991 = getelementptr inbounds float, float* %6, i64 %990
  %992 = bitcast float* %991 to <8 x float>*
  %wide.load176.1 = load <8 x float>, <8 x float>* %992, align 4, !tbaa !12, !llvm.access.group !16
  %993 = getelementptr inbounds float, float* %6, i64 %985
  %994 = bitcast float* %993 to <8 x float>*
  %wide.load177.1 = load <8 x float>, <8 x float>* %994, align 4, !tbaa !12, !llvm.access.group !16
  %995 = fsub <8 x float> %wide.load176.1, %wide.load177.1
  %996 = add nsw i32 %mul14.i.i.7, %983
  %997 = sext i32 %996 to i64
  %998 = getelementptr inbounds float, float* %9, i64 %997
  %999 = bitcast float* %998 to <8 x float>*
  %wide.load178.1 = load <8 x float>, <8 x float>* %999, align 4, !tbaa !12, !llvm.access.group !16
  %1000 = fadd <8 x float> %995, %wide.load178.1
  %1001 = getelementptr inbounds float, float* %9, i64 %985
  %1002 = bitcast float* %1001 to <8 x float>*
  %wide.load179.1 = load <8 x float>, <8 x float>* %1002, align 4, !tbaa !12, !llvm.access.group !16
  %1003 = fsub <8 x float> %1000, %wide.load179.1
  %1004 = fpext <8 x float> %1003 to <8 x double>
  %1005 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %1004, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %988)
  %1006 = fptrunc <8 x double> %1005 to <8 x float>
  %1007 = bitcast float* %986 to <8 x float>*
  store <8 x float> %1006, <8 x float>* %1007, align 4, !tbaa !12, !llvm.access.group !16
  %1008 = trunc i64 %mul.i.i.i to i32
  %1009 = or i32 %1008, 16
  %1010 = add i32 %mul.i.i.7, %1009
  %1011 = sext i32 %1010 to i64
  %1012 = getelementptr inbounds float, float* %12, i64 %1011
  %1013 = bitcast float* %1012 to <8 x float>*
  %wide.load175.2 = load <8 x float>, <8 x float>* %1013, align 4, !tbaa !12, !llvm.access.group !16
  %1014 = fpext <8 x float> %wide.load175.2 to <8 x double>
  %1015 = add i32 %1010, 1
  %1016 = sext i32 %1015 to i64
  %1017 = getelementptr inbounds float, float* %6, i64 %1016
  %1018 = bitcast float* %1017 to <8 x float>*
  %wide.load176.2 = load <8 x float>, <8 x float>* %1018, align 4, !tbaa !12, !llvm.access.group !16
  %1019 = getelementptr inbounds float, float* %6, i64 %1011
  %1020 = bitcast float* %1019 to <8 x float>*
  %wide.load177.2 = load <8 x float>, <8 x float>* %1020, align 4, !tbaa !12, !llvm.access.group !16
  %1021 = fsub <8 x float> %wide.load176.2, %wide.load177.2
  %1022 = add nsw i32 %mul14.i.i.7, %1009
  %1023 = sext i32 %1022 to i64
  %1024 = getelementptr inbounds float, float* %9, i64 %1023
  %1025 = bitcast float* %1024 to <8 x float>*
  %wide.load178.2 = load <8 x float>, <8 x float>* %1025, align 4, !tbaa !12, !llvm.access.group !16
  %1026 = fadd <8 x float> %1021, %wide.load178.2
  %1027 = getelementptr inbounds float, float* %9, i64 %1011
  %1028 = bitcast float* %1027 to <8 x float>*
  %wide.load179.2 = load <8 x float>, <8 x float>* %1028, align 4, !tbaa !12, !llvm.access.group !16
  %1029 = fsub <8 x float> %1026, %wide.load179.2
  %1030 = fpext <8 x float> %1029 to <8 x double>
  %1031 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %1030, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %1014)
  %1032 = fptrunc <8 x double> %1031 to <8 x float>
  %1033 = bitcast float* %1012 to <8 x float>*
  store <8 x float> %1032, <8 x float>* %1033, align 4, !tbaa !12, !llvm.access.group !16
  %1034 = trunc i64 %mul.i.i.i to i32
  %1035 = or i32 %1034, 24
  %1036 = add i32 %mul.i.i.7, %1035
  %1037 = sext i32 %1036 to i64
  %1038 = getelementptr inbounds float, float* %12, i64 %1037
  %1039 = bitcast float* %1038 to <8 x float>*
  %wide.load175.3 = load <8 x float>, <8 x float>* %1039, align 4, !tbaa !12, !llvm.access.group !16
  %1040 = fpext <8 x float> %wide.load175.3 to <8 x double>
  %1041 = add i32 %1036, 1
  %1042 = sext i32 %1041 to i64
  %1043 = getelementptr inbounds float, float* %6, i64 %1042
  %1044 = bitcast float* %1043 to <8 x float>*
  %wide.load176.3 = load <8 x float>, <8 x float>* %1044, align 4, !tbaa !12, !llvm.access.group !16
  %1045 = getelementptr inbounds float, float* %6, i64 %1037
  %1046 = bitcast float* %1045 to <8 x float>*
  %wide.load177.3 = load <8 x float>, <8 x float>* %1046, align 4, !tbaa !12, !llvm.access.group !16
  %1047 = fsub <8 x float> %wide.load176.3, %wide.load177.3
  %1048 = add nsw i32 %mul14.i.i.7, %1035
  %1049 = sext i32 %1048 to i64
  %1050 = getelementptr inbounds float, float* %9, i64 %1049
  %1051 = bitcast float* %1050 to <8 x float>*
  %wide.load178.3 = load <8 x float>, <8 x float>* %1051, align 4, !tbaa !12, !llvm.access.group !16
  %1052 = fadd <8 x float> %1047, %wide.load178.3
  %1053 = getelementptr inbounds float, float* %9, i64 %1037
  %1054 = bitcast float* %1053 to <8 x float>*
  %wide.load179.3 = load <8 x float>, <8 x float>* %1054, align 4, !tbaa !12, !llvm.access.group !16
  %1055 = fsub <8 x float> %1052, %wide.load179.3
  %1056 = fpext <8 x float> %1055 to <8 x double>
  %1057 = call <8 x double> @llvm.fmuladd.v8f64(<8 x double> %1056, <8 x double> <double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666, double 0xBFE6666666666666>, <8 x double> %1040)
  %1058 = fptrunc <8 x double> %1057 to <8 x float>
  %1059 = bitcast float* %1038 to <8 x float>*
  store <8 x float> %1058, <8 x float>* %1059, align 4, !tbaa !12, !llvm.access.group !16
  br label %pregion_for_end.i.i.7

pregion_for_entry.entry.i.i.7:                    ; preds = %pregion_for_entry.entry.i.i.7, %pregion_for_entry.entry.i.i.7.preheader
  %_local_id_x.i.0.7 = phi i64 [ %1073, %pregion_for_entry.entry.i.i.7 ], [ 0, %pregion_for_entry.entry.i.i.7.preheader ]
  %add1.i.i.i.7 = add nuw nsw i64 %_local_id_x.i.0.7, %mul.i.i.i
  %conv.i.i.7 = trunc i64 %add1.i.i.i.7 to i32
  %add.i.i.7 = add i32 %mul.i.i.7, %conv.i.i.7
  %idxprom.i.i.7 = sext i32 %add.i.i.7 to i64
  %arrayidx.i.i.7 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.7
  %1060 = load float, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.7 = fpext float %1060 to double
  %add6.i.i.7 = add i32 %add.i.i.7, 1
  %idxprom7.i.i.7 = sext i32 %add6.i.i.7 to i64
  %arrayidx8.i.i.7 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.7
  %1061 = load float, float* %arrayidx8.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.7 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.7
  %1062 = load float, float* %arrayidx12.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.7 = fsub float %1061, %1062
  %add15.i.i.7 = add nsw i32 %mul14.i.i.7, %conv.i.i.7
  %idxprom16.i.i.7 = sext i32 %add15.i.i.7 to i64
  %arrayidx17.i.i.7 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.7
  %1063 = load float, float* %arrayidx17.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.7 = fadd float %sub.i.i.7, %1063
  %arrayidx22.i.i.7 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.7
  %1064 = load float, float* %arrayidx22.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.7 = fsub float %add18.i.i.7, %1064
  %conv24.i.i.7 = fpext float %sub23.i.i.7 to double
  %1065 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.7, double 0xBFE6666666666666, double %conv3.i.i.7) #3
  %conv26.i.i.7 = fptrunc double %1065 to float
  store float %conv26.i.i.7, float* %arrayidx.i.i.7, align 4, !tbaa !12, !llvm.access.group !16
  %1066 = or i64 %_local_id_x.i.0.7, 1
  %add1.i.i.i.7.1 = add nuw nsw i64 %1066, %mul.i.i.i
  %conv.i.i.7.1 = trunc i64 %add1.i.i.i.7.1 to i32
  %add.i.i.7.1 = add i32 %mul.i.i.7, %conv.i.i.7.1
  %idxprom.i.i.7.1 = sext i32 %add.i.i.7.1 to i64
  %arrayidx.i.i.7.1 = getelementptr inbounds float, float* %12, i64 %idxprom.i.i.7.1
  %1067 = load float, float* %arrayidx.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %conv3.i.i.7.1 = fpext float %1067 to double
  %add6.i.i.7.1 = add i32 %add.i.i.7.1, 1
  %idxprom7.i.i.7.1 = sext i32 %add6.i.i.7.1 to i64
  %arrayidx8.i.i.7.1 = getelementptr inbounds float, float* %6, i64 %idxprom7.i.i.7.1
  %1068 = load float, float* %arrayidx8.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %arrayidx12.i.i.7.1 = getelementptr inbounds float, float* %6, i64 %idxprom.i.i.7.1
  %1069 = load float, float* %arrayidx12.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub.i.i.7.1 = fsub float %1068, %1069
  %add15.i.i.7.1 = add nsw i32 %mul14.i.i.7, %conv.i.i.7.1
  %idxprom16.i.i.7.1 = sext i32 %add15.i.i.7.1 to i64
  %arrayidx17.i.i.7.1 = getelementptr inbounds float, float* %9, i64 %idxprom16.i.i.7.1
  %1070 = load float, float* %arrayidx17.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %add18.i.i.7.1 = fadd float %sub.i.i.7.1, %1070
  %arrayidx22.i.i.7.1 = getelementptr inbounds float, float* %9, i64 %idxprom.i.i.7.1
  %1071 = load float, float* %arrayidx22.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %sub23.i.i.7.1 = fsub float %add18.i.i.7.1, %1071
  %conv24.i.i.7.1 = fpext float %sub23.i.i.7.1 to double
  %1072 = tail call double @llvm.fmuladd.f64(double %conv24.i.i.7.1, double 0xBFE6666666666666, double %conv3.i.i.7.1) #3
  %conv26.i.i.7.1 = fptrunc double %1072 to float
  store float %conv26.i.i.7.1, float* %arrayidx.i.i.7.1, align 4, !tbaa !12, !llvm.access.group !16
  %1073 = add nuw nsw i64 %_local_id_x.i.0.7, 2
  %exitcond.7.not.1 = icmp eq i64 %1073, 32
  br i1 %exitcond.7.not.1, label %pregion_for_end.i.i.7.loopexit, label %pregion_for_entry.entry.i.i.7, !llvm.loop !44

pregion_for_end.i.i.7.loopexit:                   ; preds = %pregion_for_entry.entry.i.i.7
  br label %pregion_for_end.i.i.7

pregion_for_end.i.i.7:                            ; preds = %pregion_for_end.i.i.7.loopexit, %vector.body158
  ret void
}

; Function Attrs: nounwind readnone speculatable willreturn
declare <8 x double> @llvm.fmuladd.v8f64(<8 x double>, <8 x double>, <8 x double>) #0

attributes #0 = { nounwind readnone speculatable willreturn }
attributes #1 = { alwaysinline nofree norecurse nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "stackrealign" "target-cpu"="skylake" "target-features"="+adx,+aes,+avx,+avx2,+bmi,+bmi2,+clflushopt,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+popcnt,+prfchw,+rdrnd,+rdseed,+sahf,+sgx,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "uniform-work-group-size"="true" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { nofree nounwind }
attributes #3 = { nounwind }

!llvm.module.flags = !{!0, !1, !2}
!opencl.ocl.version = !{!3}
!llvm.ident = !{!4}
!opencl.spir.version = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 1, i32 2}
!4 = !{!"clang version 11.0.0 (git@github.com:llvm/llvm-project.git 91e89f9a5115b0f83b8f026e1ad0e6d1f885fa9b)"}
!5 = !{i32 1, i32 1, i32 1, i32 0, i32 0}
!6 = !{!"none", !"none", !"none", !"none", !"none"}
!7 = !{!"DATA_TYPE*", !"DATA_TYPE*", !"DATA_TYPE*", !"int", !"int"}
!8 = !{!"float*", !"float*", !"float*", !"int", !"int"}
!9 = !{!"", !"", !"", !"", !""}
!10 = !{!"ex", !"ey", !"hz", !"nx", !"ny"}
!11 = !{i32 1}
!12 = !{!13, !13, i64 0}
!13 = !{!"float", !14, i64 0}
!14 = !{!"omnipotent char", !15, i64 0}
!15 = !{!"Simple C/C++ TBAA"}
!16 = !{!17, !18}
!17 = distinct !{}
!18 = distinct !{}
!19 = distinct !{!19, !20, !21}
!20 = !{!"llvm.loop.parallel_accesses", !17}
!21 = !{!"llvm.loop.isvectorized", i32 1}
!22 = distinct !{!22, !20, !21}
!23 = distinct !{!23, !20, !21}
!24 = distinct !{!24, !20, !21}
!25 = distinct !{!25, !20, !21}
!26 = distinct !{!26, !20, !21}
!27 = distinct !{!27, !20, !21}
!28 = distinct !{!28, !20, !21}
!29 = distinct !{!29, !20, !21}
!30 = distinct !{!30, !20, !21}
!31 = distinct !{!31, !20, !21}
!32 = distinct !{!32, !20, !21}
!33 = distinct !{!33, !20, !21}
!34 = distinct !{!34, !20, !21}
!35 = distinct !{!35, !20, !21}
!36 = distinct !{!36, !20, !21}
!37 = distinct !{!37, !20, !21}
!38 = distinct !{!38, !20, !21}
!39 = distinct !{!39, !20, !21}
!40 = distinct !{!40, !20, !21}
!41 = distinct !{!41, !20, !21}
!42 = distinct !{!42, !20, !21}
!43 = distinct !{!43, !20, !21}
!44 = distinct !{!44, !20, !21}
